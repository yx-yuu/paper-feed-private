<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Tue, 10 Feb 2026 05:00:00 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[arXiv-CR 2026] Hydra: Robust Hardware-Assisted Malware Detection</title><link>https://arxiv.org/abs/2602.07240</link><description>arXiv:2602.07240v1 Announce Type: new 
Abstract: Malware detection using Hardware Performance Counters (HPCs) offers a promising, low-overhead approach for monitoring program behavior. However, a fundamental architectural constraint, that only a limited number of hardware events can be monitored concurrently, creates a significant bottleneck, leading to detection blind spots. Prior work has primarily focused on optimizing machine learning models for a single, statically chosen event set, or on ensembling models over the same feature set. We argue that robustness requires diversifying not only the models, but also the underlying feature sets (i.e., the monitored hardware events) in order to capture a broader spectrum of program behavior. This observation motivates the following research question: Can detection performance be improved by trading temporal granularity for broader coverage, via the strategic scheduling of different feature sets over time? To answer this question, we propose Hydra, a novel detection mechanism that partitions execution traces into time slices and learns an effective schedule of feature sets and corresponding classifiers for deployment. By cycling through complementary feature sets, Hydra mitigates the limitations of a fixed monitoring perspective. Our experimental evaluation shows that Hydra significantly outperforms state-of-the-art single-feature-set baselines, achieving a 19.32% improvement in F1 score and a 60.23% reduction in false positive rate. These results underscore the importance of feature-set diversity and establish strategic multi-feature-set scheduling as an effective principle for robust, hardware-assisted malware detection.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07240v1</guid></item><item><title>[arXiv-CR 2026] Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction</title><link>https://arxiv.org/abs/2602.07287</link><description>arXiv:2602.07287v1 Announce Type: new 
Abstract: Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches.
  In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities collected from KernelCTF, our results show that K-Repro can generate PoCs that reproduce over 50\% of the cases with practical time and monetary cost.
  Beyond aggregate success rates, we perform an extensive study of effectiveness, efficiency, stability, and impact factors to explain when agentic reproduction succeeds, where it fails, and which components drive performance. These findings provide actionable guidance for building more reliable autonomous security agents and for assessing real-world N-day risk from both offensive and defensive perspectives.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07287v1</guid></item><item><title>[arXiv-CR 2026] AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management</title><link>https://arxiv.org/abs/2602.07398</link><description>arXiv:2602.07398v1 Announce Type: new 
Abstract: Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.
  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.
  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07398v1</guid></item><item><title>[arXiv-CR 2026] Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model</title><link>https://arxiv.org/abs/2602.07422</link><description>arXiv:2602.07422v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07422v1</guid></item><item><title>[arXiv-CR 2026] SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients</title><link>https://arxiv.org/abs/2602.07513</link><description>arXiv:2602.07513v1 Announce Type: new 
Abstract: Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse.
  We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07513v1</guid></item><item><title>[arXiv-CR 2026] MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots</title><link>https://arxiv.org/abs/2602.07517</link><description>arXiv:2602.07517v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07517v1</guid></item><item><title>[arXiv-CR 2026] AirCatch: Effectively tracing advanced tag-based trackers</title><link>https://arxiv.org/abs/2602.07656</link><description>arXiv:2602.07656v1 Announce Type: new 
Abstract: Tag-based tracking ecosystems help users locate lost items, but can be leveraged for unwanted tracking and stalking. Existing protocol-driven defenses and prior academic solutions largely assume stable identifiers or predictable beaconing. However, identifier-based defenses fundamentally break down against advanced rogue trackers that aggressively rotate identifiers. We present AirCatch, a passive detection system that exploits a physical-layer constraint: while logical identifiers can change arbitrarily fast, the transmitter's analog imprint remains stable and reappears as a compact and persistently occupied region in Carrier Frequency Offset (CFO) feature space. AirCatch advances the state of the art along three axes: (i) a novel, modulation-aware CFO fingerprint that augments packet-level CFO with content-independent CFO components that amplify device distinctiveness; (ii) a new tracking detection algorithm based on high core density and persistence that is robust to contamination and evasion through per-identifier segmentation; and (iii) an ultra-low-cost receiver, an approximately 10 dollar BLE SDR named BlePhasyr, built from commodity components, that makes RF fingerprinting based detection practical in resource-constrained deployments. We evaluate AirCatch across Apple, Google, Tile, and Samsung tag families in multi-hour captures, systematically stress-test evasion using a scenario generator over a grid of transmission and rotation periods, and validate in diverse real-world mobility traces including home and office commutes, public transport, car travel, and airport journeys while sweeping background tag density. Across these stress tests, AirCatch achieves no false positives and early detection over a wide range of adversarial configurations and environments, degrading gracefully only in extreme low-rate regimes that also reduce attacker utility.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07656v1</guid></item><item><title>[arXiv-CR 2026] SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned</title><link>https://arxiv.org/abs/2602.07666</link><description>arXiv:2602.07666v1 Announce Type: new 
Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward deploying autonomous CRSs in practice.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07666v1</guid></item><item><title>[arXiv-CR 2026] CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment</title><link>https://arxiv.org/abs/2602.08023</link><description>arXiv:2602.08023v1 Announce Type: new 
Abstract: Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08023v1</guid></item><item><title>[arXiv-CR 2026] IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports</title><link>https://arxiv.org/abs/2602.08072</link><description>arXiv:2602.08072v1 Announce Type: new 
Abstract: GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\% on a benchmark dataset, outperforming traditional regex-based scanners. \textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \href{https://github.com/nafiurahman00/IssueGuard}{https://github.com/nafiurahman00/IssueGuard}, and a demonstration video is available at \href{https://youtu.be/kvbWA8rr9cU}{https://youtu.be/kvbWA8rr9cU}.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08072v1</guid></item><item><title>[arXiv-CR 2026] Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4</title><link>https://arxiv.org/abs/2602.08384</link><description>arXiv:2602.08384v1 Announce Type: new 
Abstract: Formal methods (FM) are reliable but costly to apply, often requiring years of expert effort in industrial-scale projects such as seL4, especially for theorem proving. Recent advances in large language models (LLMs) have made automated theorem proving increasingly feasible. However, most prior work focuses on mathematics-oriented benchmarks such as miniF2F, with limited evaluation on real-world verification projects. The few studies that consider industrial-scale verification mostly rely on closed-source models with hundreds of billions of parameters, which cannot be locally deployed and incur substantial usage costs. In this paper, we propose AutoReal, an LLM-driven theorem proving method for real-world industrial-scale systems with support for lightweight local deployment. We evaluate AutoReal on the seL4-Isabelle verification project as a representative and challenging case study. AutoReal incorporates two key improvements: (1) chain-of-thought (CoT)-based proof training, which teaches the LLM the reasoning behind proof steps and enables step-wise explanations alongside proofs, and (2) context augmentation, which leverages proof context from the project to enhance LLM-driven proving. Based on the AutoReal methodology, we fine-tune a base model to obtain AutoReal-Prover, a compact 7B-scale prover for industrial-scale theorem proving. AutoReal-Prover achieves a 51.67% proof success rate on 660 theorems from seL4-designated Important Theories across all 10 seL4 proof categories, substantially outperforming prior attempts on seL4 (27.06%). To evaluate generalization, we further apply AutoReal-Prover to three security-related projects from the Archive of Formal Proofs (AFP), covering all 451 theorems and achieving a proof success rate of 53.88%. Overall, this work advances the application of LLM-driven theorem proving in real-world industrial-scale verification.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08384v1</guid></item><item><title>[arXiv-CR 2026] LLMs + Security = Trouble</title><link>https://arxiv.org/abs/2602.08422</link><description>arXiv:2602.08422v1 Announce Type: new 
Abstract: We argue that when it comes to producing secure code with AI, the prevailing "fighting fire with fire" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.
  While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the "vibe coding" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.
  In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08422v1</guid></item><item><title>[arXiv-CR 2026] Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion</title><link>https://arxiv.org/abs/2602.08668</link><description>arXiv:2602.08668v1 Announce Type: new 
Abstract: Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved "seed" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.
  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.
  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08668v1</guid></item><item><title>[arXiv-CR 2026] DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing</title><link>https://arxiv.org/abs/2602.08750</link><description>arXiv:2602.08750v1 Announce Type: new 
Abstract: The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08750v1</guid></item><item><title>[arXiv-CR 2026] RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks</title><link>https://arxiv.org/abs/2602.08446</link><description>arXiv:2602.08446v1 Announce Type: cross 
Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08446v1</guid></item><item><title>[arXiv-CR 2026] Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs</title><link>https://arxiv.org/abs/2602.08563</link><description>arXiv:2602.08563v1 Announce Type: cross 
Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08563v1</guid></item><item><title>[arXiv-CR 2026] StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors</title><link>https://arxiv.org/abs/2602.08934</link><description>arXiv:2602.08934v1 Announce Type: cross 
Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08934v1</guid></item><item><title>[arXiv-CR 2026] Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence</title><link>https://arxiv.org/abs/2509.23573</link><description>arXiv:2509.23573v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to help security analysts manage the surge of cyber threats, automating tasks from vulnerability assessment to incident response. Yet in operational CTI workflows, reliability gaps remain substantial. Existing explanations often point to generic model issues (e.g., hallucination), but we argue the dominant bottleneck is the threat landscape itself: CTI is heterogeneous, volatile, and fragmented. Under these conditions, evidence is intertwined, crowdsourced, and temporally unstable, which are properties that standard LLM-based studies rarely capture.
  In this paper, we present a comprehensive empirical study of LLM vulnerabilities in CTI reasoning. We introduce a human-in-the-loop categorization framework that robustly labels failure modes across the CTI lifecycle, avoiding the brittleness of automated "LLM-as-a-judge" pipelines. We identify three domain-specific cognitive failures: spurious correlations from superficial metadata, contradictory knowledge from conflicting sources, and constrained generalization to emerging threats. We validate these mechanisms via causal interventions and show that targeted defenses reduce failure rates significantly. Together, these results offer a concrete roadmap for building resilient, domain-aware CTI agents.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.23573v3</guid></item><item><title>[arXiv-CR 2026] SoK: Trust-Authorization Mismatch in LLM Agent Interactions</title><link>https://arxiv.org/abs/2512.06914</link><description>arXiv:2512.06914v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are evolving into autonomous agents capable of executing complex workflows via standardized protocols (e.g., MCP). However, this paradigm shifts control from deterministic code to probabilistic inference, creating a fundamental Trust-Authorization Mismatch: static permissions are structurally decoupled from the agent's fluctuating runtime trustworthiness. In this Systematization of Knowledge (SoK), we survey more than 200 representative papers to categorize the emerging landscape of agent security. We propose the Belief-Intention-Permission (B-I-P) framework as a unifying formal lens. By decomposing agent execution into three distinct stages-Belief Formation, Intent Generation, and Permission Grant-we demonstrate that diverse threats, from prompt injection to tool poisoning, share a common root cause: the desynchronization between dynamic trust states and static authorization boundaries. Using the B-I-P lens, we systematically map existing attacks and defenses and identify critical gaps where current mechanisms fail to bridge this mismatch. Finally, we outline a research agenda for shifting from static Role-Based Access Control (RBAC) to dynamic, risk-adaptive authorization.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.06914v2</guid></item><item><title>[arXiv-CR 2026] Towards Cybersecurity Superintelligence: from AI-guided humans to human-guided AI</title><link>https://arxiv.org/abs/2601.14614</link><description>arXiv:2601.14614v3 Announce Type: replace 
Abstract: Cybersecurity superintelligence -- artificial intelligence exceeding the best human capability in both speed and strategic reasoning -- represents the next frontier in security. This paper documents the emergence of such capability through three major contributions that have pioneered the field of AI Security. First, PentestGPT (2023) established LLM-guided penetration testing, achieving 228.6% improvement over baseline models through an architecture that externalizes security expertise into natural language guidance. Second, Cybersecurity AI (CAI, 2025) demonstrated automated expert-level performance, operating 3,600x faster than humans while reducing costs 156-fold, validated through #1 rankings at international competitions including the $50,000 Neurogrid CTF prize. Third, Generative Cut-the-Rope (G-CTR, 2026) introduces a neurosymbolic architecture embedding game-theoretic reasoning into LLM-based agents: symbolic equilibrium computation augments neural inference, doubling success rates while reducing behavioral variance 5.2x and achieving 2:1 advantage over non-strategic AI in Attack &amp; Defense scenarios. Together, these advances establish a clear progression from AI-guided humans to human-guided game-theoretic cybersecurity superintelligence.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14614v3</guid></item><item><title>[arXiv-CR 2026] Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software</title><link>https://arxiv.org/abs/2602.04894</link><description>arXiv:2602.04894v2 Announce Type: replace 
Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04894v2</guid></item><item><title>[arXiv-CR 2026] Can We Infer Confidential Properties of Training Data from LLMs?</title><link>https://arxiv.org/abs/2506.10364</link><description>arXiv:2506.10364v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties -- such as patient demographics or disease prevalence -- that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.10364v4</guid></item><item><title>[arXiv-CR 2026] Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title><link>https://arxiv.org/abs/2512.03310</link><description>arXiv:2512.03310v2 Announce Type: replace-cross 
Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.03310v2</guid></item><item><title>[arXiv-SE 2026] Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability</title><link>https://arxiv.org/abs/2602.07071</link><description>arXiv:2602.07071v1 Announce Type: new 
Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07071v1</guid></item><item><title>[arXiv-SE 2026] AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation</title><link>https://arxiv.org/abs/2602.07072</link><description>arXiv:2602.07072v1 Announce Type: new 
Abstract: Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07072v1</guid></item><item><title>[arXiv-SE 2026] CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs</title><link>https://arxiv.org/abs/2602.07080</link><description>arXiv:2602.07080v1 Announce Type: new 
Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07080v1</guid></item><item><title>[arXiv-SE 2026] Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation</title><link>https://arxiv.org/abs/2602.07083</link><description>arXiv:2602.07083v1 Announce Type: new 
Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07083v1</guid></item><item><title>[arXiv-SE 2026] Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation</title><link>https://arxiv.org/abs/2602.07086</link><description>arXiv:2602.07086v1 Announce Type: new 
Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07086v1</guid></item><item><title>[arXiv-SE 2026] Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study</title><link>https://arxiv.org/abs/2602.07147</link><description>arXiv:2602.07147v1 Announce Type: new 
Abstract: Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07147v1</guid></item><item><title>[arXiv-SE 2026] Forecasting Developer Environments with GenAI: A Research Perspective</title><link>https://arxiv.org/abs/2602.07412</link><description>arXiv:2602.07412v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07412v1</guid></item><item><title>[arXiv-SE 2026] Pull Requests as a Training Signal for Repo-Level Code Editing</title><link>https://arxiv.org/abs/2602.07457</link><description>arXiv:2602.07457v1 Announce Type: new 
Abstract: Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07457v1</guid></item><item><title>[arXiv-SE 2026] ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair</title><link>https://arxiv.org/abs/2602.07561</link><description>arXiv:2602.07561v1 Announce Type: new 
Abstract: Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07561v1</guid></item><item><title>[arXiv-SE 2026] A Course on the Introduction to Quantum Software Engineering: Experience Report</title><link>https://arxiv.org/abs/2602.07589</link><description>arXiv:2602.07589v1 Announce Type: new 
Abstract: Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.
  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.
  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07589v1</guid></item><item><title>[arXiv-SE 2026] Evaluating Large Language Models for Detecting Architectural Decision Violations</title><link>https://arxiv.org/abs/2602.07609</link><description>arXiv:2602.07609v1 Announce Type: new 
Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07609v1</guid></item><item><title>[arXiv-SE 2026] Debugging code world models</title><link>https://arxiv.org/abs/2602.07672</link><description>arXiv:2602.07672v1 Announce Type: new 
Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07672v1</guid></item><item><title>[arXiv-SE 2026] Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents</title><link>https://arxiv.org/abs/2602.07900</link><description>arXiv:2602.07900v1 Announce Type: new 
Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07900v1</guid></item><item><title>[arXiv-SE 2026] Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation</title><link>https://arxiv.org/abs/2602.08146</link><description>arXiv:2602.08146v1 Announce Type: new 
Abstract: Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants "hacking" the blind spots of T's current test suite, while T iteratively refines its test cases to "kill" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08146v1</guid></item><item><title>[arXiv-SE 2026] Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects</title><link>https://arxiv.org/abs/2602.08166</link><description>arXiv:2602.08166v1 Announce Type: new 
Abstract: Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \emph{extractors}, and supports \emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08166v1</guid></item><item><title>[arXiv-SE 2026] Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications</title><link>https://arxiv.org/abs/2602.08242</link><description>arXiv:2602.08242v1 Announce Type: new 
Abstract: Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08242v1</guid></item><item><title>[arXiv-SE 2026] Specification Vibing for Automated Program Repair</title><link>https://arxiv.org/abs/2602.08263</link><description>arXiv:2602.08263v1 Announce Type: new 
Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08263v1</guid></item><item><title>[arXiv-SE 2026] SWE Context Bench: A Benchmark for Context Learning in Coding</title><link>https://arxiv.org/abs/2602.08316</link><description>arXiv:2602.08316v1 Announce Type: new 
Abstract: Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problems. As a result, the ability of agents to accumulate, retrieve, and apply prior experience, as well as the efficiency gains from such reuse, remains difficult to measure. We introduce SWE-ContextBench, a benchmark designed to explicitly evaluate experience reuse in programming agents. Built on SWE-Bench Lite, SWE-ContextBench augments 300 base tasks with 99 related tasks derived from real dependency and reference relationships among GitHub issues and pull requests, forming task sequences with shared context. The benchmark evaluates agents along three complementary dimensions: prediction accuracy, time efficiency, and cost efficiency. Using SWE-ContextBench, we study multiple experience reuse settings, including oracle guided and autonomous retrieval, as well as full execution trajectories and compact summaries. Our results show that correctly selected summarized experience improves resolution accuracy and substantially reduces runtime and token cost, particularly on harder tasks. In contrast, unfiltered or incorrectly selected experience provides limited or negative benefits. These findings highlight the importance of experience representation and retrieval quality, and position SWE-ContextBench as a principled benchmark for studying experience reuse in programming agents.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08316v1</guid></item><item><title>[arXiv-SE 2026] Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches</title><link>https://arxiv.org/abs/2602.08561</link><description>arXiv:2602.08561v1 Announce Type: new 
Abstract: Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08561v1</guid></item><item><title>[arXiv-SE 2026] Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance</title><link>https://arxiv.org/abs/2602.08915</link><description>arXiv:2602.08915v1 Announce Type: new 
Abstract: The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08915v1</guid></item><item><title>[arXiv-SE 2026] CyberRAG: An Agentic RAG cyber attack classification and reporting tool</title><link>https://arxiv.org/abs/2507.02424</link><description>arXiv:2507.02424v2 Announce Type: cross 
Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming analysts with logs requiring rapidly evolving expertise. Conventional machine-learning detectors reduce alert volume but still yield many false positives, while standard Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify predictions. We present CyberRAG, a modular agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates: (i) fine-tuned classifiers specialized by attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that queries a domain-specific knowledge base until evidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG adopts an agentic design that enables dynamic control flow and adaptive reasoning. This architecture autonomously refines threat labels and natural-language justifications, reducing false positives and enhancing interpretability. It is also extensible: new attack types can be supported by adding classifiers without retraining the core agent. CyberRAG was evaluated on SQL Injection, XSS, and SSTI, achieving over 94\% accuracy per class and a final classification accuracy of 94.92\% through semantic orchestration. Generated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation, with robustness preserved against adversarial and unseen payloads. These results show that agentic, specialist-oriented RAG can combine high detection accuracy with trustworthy, SOC-ready prose, offering a flexible path toward partially automated cyber-defense workflows.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.02424v2</guid></item><item><title>[arXiv-SE 2026] On Randomness in Agentic Evals</title><link>https://arxiv.org/abs/2602.07150</link><description>arXiv:2602.07150v1 Announce Type: cross 
Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k&gt;1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07150v1</guid></item><item><title>[arXiv-SE 2026] KRONE: Hierarchical and Modular Log Anomaly Detection</title><link>https://arxiv.org/abs/2602.07303</link><description>arXiv:2602.07303v1 Announce Type: cross 
Abstract: Log anomaly detection is crucial for uncovering system failures and security risks. Although logs originate from nested component executions with clear boundaries, this structure is lost when they are stored as flat sequences. As a result, state-of-the-art methods risk missing true dependencies within executions while learning spurious ones across unrelated events. We propose KRONE, the first hierarchical anomaly detection framework that automatically derives execution hierarchies from flat logs for modular multi-level anomaly detection. At its core, the KRONE Log Abstraction Model captures application-specific semantic hierarchies from log data. This hierarchy is then leveraged to recursively decompose log sequences into multiple levels of coherent execution chunks, referred to as KRONE Seqs, transforming sequence-level anomaly detection into a set of modular KRONE Seq-level detection tasks. For each test KRONE Seq, KRONE employs a hybrid modular detection mechanism that dynamically routes between an efficient level-independent Local-Context detector, which rapidly filters normal sequences, and a Nested-Aware detector that incorporates cross-level semantic dependencies and supports LLM-based anomaly detection and explanation. KRONE further optimizes hierarchical detection through cached result reuse and early-exit strategies. Experiments on three public benchmarks and one industrial dataset from ByteDance Cloud demonstrate that KRONE achieves consistent improvements in detection accuracy, F1-score, data efficiency, resource efficiency, and interpretability. KRONE improves the F1-score by more than 10 percentage points over prior methods while reducing LLM usage to only a small fraction of the test data.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07303v1</guid></item><item><title>[arXiv-SE 2026] aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in Repository-level Code Completion</title><link>https://arxiv.org/abs/2503.15301</link><description>arXiv:2503.15301v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising results in repository-level code completion, which completes code based on the in-file and cross-file context of a repository. The cross-file context typically contains different types of information (e.g., relevant APIs and similar code) and is lengthy. In this paper, we found that LLMs struggle to fully utilize the information in the cross-file context. We hypothesize that one of the root causes of the limitation is the misalignment between pre-training (i.e., relying on nearby context) and repo-level code completion (i.e., frequently attending to long-range cross-file context). To address the above misalignment, we propose Code Long-context Alignment - COLA, a purely data-driven approach to explicitly teach LLMs to focus on the cross-file context. Specifically, COLA constructs a large-scale repo-level code completion dataset - COLA-132K, where each sample contains the long cross-file context (up to 128K tokens) and requires generating context-aware code (i.e., cross-file API invocations and code spans similar to cross-file context). Through a two-stage training pipeline upon COLA-132K, LLMs learn the capability of finding relevant information in the cross-file context, thus aligning LLMs with repo-level code completion. We apply COLA to multiple popular LLMs (e.g., aiXcoder-7B) and extensive experiments on COLA-132K and a public benchmark - CrossCodeEval. Our experiments yield the following results. 1) Effectiveness. COLA substantially improves the performance of multiple LLMs in repo-level code completion. For example, it improves aiXcoder-7B by up to 19.7% in exact match. 2) Generalizability. The capability learned by COLA can generalize to new languages. 3) Enhanced Context Utilization Capability. We design two probing experiments, which show COLA improves the capability of LLMs in utilizing the information in cross-file context.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.15301v2</guid></item><item><title>[arXiv-SE 2026] Prometheus: Towards Long-Horizon Codebase Navigation for Repository-Level Problem Solving</title><link>https://arxiv.org/abs/2507.19942</link><description>arXiv:2507.19942v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in automating software engineering tasks, spurring the emergence of coding agents that scaffold LLMs with external tools to resolve repository-level problems. However, existing agents still struggle to navigate large-scale codebases, as the Needle-in-a-Haystack problem persists even with million-token context windows, where relevant evidence is often overwhelmed by large volumes of irrelevant code and documentation. Prior codebase navigation approaches, including embedding-based retrieval, file-system exploration, and graph-based retrieval, address parts of this challenge but fail to capture the temporal continuity of agent reasoning, rendering agents stateless and causing repeated repository traversals that hinder scalable planning and reasoning. To address these limitations, we present Prometheus, a memory-centric coding agent framework for long-horizon codebase navigation. Prometheus represents the repository as a unified knowledge graph to encode semantic dependencies and employs a context engine augmented with working memory that retains and reuses previously explored contexts to ensure continuity across reasoning steps. Built upon this engine, Prometheus integrates memory-enhanced navigation into a multi-agent system for automated issue resolution, encompassing issue classification, bug reproduction, patch generation, and verification. Comprehensive experiments are conducted on two widely used issue resolution benchmarks, i.e., SWE-bench Verified and SWE-PolyBench Verified. Powered by GPT-5, Prometheus achieves state-of-the-art performance with 74.4% and 33.8% resolution rates on the two benchmarks, ranking Top-6 and Top-1 among open-source agent systems, respectively. Our data and code are available at https://github.com/EuniAI/Prometheus.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.19942v2</guid></item><item><title>[arXiv-SE 2026] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub</title><link>https://arxiv.org/abs/2509.14745</link><description>arXiv:2509.14745v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly being integrated into software development processes. The ability to generate code and submit pull requests with minimal human intervention, through the use of autonomous AI agents, is poised to become a standard practice. However, little is known about the practical usefulness of these pull requests and the extent to which their contributions are accepted in real-world projects. In this paper, we empirically study 567 GitHub pull requests (PRs) generated using Claude Code, an agentic coding tool, across 157 diverse open-source projects. Our analysis reveals that developers tend to rely on agents for tasks such as refactoring, documentation, and testing. The results indicate that 83.8% of these agent-assisted PRs are eventually accepted and merged by project maintainers, with 54.9% of the merged PRs are integrated without further modification. The remaining 45.1% require additional changes benefit from human revisions, especially for bug fixes, documentation, and adherence to project-specific standards. These findings suggest that while agent-assisted PRs are largely acceptable, they still benefit from human oversight and refinement.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.14745v3</guid></item><item><title>[arXiv-SE 2026] Improving Code Localization with Repository Memory</title><link>https://arxiv.org/abs/2510.01003</link><description>arXiv:2510.01003v2 Announce Type: replace 
Abstract: Code localization is a fundamental challenge in repository-level software engineering tasks such as bug fixing. While existing methods equip language agents with comprehensive tools/interfaces to fetch information from the repository, they overlook the critical aspect of memory, where each instance is typically handled from scratch assuming no prior repository knowledge. In contrast, human developers naturally build long-term repository memory, such as the functionality of key modules and associations between various bug types and their likely fix locations. In this work, we augment language agents with such memory by leveraging a repository's commit history -- a rich yet underutilized resource that chronicles the codebase's evolution. We introduce tools that allow the agent to retrieve from a non-parametric memory encompassing recent historical commits and linked issues, as well as functionality summaries of actively evolving parts of the codebase identified via commit patterns. We demonstrate that augmenting such a memory can significantly improve LocAgent, a state-of-the-art localization framework, on both SWE-bench-verified and the more recent SWE-bench-live benchmarks. Our research contributes towards developing agents that can accumulate and leverage past experience for long-horizon tasks, more closely emulating the expertise of human developers.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.01003v2</guid></item><item><title>[arXiv-SE 2026] TritonRL: Training LLMs to Think and Code Triton Without Cheating</title><link>https://arxiv.org/abs/2510.17891</link><description>arXiv:2510.17891v2 Announce Type: replace 
Abstract: The rapid evolution of Large Language Models (LLMs) has driven a growing demand for automated, high-performance system kernels to accelerate machine learning workloads. We introduce TritonRL, a domain-specialized 8B-scale LLM for Triton programming, trained via a novel reinforcement learning (RL) framework. While Triton synthesis faces unique challenges, including data scarcity and a high susceptibility to reward hacking, our approach enables robust kernel generation through two primary innovations. First, we implement a multi-layered verification system that provides high-fidelity reward signals, ensuring that generated kernels are both syntactically and functionally valid. Second, we propose Hierarchical Reward Decomposition (HRD), which decouples reinforcement for high-level reasoning and low-level implementation to resolve the credit assignment problem in long-sequence generation. Comprehensive evaluations on KernelBench demonstrate that TritonRL achieves state-of-the-art correctness and runtime speedup, outperforming concurrent Triton-specific models and matching the performance of frontier models with over 100B parameters. Our results highlight the effectiveness of hardware-aware RL paradigms in specialized domain adaptation.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.17891v2</guid></item><item><title>[arXiv-SE 2026] GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion</title><link>https://arxiv.org/abs/2601.23254</link><description>arXiv:2601.23254v2 Announce Type: replace 
Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.23254v2</guid></item><item><title>[arXiv-SE 2026] CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems</title><link>https://arxiv.org/abs/2602.02138</link><description>arXiv:2602.02138v2 Announce Type: replace 
Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02138v2</guid></item><item><title>[arXiv-SE 2026] ASA: Training-Free Representation Engineering for Tool-Calling Agents</title><link>https://arxiv.org/abs/2602.04935</link><description>arXiv:2602.04935v2 Announce Type: replace 
Abstract: Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conservative in entering tool mode, revealing a representation-behavior gap. We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B, ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04935v2</guid></item><item><title>[arXiv-SE 2026] A Dual-Loop Agent Framework for Automated Vulnerability Reproduction</title><link>https://arxiv.org/abs/2602.05721</link><description>arXiv:2602.05721v2 Announce Type: replace 
Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose CVE2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the Tactical Loop for code-level refinement, while the Strategic Loop for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that CVE2PoC achieves 82.9% and 54.3% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3% and 20.4%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05721v2</guid></item><item><title>[arXiv-SE 2026] Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic</title><link>https://arxiv.org/abs/2601.11840</link><description>arXiv:2601.11840v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11840v2</guid></item><item><title>[arXiv-SE 2026] Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering</title><link>https://arxiv.org/abs/2602.01465</link><description>arXiv:2602.01465v2 Announce Type: replace-cross 
Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.2% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01465v2</guid></item><item><title>[arXiv-SE 2026] ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</title><link>https://arxiv.org/abs/2602.01655</link><description>arXiv:2602.01655v2 Announce Type: replace-cross 
Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01655v2</guid></item><item><title>[arXiv-PL 2026] Static Analysis Under Non-Deterministic Program Assumptions</title><link>https://arxiv.org/abs/2602.07324</link><description>arXiv:2602.07324v1 Announce Type: new 
Abstract: Static analyses overwhelmingly trade precision for soundness and automation. For this reason, their use-cases are restricted to situations where imprecision isn't prohibitive. In this paper, we propose and specify a static analysis that accepts user-supplied program assumptions that are local to program locations. Such assumptions can be used to counteract imprecision in static analyses, enabling their use in a much wider variety of applications. These assumptions are taken by the analyzer non-deterministically, resulting in a function from sets of accepted assumptions to the resulting analysis under those assumptions. We also demonstrate the utility of such a function in two ways, both of which showcase how it can enable optimization over a search space of assumptions that is otherwise infeasible without the specified analysis.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07324v1</guid></item><item><title>[arXiv-PL 2026] Series-Parallel-Loop Decompositions of Control-flow Graphs</title><link>https://arxiv.org/abs/2602.07627</link><description>arXiv:2602.07627v1 Announce Type: new 
Abstract: Control-flow graphs (CFGs) of structured programs are well known to exhibit strong sparsity properties. Traditionally, this sparsity has been modeled using graph parameters such as treewidth and pathwidth, enabling the development of faster parameterized algorithms for tasks in compiler optimization, model checking, and program analysis. However, these parameters only approximate the structural constraints of CFGs: although every structured CFG has treewidth at most~7, many graphs with treewidth at most~7 cannot arise as CFGs. As a result, existing parameterized techniques are optimized for a substantially broader class of graphs than those encountered in practice.
  In this work, we introduce a new grammar-based decomposition framework that characterizes \emph{exactly} the class of control-flow graphs generated by structured programs. Our decomposition is intuitive, mirrors the syntactic structure of programs, and remains fully compatible with the dynamic-programming paradigm of treewidth-based methods. Using this framework, we design improved algorithms for two classical compiler optimization problems: \emph{Register Allocation} and \emph{Lifetime-Optimal Speculative Partial Redundancy Elimination (LOSPRE)}. Extensive experimental evaluation demonstrates significant performance improvements over previous state-of-the-art approaches, highlighting the benefits of using decompositions tailored specifically to CFGs.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07627v1</guid></item><item><title>[arXiv-PL 2026] Gillian Debugging: Swinging Through the (Compositional Symbolic Execution) Trees, Extended Version</title><link>https://arxiv.org/abs/2602.07742</link><description>arXiv:2602.07742v1 Announce Type: new 
Abstract: In recent years, compositional symbolic execution (CSE) tools have been growing in prominence and are becoming more and more applicable to real-world codebases. Still to this day, however, debugging the output of these tools remains difficult, even for specialist users. To address this, we introduce a debugging interface for symbolic execution tools, integrated with Visual Studio Code and the Gillian multi-language CSE platform, with strong focus on visualisation, interactivity, and intuitive representation of symbolic execution trees. We take care in making this interface tool-agnostic, easing its transfer to other symbolic analysis tools in future. We empirically evaluate our work with a user study, the results of which show the debugger's usefulness in helping early researchers understand the principles of CSE and verify fundamental data structure algorithms in Gillian.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07742v1</guid></item><item><title>[arXiv-PL 2026] Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks</title><link>https://arxiv.org/abs/2602.06976</link><description>arXiv:2602.06976v1 Announce Type: cross 
Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06976v1</guid></item><item><title>[arXiv-PL 2026] RefineStat: Efficient Exploration for Probabilistic Program Synthesis</title><link>https://arxiv.org/abs/2509.01082</link><description>arXiv:2509.01082v2 Announce Type: replace-cross 
Abstract: Probabilistic programming offers a powerful framework for modeling uncertainty, yet statistical model discovery in this domain entails navigating an immense search space under strict domain-specific constraints. When small language models are tasked with generating probabilistic programs, they frequently produce outputs that suffer from both syntactic and semantic errors, such as flawed inference constructs. Motivated by probabilistic programmers' domain expertise and debugging strategies, we introduce RefineStat, a language model--driven framework that enforces semantic constraints ensuring synthesized programs contain valid distributions and well-formed parameters, and then applies diagnostic-aware refinement by resampling prior or likelihood components whenever reliability checks fail. We evaluate RefineStat on multiple probabilistic-programming code-generation tasks using smaller language models (SLMs) and find that it produces programs that are both syntactically sound and statistically reliable, often matching or surpassing those from closed-source large language models (e.g., OpenAI o3).</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.01082v2</guid></item><item><title>[arXiv-PL 2026] Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling</title><link>https://arxiv.org/abs/2509.26553</link><description>arXiv:2509.26553v2 Announce Type: replace-cross 
Abstract: Existing benchmarks for tool-augmented language models (TaLMs) lack fine-grained control over task difficulty and remain vulnerable to data contamination. We present FuncBenchGen, a unified, contamination-free framework that evaluates TaLMs by generating synthetic multi-step tool-use tasks to stress-test TaLMs. The key idea is to cast tool use as traversal over a hidden function-dependency DAG where models must infer the correct sequence of calls to compute a target value. FuncBenchGen allows precise control over task difficulty (e.g., graph size, dependency depth, and distractor functions) while avoiding pretraining/test-time leakage. Our evaluation demonstrates reasoning-optimized models consistently outperform general-purpose models with GPT-5 significantly outperforming other available models. Performance declines sharply as dependency depth increases. Furthermore, connected distractors -- irrelevant functions sharing type-compatible variables with relevant functions -- prove especially difficult to handle. Also, strong models often make syntactically valid function calls but propagate incorrect or stale argument values across steps, revealing brittle state tracking by LLMs in multi-turn tool use. Motivated by this observation, we introduce a simple mitigation strategy that explicitly restates prior variable values to the agent at each step. Surprisingly, this lightweight change yields substantial gains across models. e.g., yielding an improvement in success rate from 62.5% to 81.3% for GPT-5.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.26553v2</guid></item><item><title>[arXiv-AI 2026] LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation</title><link>https://arxiv.org/abs/2602.07032</link><description>arXiv:2602.07032v1 Announce Type: new 
Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07032v1</guid></item><item><title>[arXiv-AI 2026] ST-Raptor: An Agentic System for Semi-Structured Table QA</title><link>https://arxiv.org/abs/2602.07034</link><description>arXiv:2602.07034v1 Announce Type: new 
Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07034v1</guid></item><item><title>[arXiv-AI 2026] DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents</title><link>https://arxiv.org/abs/2602.07035</link><description>arXiv:2602.07035v1 Announce Type: new 
Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07035v1</guid></item><item><title>[arXiv-AI 2026] PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents</title><link>https://arxiv.org/abs/2602.07187</link><description>arXiv:2602.07187v1 Announce Type: new 
Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07187v1</guid></item><item><title>[arXiv-AI 2026] From Out-of-Distribution Detection to Hallucination Detection: A Geometric View</title><link>https://arxiv.org/abs/2602.07253</link><description>arXiv:2602.07253v1 Announce Type: new 
Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07253v1</guid></item><item><title>[arXiv-AI 2026] NAAMSE: Framework for Evolutionary Security Evaluation of Agents</title><link>https://arxiv.org/abs/2602.07391</link><description>arXiv:2602.07391v1 Announce Type: new 
Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07391v1</guid></item><item><title>[arXiv-AI 2026] VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation</title><link>https://arxiv.org/abs/2602.07399</link><description>arXiv:2602.07399v1 Announce Type: new 
Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07399v1</guid></item><item><title>[arXiv-AI 2026] Progressive Multi-Agent Reasoning for Biological Perturbation Prediction</title><link>https://arxiv.org/abs/2602.07408</link><description>arXiv:2602.07408v1 Announce Type: new 
Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07408v1</guid></item><item><title>[arXiv-AI 2026] GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design</title><link>https://arxiv.org/abs/2602.07491</link><description>arXiv:2602.07491v1 Announce Type: new 
Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07491v1</guid></item><item><title>[arXiv-AI 2026] M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions</title><link>https://arxiv.org/abs/2602.07624</link><description>arXiv:2602.07624v1 Announce Type: new 
Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07624v1</guid></item><item><title>[arXiv-AI 2026] Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution</title><link>https://arxiv.org/abs/2602.07749</link><description>arXiv:2602.07749v1 Announce Type: new 
Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07749v1</guid></item><item><title>[arXiv-AI 2026] Learning to Continually Learn via Meta-learning Agentic Memory Designs</title><link>https://arxiv.org/abs/2602.07755</link><description>arXiv:2602.07755v1 Announce Type: new 
Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07755v1</guid></item><item><title>[arXiv-AI 2026] MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation</title><link>https://arxiv.org/abs/2602.07905</link><description>arXiv:2602.07905v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07905v1</guid></item><item><title>[arXiv-AI 2026] Weak-Driven Learning: How Weak Agents make Strong Agents Stronger</title><link>https://arxiv.org/abs/2602.08222</link><description>arXiv:2602.08222v1 Announce Type: new 
Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08222v1</guid></item><item><title>[arXiv-AI 2026] SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains</title><link>https://arxiv.org/abs/2602.08400</link><description>arXiv:2602.08400v1 Announce Type: new 
Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08400v1</guid></item><item><title>[arXiv-AI 2026] From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent</title><link>https://arxiv.org/abs/2602.08412</link><description>arXiv:2602.08412v1 Announce Type: new 
Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08412v1</guid></item><item><title>[arXiv-AI 2026] PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition</title><link>https://arxiv.org/abs/2602.08586</link><description>arXiv:2602.08586v1 Announce Type: new 
Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08586v1</guid></item><item><title>[arXiv-AI 2026] Scalable Delphi: Large Language Models for Structured Risk Estimation</title><link>https://arxiv.org/abs/2602.08889</link><description>arXiv:2602.08889v1 Announce Type: new 
Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08889v1</guid></item><item><title>[arXiv-AI 2026] Efficient and Stable Reinforcement Learning for Diffusion Language Models</title><link>https://arxiv.org/abs/2602.08905</link><description>arXiv:2602.08905v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08905v1</guid></item><item><title>[arXiv-AI 2026] Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models</title><link>https://arxiv.org/abs/2602.06967</link><description>arXiv:2602.06967v1 Announce Type: cross 
Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06967v1</guid></item><item><title>[arXiv-AI 2026] BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents</title><link>https://arxiv.org/abs/2602.06975</link><description>arXiv:2602.06975v1 Announce Type: cross 
Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06975v1</guid></item><item><title>[arXiv-AI 2026] Reasoning-Augmented Representations for Multimodal Retrieval</title><link>https://arxiv.org/abs/2602.07125</link><description>arXiv:2602.07125v1 Announce Type: cross 
Abstract: Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry "silent" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07125v1</guid></item><item><title>[arXiv-AI 2026] Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation</title><link>https://arxiv.org/abs/2602.07550</link><description>arXiv:2602.07550v1 Announce Type: cross 
Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07550v1</guid></item><item><title>[arXiv-AI 2026] VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation</title><link>https://arxiv.org/abs/2602.07555</link><description>arXiv:2602.07555v1 Announce Type: cross 
Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07555v1</guid></item><item><title>[arXiv-AI 2026] Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2602.07605</link><description>arXiv:2602.07605v1 Announce Type: cross 
Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07605v1</guid></item><item><title>[arXiv-AI 2026] SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models</title><link>https://arxiv.org/abs/2602.07616</link><description>arXiv:2602.07616v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07616v1</guid></item><item><title>[arXiv-AI 2026] AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning</title><link>https://arxiv.org/abs/2602.07625</link><description>arXiv:2602.07625v1 Announce Type: cross 
Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07625v1</guid></item><item><title>[arXiv-AI 2026] Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation</title><link>https://arxiv.org/abs/2602.07670</link><description>arXiv:2602.07670v1 Announce Type: cross 
Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07670v1</guid></item><item><title>[arXiv-AI 2026] SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2602.07833</link><description>arXiv:2602.07833v1 Announce Type: cross 
Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07833v1</guid></item><item><title>[arXiv-AI 2026] AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering</title><link>https://arxiv.org/abs/2602.07906</link><description>arXiv:2602.07906v1 Announce Type: cross 
Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07906v1</guid></item><item><title>[arXiv-AI 2026] Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation</title><link>https://arxiv.org/abs/2602.07954</link><description>arXiv:2602.07954v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07954v1</guid></item><item><title>[arXiv-AI 2026] DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity</title><link>https://arxiv.org/abs/2602.08005</link><description>arXiv:2602.08005v1 Announce Type: cross 
Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08005v1</guid></item><item><title>[arXiv-AI 2026] V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning</title><link>https://arxiv.org/abs/2602.08043</link><description>arXiv:2602.08043v1 Announce Type: cross 
Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08043v1</guid></item><item><title>[arXiv-AI 2026] Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders</title><link>https://arxiv.org/abs/2602.08077</link><description>arXiv:2602.08077v1 Announce Type: cross 
Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08077v1</guid></item><item><title>[arXiv-AI 2026] Reliable and Responsible Foundation Models: A Comprehensive Survey</title><link>https://arxiv.org/abs/2602.08145</link><description>arXiv:2602.08145v1 Announce Type: cross 
Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08145v1</guid></item><item><title>[arXiv-AI 2026] DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries</title><link>https://arxiv.org/abs/2602.08149</link><description>arXiv:2602.08149v1 Announce Type: cross 
Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08149v1</guid></item><item><title>[arXiv-AI 2026] Dreaming in Code for Curriculum Learning in Open-Ended Worlds</title><link>https://arxiv.org/abs/2602.08194</link><description>arXiv:2602.08194v1 Announce Type: cross 
Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08194v1</guid></item><item><title>[arXiv-AI 2026] DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning</title><link>https://arxiv.org/abs/2602.08213</link><description>arXiv:2602.08213v1 Announce Type: cross 
Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08213v1</guid></item><item><title>[arXiv-AI 2026] Inverting Data Transformations via Diffusion Sampling</title><link>https://arxiv.org/abs/2602.08267</link><description>arXiv:2602.08267v1 Announce Type: cross 
Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08267v1</guid></item><item><title>[arXiv-AI 2026] CLEAR: A Knowledge-Centric Vessel Trajectory Analysis Platform</title><link>https://arxiv.org/abs/2602.08482</link><description>arXiv:2602.08482v1 Announce Type: cross 
Abstract: Vessel trajectory data from the Automatic Identification System (AIS) is used widely in maritime analytics. Yet, analysis is difficult for non-expert users due to the incompleteness and complexity of AIS data. We present CLEAR, a knowledge-centric vessel trajectory analysis platform that aims to overcome these barriers. By leveraging the reasoning and generative capabilities of Large Language Models (LLMs), CLEAR transforms raw AIS data into complete, interpretable, and easily explorable vessel trajectories through a Structured Data-derived Knowledge Graph (SD-KG). As part of the demo, participants can configure parameters to automatically download and process AIS data, observe how trajectories are completed and annotated, inspect both raw and imputed segments together with their SD-KG evidence, and interactively explore the SD-KG through a dedicated graph viewer, gaining an intuitive and transparent understanding of vessel movements.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08482v1</guid></item><item><title>[arXiv-AI 2026] Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation</title><link>https://arxiv.org/abs/2602.08873</link><description>arXiv:2602.08873v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08873v1</guid></item><item><title>[arXiv-AI 2026] Tree Search for Language Model Agents</title><link>https://arxiv.org/abs/2407.01476</link><description>arXiv:2407.01476v4 Announce Type: replace 
Abstract: Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2407.01476v4</guid></item><item><title>[arXiv-AI 2026] Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives</title><link>https://arxiv.org/abs/2505.15693</link><description>arXiv:2505.15693v2 Announce Type: replace 
Abstract: Recent advances in reinforcement learning (RL) have renewed interest in reward design for shaping agent behavior, but manually crafting reward functions is tedious and error-prone. A principled alternative is to specify behavioral requirements in a formal, unambiguous language and automatically compile them into learning objectives. $\omega$-regular languages are a natural fit, given their role in formal verification and synthesis. However, most existing $\omega$-regular RL approaches operate in an episodic, discounted setting with periodic resets, which is misaligned with $\omega$-regular semantics over infinite traces. For continuing tasks, where the agent interacts with the environment over a single uninterrupted lifetime, the average-reward criterion is more appropriate.
  We focus on absolute liveness specifications, a subclass of $\omega$-regular languages that cannot be violated by any finite prefix and thus aligns naturally with continuing interaction. We present the first model-free RL framework that translates absolute liveness specifications into average-reward objectives and enables learning in unknown communicating Markov decision processes (MDPs) without episodic resetting. We also introduce a reward structure for lexicographic multi-objective optimization: among policies that maximize the satisfaction probability of an absolute liveness specification, the agent maximizes an external average-reward objective. Our method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions that do not require full environment knowledge, enabling model-free learning. Experiments across several benchmarks show that the continuing, average-reward approach outperforms competing discount-based methods.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.15693v2</guid></item><item><title>[arXiv-AI 2026] On Reasoning Strength Planning in Large Reasoning Models</title><link>https://arxiv.org/abs/2506.08390</link><description>arXiv:2506.08390v2 Announce Type: replace 
Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can automatically allocate more reasoning strengths (i.e., the number of reasoning tokens) for harder problems, exhibiting difficulty-awareness for better task performance. While this automatic reasoning strength allocation phenomenon has been widely observed, its underlying mechanism remains largely unexplored. To this end, we provide explanations for this phenomenon from the perspective of model activations. We find evidence that LRMs pre-plan the reasoning strengths in their activations even before generation, with this reasoning strength causally controlled by the magnitude of a pre-allocated directional vector. Specifically, we show that the number of reasoning tokens is predictable solely based on the question activations using linear probes, indicating that LRMs estimate the required reasoning strength in advance. We then uncover that LRMs encode this reasoning strength through a pre-allocated directional vector embedded in the activations of the model, where the vector's magnitude modulates the reasoning strength. Subtracting this vector can lead to reduced reasoning token number and performance, while adding this vector can lead to increased reasoning token number and even improved performance. We further reveal that this direction vector consistently yields positive reasoning length prediction, and it modifies the logits of end-of-reasoning token  to affect the reasoning length. Finally, we demonstrate two potential applications of our findings: overthinking behavior detection and enabling efficient reasoning on simple problems. Our work provides new insights into the internal mechanisms of reasoning in LRMs and offers practical tools for controlling their reasoning behaviors. Our code is available at https://github.com/AlphaLab-USTC/LRM-plans-CoT.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.08390v2</guid></item><item><title>[arXiv-AI 2026] CID-GraphRAG: Enhancing Multi-Turn Dialogue Systems through Dual-Pathway Retrieval of Conversation Flow and Context Semantics</title><link>https://arxiv.org/abs/2506.19385</link><description>arXiv:2506.19385v4 Announce Type: replace 
Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval-Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike traditional RAG systems that rely solely on semantic similarity or static knowledge graphs, CID-GraphRAG constructs intent transition graphs from goal-achieved historical dialogues and implements a dual-retrieval mechanism that balances intent-based graph traversal with semantic search. This approach enables the system to simultaneously leverage both conversational intent flow patterns and contextual semantics, significantly improving retrieval quality and response quality. In extensive experiments on real-world customer service dialogues, we demonstrated that CID-GraphRAG significantly outperforms both semantic-based and intent-based baselines across automatic metrics, LLM-as-a-Judge evaluations and human evaluations, with relative gains of 11.4% in BLEU, 4.9% in ROUGE, and 5.9% in METEOR. Most notably, CID-GraphRAG achieves a 57.9% improvement in response quality according to LLM-as-a-Judge evaluations. These results demonstrate that integrating intent transition structures with semantic retrieval creates a synergistic effect that neither approach achieves independently, establishing CID-GraphRAG as an effective framework for real-world multi-turn dialogue systems in customer service and other knowledge-intensive domains.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.19385v4</guid></item><item><title>[arXiv-AI 2026] Rethinking Explainable Disease Prediction: Synergizing Accuracy and Reliability via Reflective Cognitive Architecture</title><link>https://arxiv.org/abs/2509.21266</link><description>arXiv:2509.21266v2 Announce Type: replace 
Abstract: In clinical decision-making, predictive models face a persistent trade-off: accurate models are often opaque "black boxes," while interpretable methods frequently lack predictive precision or statistical grounding. In this paper, we challenge this dichotomy, positing that high predictive accuracy and high-quality descriptive explanations are not competing goals but synergistic outcomes of a deep, first-hand understanding of data. We propose the Reflective Cognitive Architecture (RCA), a novel framework designed to enable Large Language Models (LLMs) to learn directly from tabular data through experience and reflection. RCA integrates two core mechanisms: an iterative rules optimization process that refines logical argumentation by learning from prediction errors, and a distribution-aware rules check that grounds this logic in global statistical evidence to ensure robustness. We evaluated RCA against over 20 baselines - ranging from traditional machine learning to advanced reasoning LLMs and agents - across diverse medical datasets, including a proprietary real-world Catheter-Related Thrombosis (CRT) cohort. Crucially, to demonstrate real-world scalability, we extended our evaluation to two large-scale datasets. The results confirm that RCA achieves state-of-the-art predictive performance and superior robustness to data noise while simultaneously generating clear, logical, and evidence-based explanatory statements, maintaining its efficacy even at scale. The code is available at https://github.com/ssssszj/RCA.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21266v2</guid></item><item><title>[arXiv-AI 2026] SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation</title><link>https://arxiv.org/abs/2510.07733</link><description>arXiv:2510.07733v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly adopted for automating survey paper generation \cite{wang2406autosurvey, liang2025surveyx, yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing approaches typically extract content from a large collection of related papers and prompt LLMs to summarize them directly. However, such methods often overlook the structural relationships among papers, resulting in generated surveys that lack a coherent taxonomy and a deeper contextual understanding of research progress. To address these shortcomings, we propose \textbf{SurveyG}, an LLM-based agent framework that integrates \textit{hierarchical citation graph}, where nodes denote research papers and edges capture both citation dependencies and semantic relatedness between their contents, thereby embedding structural and contextual knowledge into the survey generation process. The graph is organized into three layers: \textbf{Foundation}, \textbf{Development}, and \textbf{Frontier}, to capture the evolution of research from seminal works to incremental advances and emerging directions. By combining horizontal search within layers and vertical depth traversal across layers, the agent produces multi-level summaries, which are consolidated into a structured survey outline. A multi-agent validation stage then ensures consistency, coverage, and factual accuracy in generating the final survey. Experiments, including evaluations by human experts and LLM-as-a-judge, demonstrate that SurveyG outperforms state-of-the-art frameworks, producing surveys that are more comprehensive and better structured to the underlying knowledge taxonomy of a field.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.07733v3</guid></item><item><title>[arXiv-AI 2026] The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities</title><link>https://arxiv.org/abs/2510.10238</link><description>arXiv:2510.10238v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become foundational tools in natural language processing, powering a wide range of applications and research. Many studies have shown that LLMs share significant similarities with the human brain. Recent neuroscience research has found that a small subset of biological neurons in the human brain are crucial for core cognitive functions, which raises a fundamental question: do LLMs also contain a small subset of critical neurons? In this paper, we investigate this question by proposing a Perturbation-based Causal Identification of Critical Neurons method to systematically locate such critical neurons in LLMs. Our findings reveal three key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting these critical neurons can cause a 72B-parameter model with over 1.1 billion neurons to completely collapse, with perplexity increasing by up to 20 orders of magnitude; (2) These critical neurons are not uniformly distributed, but tend to concentrate in the outer layers, particularly within the MLP down\_proj components; (3) Performance degradation exhibits sharp phase transitions, rather than a gradual decline, when these critical neurons are disrupted. Through comprehensive experiments across diverse model architectures and scales, we provide deeper analysis of these phenomena and their implications for LLM robustness and interpretability. These findings can offer guidance for developing more robust model architectures and improving deployment security in safety-critical applications. Our code is available at https://github.com/qqqqqqqzx/The-Achilles-Heel-of-LLMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.10238v2</guid></item><item><title>[arXiv-AI 2026] HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions</title><link>https://arxiv.org/abs/2511.18715</link><description>arXiv:2511.18715v2 Announce Type: replace 
Abstract: Building effective LLM agents increasingly requires selecting appropriate AI models as tools from large open repositories (e.g., HuggingFace with &gt; 2M models) based on natural language requests. Unlike invoking a fixed set of API tools, repository-scale model selection must handle massive, evolving candidates with incomplete metadata. Existing approaches incorporate full model descriptions into prompts, resulting in prompt bloat, excessive token costs, and limited scalability. To address these issues, we propose HuggingR$^4$, the first framework to recast model selection as an iterative reasoning process rather than one-shot retrieval. By synergistically integrating Reasoning, Retrieval, Refinement, and Reflection, HuggingR$^4$ progressively decomposes user intent, retrieves candidates through multi-round deliberation, refines selections via fine-grained analysis, and validates results through reflection. To facilitate rigorous evaluation, we introduce a large-scale benchmark comprising 14,399 diverse user requests across 37 task categories. Experiments demonstrate that HuggingR$^4$ achieves 92.03% workability and 82.46% reasonability-outperforming current state-of-the-art baselines by 26.51% and 33.25%, respectively, while reducing token consumption by $6.9 \times$.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.18715v2</guid></item><item><title>[arXiv-AI 2026] Conversational No-code, Multi-agentic Disease Module Identification and Drug Repurposing Prediction with ChatDRex</title><link>https://arxiv.org/abs/2511.21438</link><description>arXiv:2511.21438v2 Announce Type: replace 
Abstract: Repurposing approved drugs offers a time-efficient and cost-effective alternative to traditional drug development. However, in silico prediction of repurposing candidates is challenging and requires the effective collaboration of specialists in various fields, including pharmacology, medicine, biology, and bioinformatics. Fragmented, specialized algorithms and tools often address only narrow aspects of the overall problem. Heterogeneous, unstructured data landscapes require the expertise of specialized users. Hence, these data services do not integrate smoothly across workflows. With ChatDRex, we present a conversation-based, multi-agent system that facilitates the execution of complex bioinformatic analyses aiming for network-based drug repurposing prediction. It builds on the integrated systems medicine knowledge graph (NeDRex KG). ChatDRex provides natural language access to its extensive biomedical knowledge base. It integrates bioinformatics agents for network analysis, literature mining, and drug repurposing. These are complemented by agents that evaluate functional coherence for in silico validation. Its flexible multi-agent design assigns specific tasks to specialized agents, including query routing, data retrieval, algorithm execution, and result visualization. A dedicated reasoning module keeps the user in the loop and allows for hallucination detection. By enabling physicians and researchers without computer science expertise to control complex analyses with natural language, ChatDRex democratizes access to bioinformatics as an important resource for drug repurposing. It enables clinical experts to generate hypotheses and explore drug repurposing opportunities, ultimately accelerating the discovery of novel therapies and advancing personalized medicine and translational research. ChatDRex is publicly available at apps.cosy.bio/chatdrex.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.21438v2</guid></item><item><title>[arXiv-AI 2026] Token-Level LLM Collaboration via FusionRoute</title><link>https://arxiv.org/abs/2601.05106</link><description>arXiv:2601.05106v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05106v2</guid></item><item><title>[arXiv-AI 2026] Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</title><link>https://arxiv.org/abs/2601.19245</link><description>arXiv:2601.19245v3 Announce Type: replace 
Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs' initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19245v3</guid></item><item><title>[arXiv-AI 2026] OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence</title><link>https://arxiv.org/abs/2601.21083</link><description>arXiv:2601.21083v3 Announce Type: replace 
Abstract: As large language models (LLMs) improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning (RL) environment that evaluates IR agents under realistic prompt injection scenarios with execution-based scoring: time-to-first-containment (TTFC), evidence-gated action rate (EGAR), blast radius, and per-tier injection violation rates. Evaluating four frontier models on 40 standard-tier episodes each, we find consistent over-triggering: GPT-5.2 executes containment in 100% of episodes with 82.5% false positive rate, acting at step 4 before gathering sufficient evidence. Claude Sonnet 4.5 shows partial calibration (62.5% containment, 45% FP, TTFC of 10.6), suggesting calibration is not reliably present across frontier models. All models correctly identify the ground-truth threat when they act; the calibration gap is not in detection but in restraint. Code available at https://github.com/jbarnes850/opensec-env.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.21083v3</guid></item><item><title>[arXiv-AI 2026] Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling</title><link>https://arxiv.org/abs/2601.22636</link><description>arXiv:2601.22636v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22636v2</guid></item><item><title>[arXiv-AI 2026] Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward</title><link>https://arxiv.org/abs/2602.00845</link><description>arXiv:2602.00845v2 Announce Type: replace 
Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval. The code is available at https://github.com/dl-m9/InfoReasoner</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00845v2</guid></item><item><title>[arXiv-AI 2026] Beyond Quantity: Trajectory Diversity Scaling for Code Agents</title><link>https://arxiv.org/abs/2602.03219</link><description>arXiv:2602.03219v2 Announce Type: replace 
Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03219v2</guid></item><item><title>[arXiv-AI 2026] AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration</title><link>https://arxiv.org/abs/2602.03786</link><description>arXiv:2602.03786v2 Announce Type: replace 
Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03786v2</guid></item><item><title>[arXiv-AI 2026] Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation</title><link>https://arxiv.org/abs/2602.03950</link><description>arXiv:2602.03950v2 Announce Type: replace 
Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03950v2</guid></item><item><title>[arXiv-AI 2026] TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05818</link><description>arXiv:2602.05818v2 Announce Type: replace 
Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05818v2</guid></item><item><title>[arXiv-AI 2026] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</title><link>https://arxiv.org/abs/2602.06855</link><description>arXiv:2602.06855v2 Announce Type: replace 
Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06855v2</guid></item><item><title>[arXiv-AI 2026] DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing</title><link>https://arxiv.org/abs/2310.08785</link><description>arXiv:2310.08785v3 Announce Type: replace-cross 
Abstract: Text-guided image editing faces significant challenges when considering training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models have been proposed to avoid data collection, but they are limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP textual feature differences during the inference phase. And this design endows DeltaEdit with two advantages: (1) text-free training; (2) generalization to various text prompts for zero-shot inference. Extensive experiments validate the effectiveness and versatility of DeltaEdit with different generative models, including both the GAN model and the diffusion model, in achieving flexible text-guided image editing. Code is available at https://github.com/Yueming6568/DeltaEdit.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2310.08785v3</guid></item><item><title>[arXiv-AI 2026] Reproducible Benchmarking for Lung Nodule Detection and Malignancy Classification Across Multiple Low-Dose CT Datasets</title><link>https://arxiv.org/abs/2405.04605</link><description>arXiv:2405.04605v5 Announce Type: replace-cross 
Abstract: Evaluation of artificial intelligence (AI) models for low-dose CT lung cancer screening is limited by heterogeneous datasets, annotation standards, and evaluation protocols, making performance difficult to compare and translate across clinical settings. We establish a public, reproducible multi-dataset benchmark for lung nodule detection and nodule-level cancer classification and quantify cross-dataset generalizability.
  Using the Duke Lung Cancer Screening (DLCS) dataset as a clinically curated development set, we evaluate performance across LUNA16/LIDC-IDRI, NLST-3D, and LUNA25. Detection models trained on DLCS and LUNA16 were evaluated externally on NLST-3D using free-response ROC analysis. For malignancy classification, we compared five strategies: randomly initialized ResNet50, Models Genesis, Med3D, a Foundation Model for Cancer Biomarkers, and a Strategic Warm-Start (ResNet50-SWS) approach pretrained using detection-derived candidate patches stratified by confidence. Performance was summarized using AUC with 95% confidence intervals and DeLong tests.
  Detection performance varied substantially by training dataset, with DLCS-trained models outperforming LUNA16-trained models on external NLST-3D evaluation (sensitivity at 2 false positives per scan: 0.72 vs. 0.64; p &lt; 0.001). For malignancy classification, ResNet50-SWS achieved AUCs of 0.71 (DLCS), 0.90 (LUNA16), 0.81 (NLST-3D), and 0.80 (LUNA25), consistently matching or exceeding alternative pretraining strategies. These results demonstrate that dataset characteristics strongly influence lung cancer AI performance and highlight the need for transparent, multi-dataset benchmarking.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2405.04605v5</guid></item><item><title>[arXiv-AI 2026] ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems</title><link>https://arxiv.org/abs/2409.01392</link><description>arXiv:2409.01392v3 Announce Type: replace-cross 
Abstract: Much previous AI research has focused on developing monolithic models to maximize their intelligence, with the primary goal of enhancing performance on specific tasks. In contrast, this work attempts to study using LLM-based agents to design collaborative AI systems autonomously. To explore this problem, we first introduce ComfyBench to evaluate agents's ability to design collaborative AI systems in ComfyUI. ComfyBench is a comprehensive benchmark comprising 200 diverse tasks covering various instruction-following generation challenges, along with detailed annotations for 3,205 nodes and 20 workflows. Based on ComfyBench, we further develop ComfyAgent, a novel framework that empowers LLM-based agents to autonomously design collaborative AI systems by generating workflows. ComfyAgent is based on two core concepts. First, it represents workflows with code, which can be reversibly converted into workflows and executed as collaborative systems by the interpreter. Second, it constructs a multi-agent system that cooperates to learn from existing workflows and generate new workflows for a given task. While experimental results demonstrate that ComfyAgent achieves a comparable resolve rate to o1-preview and significantly surpasses other agents on ComfyBench, ComfyAgent has resolved only 15\% of creative tasks. LLM-based agents still have a long way to go in autonomously designing collaborative AI systems. Progress with ComfyBench is paving the way for more intelligent and autonomous collaborative AI systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2409.01392v3</guid></item><item><title>[arXiv-AI 2026] TruthPrInt: Mitigating Large Vision-Language Models Object Hallucination Via Latent Truthful-Guided Pre-Intervention</title><link>https://arxiv.org/abs/2503.10602</link><description>arXiv:2503.10602v3 Announce Type: replace-cross 
Abstract: Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the "overall truthfulness" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as "per-token" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states with OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist "generic truthful directions" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose TruthPrInt to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.10602v3</guid></item><item><title>[arXiv-AI 2026] Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex</title><link>https://arxiv.org/abs/2504.12474</link><description>arXiv:2504.12474v4 Announce Type: replace-cross 
Abstract: Text-attributed graphs (TAGs) present unique challenges in representation learning by requiring models to capture both the semantic richness of node-associated texts and the structural dependencies of the graph. While graph neural networks (GNNs) excel at modeling topological information, they lack the capacity to process unstructured text. Conversely, large language models (LLMs) are proficient in text understanding but are typically unaware of graph structure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel architecture that tightly integrates GNNs and LLMs through stacked Graph-Text Fusion Units. Each unit allows for mutual attention between textual and structural representations, enabling information to flow in both directions, text influencing structure and structure guiding textual interpretation. The proposed architecture is trained using parameter-efficient fine-tuning (LoRA), keeping the LLM frozen while adapting to task-specific signals. Extensive experiments on five benchmark datasets demonstrate that BiGTex achieves state-of-the-art performance in node classification and generalizes effectively to link prediction. An ablation study further highlights the importance of soft prompting and bi-directional attention in the model's success.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.12474v4</guid></item><item><title>[arXiv-AI 2026] ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models</title><link>https://arxiv.org/abs/2505.14238</link><description>arXiv:2505.14238v4 Announce Type: replace-cross 
Abstract: Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget, a property we validate through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.14238v4</guid></item><item><title>[arXiv-AI 2026] DRAGOn: Designing RAG On Periodically Updated Corpus</title><link>https://arxiv.org/abs/2507.05713</link><description>arXiv:2507.05713v3 Announce Type: replace-cross 
Abstract: This paper introduces DRAGOn, method to design a RAG benchmark on a regularly updated corpus. It features recent reference datasets, a question generation framework, an automatic evaluation pipeline, and a public leaderboard. Specified reference datasets allow for uniform comparison of RAG systems, while newly generated dataset versions mitigate data leakage and ensure that all models are evaluated on unseen, comparable data. The pipeline for automatic question generation extracts the Knowledge Graph from the text corpus and produces multiple question-answer pairs utilizing modern LLM capabilities. A set of diverse LLM-as-Judge metrics is provided for a comprehensive model evaluation. We used Russian news outlets to form the datasets and demonstrate our methodology. We launch a public leaderboard to track the development of RAG systems and encourage community participation.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.05713v3</guid></item><item><title>[arXiv-AI 2026] Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</title><link>https://arxiv.org/abs/2508.05342</link><description>arXiv:2508.05342v2 Announce Type: replace-cross 
Abstract: Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.05342v2</guid></item><item><title>[arXiv-AI 2026] Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS</title><link>https://arxiv.org/abs/2508.14313</link><description>arXiv:2508.14313v3 Announce Type: replace-cross 
Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.14313v3</guid></item><item><title>[arXiv-AI 2026] DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis</title><link>https://arxiv.org/abs/2508.20033</link><description>arXiv:2508.20033v2 Announce Type: replace-cross 
Abstract: The ability to research and synthesize knowledge is central to human expertise and progress. A new class of AI systems--designed for generative research synthesis--aims to automate this process by retrieving information from the live web and producing long-form, cited reports. Yet, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short, factual answers, while expert-curated datasets risk staleness and data contamination. Neither captures the complexity and evolving nature of real research synthesis tasks. We introduce DeepScholar-bench, a live benchmark and automated evaluation framework for generative research synthesis. DeepScholar-bench draws queries and human-written exemplars from recent, high-quality ArXiv papers and evaluates a real synthesis task: generating a related work section by retrieving, synthesizing, and citing prior work. Our automated framework holistically measures performance across three key dimensions--knowledge synthesis, retrieval quality, and verifiability. To further future work, we also contribute DeepScholar-ref, a simple, open-source reference pipeline, which is implemented on the LOTUS framework and provides a strong baseline. Using DeepScholar-bench, we systematically evaluate prior open-source systems, search agents with strong models, OpenAI's DeepResearch, and DeepScholar-ref. We find DeepScholar-bench is far from saturated: no system surpasses a geometric mean of $31\%$ across all metrics. These results highlight both the difficulty and importance of DeepScholar-bench as a foundation for advancing AI systems capable of generative research synthesis. We make our benchmark code and data available at https://github.com/guestrin-lab/deepscholar-bench.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.20033v2</guid></item><item><title>[arXiv-AI 2026] MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</title><link>https://arxiv.org/abs/2509.21391</link><description>arXiv:2509.21391v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating external knowledge sources during inference. Among these sources, textual graphs provide structured and semantically rich information that supports more precise and interpretable reasoning. This has led to growing interest in graph-based RAG systems. Despite their potential, most existing approaches rely on a single retriever to identify relevant subgraphs, which limits their ability to capture the diverse aspects of complex queries. Moreover, these systems often struggle to accurately judge the relevance of retrieved content, making them prone to distraction by irrelevant noise. To address these challenges, in this paper, we propose MIXRAG, a Mixture-of-Experts Graph-RAG framework that introduces multiple specialized graph retrievers and a dynamic routing controller to better handle diverse query intents. Each retriever is trained to focus on a specific aspect of graph semantics, such as entities, relations, or subgraph topology. A Mixture-of-Experts module adaptively selects and fuses relevant retrievers based on the input query. To reduce noise in the retrieved information, we introduce a query-aware GraphEncoder that carefully analyzes relationships within the retrieved subgraphs, highlighting the most relevant parts while down-weighting unnecessary noise. Empirical results demonstrate that our method achieves state-of-the-art performance and consistently outperforms various baselines. MIXRAG is effective across a wide range of graph-based tasks in different domains. The code will be released upon paper acceptance.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21391v2</guid></item><item><title>[arXiv-AI 2026] Copy-Paste to Mitigate Large Language Model Hallucinations</title><link>https://arxiv.org/abs/2510.00508</link><description>arXiv:2510.00508v2 Announce Type: replace-cross 
Abstract: While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting that higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2% to 24.5% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples -- 1/50th of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation. All codes are available at https://github.com/longyongchao/CopyPasteLLM</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.00508v2</guid></item><item><title>[arXiv-AI 2026] Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading</title><link>https://arxiv.org/abs/2510.04787</link><description>arXiv:2510.04787v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) and agentic systems have shown exceptional decision-making capabilities, revealing significant potential for autonomic finance. Current financial trading agents predominantly simulate anthropomorphic roles that inadvertently introduce emotional biases and rely on peripheral information, while being constrained by the necessity for continuous inference during deployment. In this paper, we pioneer the harmonization of strategic depth in agents with the mechanical rationality essential for quantitative trading. Consequently, we present TiMi (Trade in Minutes), a rationality-driven multi-agent system that architecturally decouples strategy development from minute-level deployment. TiMi leverages specialized LLM capabilities of semantic analysis, code programming, and mathematical reasoning within a comprehensive policy-optimization-deployment chain. Specifically, we propose a two-tier analytical paradigm from macro patterns to micro customization, layered programming design for trading bot implementation, and closed-loop optimization driven by mathematical reflection. Extensive evaluations across 200+ trading pairs in stock and cryptocurrency markets empirically validate the efficacy of TiMi in stable profitability, action efficiency, and risk control under volatile market dynamics.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.04787v2</guid></item><item><title>[arXiv-AI 2026] Report for NSF Workshop on AI for Electronic Design Automation</title><link>https://arxiv.org/abs/2601.14541</link><description>arXiv:2601.14541v3 Announce Type: replace-cross 
Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14541v3</guid></item><item><title>[arXiv-AI 2026] Membership Inference Attacks Against Fine-tuned Diffusion Language Models</title><link>https://arxiv.org/abs/2601.20125</link><description>arXiv:2601.20125v3 Announce Type: replace-cross 
Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20125v3</guid></item><item><title>[arXiv-LG 2026] AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization</title><link>https://arxiv.org/abs/2602.07054</link><description>arXiv:2602.07054v1 Announce Type: new 
Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07054v1</guid></item><item><title>[arXiv-LG 2026] Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures</title><link>https://arxiv.org/abs/2602.07070</link><description>arXiv:2602.07070v1 Announce Type: new 
Abstract: Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07070v1</guid></item><item><title>[arXiv-LG 2026] Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting</title><link>https://arxiv.org/abs/2602.07126</link><description>arXiv:2602.07126v1 Announce Type: new 
Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07126v1</guid></item><item><title>[arXiv-LG 2026] MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution</title><link>https://arxiv.org/abs/2602.07529</link><description>arXiv:2602.07529v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07529v1</guid></item><item><title>[arXiv-LG 2026] Efficient Planning in Reinforcement Learning via Model Introspection</title><link>https://arxiv.org/abs/2602.07719</link><description>arXiv:2602.07719v1 Announce Type: new 
Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07719v1</guid></item><item><title>[arXiv-LG 2026] MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation</title><link>https://arxiv.org/abs/2602.07848</link><description>arXiv:2602.07848v1 Announce Type: new 
Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07848v1</guid></item><item><title>[arXiv-LG 2026] Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion</title><link>https://arxiv.org/abs/2602.07875</link><description>arXiv:2602.07875v1 Announce Type: new 
Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07875v1</guid></item><item><title>[arXiv-LG 2026] Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection</title><link>https://arxiv.org/abs/2602.07892</link><description>arXiv:2602.07892v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07892v1</guid></item><item><title>[arXiv-LG 2026] TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation</title><link>https://arxiv.org/abs/2602.08036</link><description>arXiv:2602.08036v1 Announce Type: new 
Abstract: Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08036v1</guid></item><item><title>[arXiv-LG 2026] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</title><link>https://arxiv.org/abs/2602.08234</link><description>arXiv:2602.08234v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08234v1</guid></item><item><title>[arXiv-LG 2026] FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models</title><link>https://arxiv.org/abs/2602.08818</link><description>arXiv:2602.08818v1 Announce Type: new 
Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08818v1</guid></item><item><title>[arXiv-LG 2026] Deep Meta Coordination Graphs for Multi-agent Reinforcement Learning</title><link>https://arxiv.org/abs/2502.04028</link><description>arXiv:2502.04028v2 Announce Type: replace 
Abstract: This paper presents deep meta coordination graphs (DMCG) for learning cooperative policies in multi-agent reinforcement learning (MARL). Coordination graph formulations encode local interactions and accordingly factorize the joint value function of all agents to improve efficiency in MARL. Through DMCG, we dynamically compose what we refer to as \textit{meta coordination graphs}, to learn a more expressive representation of agent interactions and use them to integrate agent information through graph convolutional networks. The goal is to enable an evolving coordination graph to guide effective coordination in cooperative MARL tasks. The graphs are jointly optimized with agents' value functions to learn to implicitly reason about joint actions, facilitating the end-to-end learning of interaction representations and coordinated policies. We demonstrate that DMCG consistently achieves state-of-the-art coordination performance and sample efficiency on challenging cooperative tasks, outperforming several prior graph-based and non-graph-based MARL baselines. Through several ablations, we also isolate the impact of individual components in DMCG, showing that the observed improvements are due to the meaningful design choices in this approach. We also include an analysis of its computational complexity to discuss its practicality in real-world applications. All codes can be found here: {\color{blue}{https://github.com/Nikunj-Gupta/dmcg-marl}.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2502.04028v2</guid></item><item><title>[arXiv-LG 2026] Remasking Discrete Diffusion Models with Inference-Time Scaling</title><link>https://arxiv.org/abs/2503.00307</link><description>arXiv:2503.00307v4 Announce Type: replace 
Abstract: Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://guanghanwang.com/remdm</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.00307v4</guid></item><item><title>[arXiv-LG 2026] LogicXGNN: Grounded Logical Rules for Explaining Graph Neural Networks</title><link>https://arxiv.org/abs/2503.19476</link><description>arXiv:2503.19476v4 Announce Type: replace 
Abstract: Existing rule-based explanations for Graph Neural Networks (GNNs) provide global interpretability but often optimize and assess fidelity in an intermediate, uninterpretable concept space, overlooking grounding quality for end users in the final subgraph explanations. This gap yields explanations that may appear faithful yet be unreliable in practice. To this end, we propose LogicXGNN, a post-hoc framework that constructs logical rules over reliable predicates explicitly designed to capture the GNN's message-passing structure, thereby ensuring effective grounding. We further introduce data-grounded fidelity ($\textit{Fid}_{\mathcal{D}}$), a realistic metric that evaluates explanations in their final-graph form, along with complementary utility metrics such as coverage and validity. Across extensive experiments, LogicXGNN improves $\textit{Fid}_{\mathcal{D}}$ by over 20% on average relative to state-of-the-art methods while being 10-100 $\times$ faster. With strong scalability and utility performance, LogicXGNN produces explanations that are faithful to the model's logic and reliably grounded in observable data. Our code is available at https://github.com/allengeng123/LogicXGNN/.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.19476v4</guid></item><item><title>[arXiv-LG 2026] Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL</title><link>https://arxiv.org/abs/2504.15077</link><description>arXiv:2504.15077v3 Announce Type: replace 
Abstract: While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.15077v3</guid></item><item><title>[arXiv-LG 2026] Out of the Shadows: Exploring a Latent Space for Neural Network Verification</title><link>https://arxiv.org/abs/2505.17854</link><description>arXiv:2505.17854v3 Announce Type: replace 
Abstract: Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we propose a novel specification-driven input refinement procedure, i.e., we iteratively enclose the preimage of a neural network for all unsafe outputs to reduce the set of possible inputs to only enclose the unsafe ones. For that, we transfer output specifications to the input space by exploiting a latent space, which is an artifact of the propagation of a projection-based set representation through a neural network. A projection-based set representation, e.g., a zonotope, is a "shadow" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are "shadows" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance compared to the top-ranking tools of the international neural network verification competition.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17854v3</guid></item><item><title>[arXiv-LG 2026] d2: Improved Techniques for Training Reasoning Diffusion Language Models</title><link>https://arxiv.org/abs/2509.21474</link><description>arXiv:2509.21474v3 Announce Type: replace 
Abstract: While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on accurate estimates of the sampling trajectory likelihoods. Our likelihood estimator, d2-AnyOrder, achieves exact trajectory likelihood with a single model pass for DLMs that support a sampling algorithm called any-order decoding. Through an empirical study of widely used DLMs, we show that any-order decoding is not universally supported in practice. Consequently, for DLMs that do not naturally support any-order decoding, we propose another estimator, d2-StepMerge, which, unlike d2-AnyOrder, only approximates the trajectory likelihood. d2-StepMerge trades off compute for approximation accuracy in an analytically tractable manner. Empirically, d2 significantly outperforms widely-used RL baselines when applied to popular DLMs, and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500). We provide the code along with a blog post on the project page: https://guanghanwang.com/d2</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21474v3</guid></item><item><title>[arXiv-LG 2026] Test-Time Iterative Error Correction for Efficient Diffusion Models</title><link>https://arxiv.org/abs/2511.06250</link><description>arXiv:2511.06250v3 Announce Type: replace 
Abstract: With the growing demand for high-quality image generation on resource-constrained devices, efficient diffusion models have received increasing attention. However, such models suffer from approximation errors introduced by efficiency techniques, which significantly degrade generation quality. Once deployed, these errors are difficult to correct, as modifying the model is typically infeasible in deployment environments. Through an analysis of error propagation across diffusion timesteps, we reveal that these approximation errors can accumulate exponentially, severely impairing output quality. Motivated by this insight, we propose Iterative Error Correction (IEC), a novel test-time method that mitigates inference-time errors by iteratively refining the model's output. IEC is theoretically proven to reduce error propagation from exponential to linear growth, without requiring any retraining or architectural changes. IEC can seamlessly integrate into the inference process of existing diffusion models, enabling a flexible trade-off between performance and efficiency. Extensive experiments show that IEC consistently improves generation quality across various datasets, efficiency techniques, and model architectures, establishing it as a practical and generalizable solution for test-time enhancement of efficient diffusion models. The code is available in https://github.com/zysxmu/IEC.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.06250v3</guid></item><item><title>[arXiv-LG 2026] Categorical Reparameterization with Denoising Diffusion models</title><link>https://arxiv.org/abs/2601.00781</link><description>arXiv:2601.00781v2 Announce Type: replace 
Abstract: Learning models with categorical variables requires optimizing expectations over discrete distributions, a setting in which stochastic gradient-based optimization is challenging due to the non-differentiability of categorical sampling. A common workaround is to replace the discrete distribution with a continuous relaxation, yielding a smooth surrogate that admits reparameterized gradient estimates via the reparameterization trick. Building on this idea, we introduce ReDGE, a novel and efficient diffusion-based soft reparameterization method for categorical distributions. Our approach defines a flexible class of gradient estimators that includes the Straight-Through estimator as a special case. Experiments spanning latent variable models and inference-time reward guidance in discrete diffusion models demonstrate that ReDGE consistently matches or outperforms existing gradient-based methods. The code will be made available at https://github.com/samsongourevitch/redge.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.00781v2</guid></item><item><title>[arXiv-LG 2026] Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction</title><link>https://arxiv.org/abs/2601.17668</link><description>arXiv:2601.17668v2 Announce Type: replace 
Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17668v2</guid></item><item><title>[arXiv-LG 2026] Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning</title><link>https://arxiv.org/abs/2602.02206</link><description>arXiv:2602.02206v2 Announce Type: replace 
Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02206v2</guid></item><item><title>[arXiv-IR 2026] IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory</title><link>https://arxiv.org/abs/2602.07525</link><description>arXiv:2602.07525v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.</description><author>cs.IR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07525v1</guid></item><item><title>[arXiv-IR 2026] DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2602.08545</link><description>arXiv:2602.08545v1 Announce Type: new 
Abstract: Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.</description><author>cs.IR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08545v1</guid></item><item><title>[arXiv-statML 2026] Quantum Circuit Generation via test-time learning with large language models</title><link>https://arxiv.org/abs/2602.03466</link><description>arXiv:2602.03466v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can generate structured artifacts, but using them as dependable optimizers for scientific design requires a mechanism for iterative improvement under black-box evaluation. Here, we cast quantum circuit synthesis as a closed-loop, test-time optimization problem: an LLM proposes edits to a fixed-length gate list, and an external simulator evaluates the resulting state with the Meyer-Wallach (MW) global entanglement measure. We introduce a lightweight test-time learning recipe that can reuse prior high-performing candidates as an explicit memory trace, augments prompts with a score-difference feedback, and applies restart-from-the-best sampling to escape potential plateaus. Across fixed 20-qubit settings, the loop without feedback and restart-from-the-best improves random initial circuits over a range of gate budgets. To lift up this performance and success rate, we use the full learning strategy. For the 25-qubit, it mitigates a pronounced performance plateau when naive querying is used. Beyond raw scores, we analyze the structure of synthesized states and find that high MW solutions can correspond to stabilizer or graph-state-like constructions, but full connectivity is not guaranteed due to the metric property and prompt design. These results illustrate both the promise and the pitfalls of memory evaluator-guided LLM optimization for circuit synthesis, highlighting the critical role of prior human-made theoretical theorems to optimally design a custom tool in support of research.</description><author>stat.ML updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03466v3</guid></item><item><title>[arXiv-CR 2026] Multi-Agent-Driven Cognitive Secure Communications in Satellite-Terrestrial Networks</title><link>https://arxiv.org/abs/2602.06048</link><description>arXiv:2602.06048v1 Announce Type: new 
Abstract: Satellite-terrestrial networks (STNs) have emerged as a promising architecture for providing seamless wireless coverage and connectivity for multiple users. However, potential malicious eavesdroppers pose a serious threat to the private information via STNs due to their non-cooperative behavior and ability to launch intelligent attacks. To address this challenge, we propose a cognitive secure communication framework driven by multiple agents that coordinates spectrum scheduling and protection through real-time sensing, thereby disrupting the judgment of eavesdroppers while preserving reliable data transmission. On this basis, we formulate an optimization problem to maximize the secrecy probability of legitimate users, subject to a reliable transmission probability threshold. To tackle this problem, we propose a two-layer coordinated defense system. First, we develop a foundation layer based on multi-agent coordination schedule to determine the satellite operation matrix and the frequency slot occupation matrices, aiming to mitigate spectrum congestion and enhance transmission reliability. Then, we exploit generative adversarial networks to produce adversarial matrices, and employ learning-aided power control to set real and adversarial signal powers for protection layer, which actively degrades the inference capability of eavesdroppers. Simulation results demonstrate that the proposed method outperforms benchmark methods in terms of enhancing security performance and reducing power overhead for STNs in the cognitive secure communication scenario.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06048v1</guid></item><item><title>[arXiv-CR 2026] Identifying Adversary Tactics and Techniques in Malware Binaries with an LLM Agent</title><link>https://arxiv.org/abs/2602.06325</link><description>arXiv:2602.06325v1 Announce Type: new 
Abstract: Understanding TTPs (Tactics, Techniques, and Procedures) in malware binaries is essential for security analysis and threat intelligence, yet remains challenging in practice. Real-world malware binaries are typically stripped of symbols, contain large numbers of functions, and distribute malicious behavior across multiple code regions, making TTP attribution difficult. Recent large language models (LLMs) offer strong code understanding capabilities, but applying them directly to this task faces challenges in identifying analysis entry points, reasoning under partial observability, and misalignment with TTP-specific decision logic. We present TTPDetect, the first LLM agent for recognizing TTPs in stripped malware binaries. TTPDetect combines dense retrieval with LLM-based neural retrieval to narrow the space of analysis entry points. TTPDetect further employs a function-level analyzing agent consisting of a Context Explorer that performs on-demand, incremental context retrieval and a TTP-Specific Reasoning Guideline that achieves inference-time alignment. We build a new dataset that labels decompiled functions with TTPs across diverse malware families and platforms. TTPDetect achieves 93.25% precision and 93.81% recall on function-level TTP recognition, outperforming baselines by 10.38% and 18.78%, respectively. When evaluated on real world malware samples, TTPDetect recognizes TTPs with a precision of 87.37%. For malware with expert-written reports, TTPDetect recovers 85.7% of the documented TTPs and further discovers, on average, 10.5 previously unreported TTPs per malware.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06325v1</guid></item><item><title>[arXiv-CR 2026] Zero-Trust Runtime Verification for Agentic Payment Protocols: Mitigating Replay and Context-Binding Failures in AP2</title><link>https://arxiv.org/abs/2602.06345</link><description>arXiv:2602.06345v1 Announce Type: new 
Abstract: The deployment of autonomous AI agents capable of executing commercial transactions has motivated the adoption of mandate-based payment authorization protocols, including the Universal Commerce Protocol (UCP) and the Agent Payments Protocol (AP2). These protocols replace interactive, session-based authorization with cryptographically issued mandates, enabling asynchronous and autonomous execution. While AP2 provides specification-level guarantees through signature verification, explicit binding, and expiration semantics, real-world agentic execution introduces runtime behaviors such as retries, concurrency, and orchestration that challenge implicit assumptions about mandate usage.
  In this work, we present a security analysis of the AP2 mandate lifecycle and identify enforcement gaps that arise during runtime in agent-based payment systems. We propose a zero-trust runtime verification framework that enforces explicit context binding and consume-once mandate semantics using dynamically generated, time-bound nonces, ensuring that authorization decisions are evaluated at execution time rather than assumed from static issuance properties.
  Through simulation-based evaluation under high concurrency, we show that context-aware binding and consume-once enforcement address distinct and complementary attack classes, and that both are required to prevent replay and context-redirect attacks. The proposed framework mitigates all evaluated attacks while maintaining stable verification latency of approximately 3.8~ms at throughput levels up to 10{,}000 transactions per second. We further demonstrate that the required runtime state is bounded by peak concurrency rather than cumulative transaction history, indicating that robust runtime security for agentic payment execution can be achieved with minimal and predictable overhead.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06345v1</guid></item><item><title>[arXiv-CR 2026] Subgraph Reconstruction Attacks on Graph RAG Deployments with Practical Defenses</title><link>https://arxiv.org/abs/2602.06495</link><description>arXiv:2602.06495v1 Announce Type: new 
Abstract: Graph-based retrieval-augmented generation (Graph RAG) is increasingly deployed to support LLM applications by augmenting user queries with structured knowledge retrieved from a knowledge graph. While Graph RAG improves relational reasoning, it introduces a largely understudied threat: adversaries can reconstruct subgraphs from a target RAG system's knowledge graph, enabling privacy inference and replication of curated knowledge assets. We show that existing attacks are largely ineffective against Graph RAG even with simple prompt-based safeguards, because these attacks expose explicit exfiltration intent and are therefore easily suppressed by lightweight safe prompts. We identify three technical challenges for practical Graph RAG extraction under realistic safeguards and introduce GRASP, a closed-box, multi-turn subgraph reconstruction attack. GRASP (i) reframes extraction as a context-processing task, (ii) enforces format-compliant, instance-grounded outputs via per-record identifiers to reduce hallucinations and preserve relational details, and (iii) diversifies goal-driven attack queries using a momentum-aware scheduler to operate within strict query budgets. Across two real-world knowledge graphs, four safety-aligned LLMs, and multiple Graph RAG frameworks, GRASP attains the strongest type-faithful reconstruction where prior methods fail, reaching up to 82.9 F1. We further evaluate defenses and propose two lightweight mitigations that substantially reduce reconstruction fidelity without utility loss.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06495v1</guid></item><item><title>[arXiv-CR 2026] Dependable Artificial Intelligence with Reliability and Security (DAIReS): A Unified Syndrome Decoding Approach for Hallucination and Backdoor Trigger Detection</title><link>https://arxiv.org/abs/2602.06532</link><description>arXiv:2602.06532v1 Announce Type: new 
Abstract: Machine Learning (ML) models, including Large Language Models (LLMs), are characterized by a range of system-level attributes such as security and reliability. Recent studies have demonstrated that ML models are vulnerable to multiple forms of security violations, among which backdoor data-poisoning attacks represent a particularly insidious threat, enabling unauthorized model behavior and systematic misclassification. In parallel, deficiencies in model reliability can manifest as hallucinations in LLMs, leading to unpredictable outputs and substantial risks for end users. In this work on Dependable Artificial Intelligence with Reliability and Security (DAIReS), we propose a novel unified approach based on Syndrome Decoding for the detection of both security and reliability violations in learning-based systems. Specifically, we adapt the syndrome decoding approach to the NLP sentence-embedding space, enabling the discrimination of poisoned and non-poisoned samples within ML training datasets. Additionally, the same methodology can effectively detect hallucinated content due to self referential meta explanation tasks in LLMs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06532v1</guid></item><item><title>[arXiv-CR 2026] AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks</title><link>https://arxiv.org/abs/2602.06534</link><description>arXiv:2602.06534v1 Announce Type: new 
Abstract: Automated detection of cyber attacks is a critical capability to counteract the growing volume and sophistication of cyber attacks. However, the high numbers of security alerts issued by intrusion detection systems lead to alert fatigue among analysts working in security operations centres (SOC), which in turn causes slow reaction time and incorrect decision making. Alert grouping, which refers to clustering of security alerts according to their underlying causes, can significantly reduce the number of distinct items analysts have to consider. Unfortunately, conventional time-based alert grouping solutions are unsuitable for large scale computer networks characterised by high levels of false positive alerts and simultaneously occurring attacks. To address these limitations, we propose AlertBERT, a self-supervised framework designed to group alerts from isolated or concurrent attacks in noisy environments. Thereby, our open-source implementation of AlertBERT leverages masked-language-models and density-based clustering to support both real-time or forensic operation. To evaluate our framework, we further introduce a novel data augmentation method that enables flexible control over noise levels and simulates concurrent attack occurrences. Based on the data sets generated through this method, we demonstrate that AlertBERT consistently outperforms conventional time-based grouping techniques, achieving superior accuracy in identifying correct alert groups.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06534v1</guid></item><item><title>[arXiv-CR 2026] Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study</title><link>https://arxiv.org/abs/2602.06547</link><description>arXiv:2602.06547v1 Announce Type: new 
Abstract: Third-party agent skills extend LLM-based agents with instruction files and executable code that run on users' machines. Skills execute with user privileges and are distributed through community registries with minimal vetting, but no ground-truth dataset exists to characterize the resulting threats. We construct the first labeled dataset of malicious agent skills by behaviorally verifying 98,380 skills from two community registries, confirming 157 malicious skills with 632 vulnerabilities. These attacks are not incidental. Malicious skills average 4.03 vulnerabilities across a median of three kill chain phases, and the ecosystem has split into two archetypes: Data Thieves that exfiltrate credentials through supply chain techniques, and Agent Hijackers that subvert agent decision-making through instruction manipulation. A single actor accounts for 54.1\% of confirmed cases through templated brand impersonation. Shadow features, capabilities absent from public documentation, appear in 0\% of basic attacks but 100\% of advanced ones; several skills go further by exploiting the AI platform's own hook system and permission flags. Responsible disclosure led to 93.6\% removal within 30 days. We release the dataset and analysis pipeline to support future work on agent skill security.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06547v1</guid></item><item><title>[arXiv-CR 2026] HYDRA: Unearthing "Black Swan" Vulnerabilities in LEO Satellite Networks</title><link>https://arxiv.org/abs/2602.06612</link><description>arXiv:2602.06612v1 Announce Type: new 
Abstract: As Low Earth Orbit (LEO) become mega-constellations critical infrastructure, attacks targeting them have grown in number and range. The security analysis of LEO constellations faces a fundamental paradigm gap: traditional topology-centric methods fail to capture systemic risks arising from dynamic load imbalances and high-order dependencies, which can transform localized failures into network-wide cascades. To address this, we propose HYDRA, a hypergraph-based dynamic risk analysis framework. Its core is a novel metric, Hyper-Bridge Centrality (HBC), which quantifies node criticality via a load-to-redundancy ratio within dependency structures. A primary challenge to resilience: the most critical vulnerabilities are not in the densely connected satellite core, but in the seemingly marginal ground-space interfaces. These are the system's "Black Swan" nodes--topologically peripheral yet structurally lethal. We validate this through extensive simulations using realistic StarLink TLE data and population-based gravity model. Experiments demonstrate that HBC consistently outperforms traditional metrics, identifying critical failure points that surpass the structural damage potential of even betweenness centrality. This work shifts the security paradigm from connectivity to structural stress, demonstrating that securing the network edge is paramount and necessitates a fundamental redesign of redundancy strategies.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06612v1</guid></item><item><title>[arXiv-CR 2026] Confundo: Learning to Generate Robust Poison for Practical RAG Systems</title><link>https://arxiv.org/abs/2602.06616</link><description>arXiv:2602.06616v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) is increasingly deployed in real-world applications, where its reference-grounded design makes outputs appear trustworthy. This trust has spurred research on poisoning attacks that craft malicious content, inject it into knowledge sources, and manipulate RAG responses. However, when evaluated in practical RAG systems, existing attacks suffer from severely degraded effectiveness. This gap stems from two overlooked realities: (i) content is often processed before use, which can fragment the poison and weaken its effect, and (ii) users often do not issue the exact queries anticipated during attack design. These factors can lead practitioners to underestimate risks and develop a false sense of security. To better characterize the threat to practical systems, we present Confundo, a learning-to-poison framework that fine-tunes a large language model as a poison generator to achieve high effectiveness, robustness, and stealthiness. Confundo provides a unified framework supporting multiple attack objectives, demonstrated by manipulating factual correctness, inducing biased opinions, and triggering hallucinations. By addressing these overlooked challenges, Confundo consistently outperforms a wide range of purpose-built attacks across datasets and RAG configurations by large margins, even in the presence of defenses. Beyond exposing vulnerabilities, we also present a defensive use case that protects web content from unauthorized incorporation into RAG systems via scraping, with no impact on user experience.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06616v1</guid></item><item><title>[arXiv-CR 2026] Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models</title><link>https://arxiv.org/abs/2602.06687</link><description>arXiv:2602.06687v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06687v1</guid></item><item><title>[arXiv-CR 2026] GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models</title><link>https://arxiv.org/abs/2602.06718</link><description>arXiv:2602.06718v1 Announce Type: new 
Abstract: Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.
  To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\% to 94.93\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\% of researchers copy-paste BibTeX without checking and 44.4\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\% of reviewers do not thoroughly check references and 80.0\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06718v1</guid></item><item><title>[arXiv-CR 2026] Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection</title><link>https://arxiv.org/abs/2602.06751</link><description>arXiv:2602.06751v1 Announce Type: new 
Abstract: Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.
  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.
  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06751v1</guid></item><item><title>[arXiv-CR 2026] "Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs</title><link>https://arxiv.org/abs/2602.06759</link><description>arXiv:2602.06759v1 Announce Type: new 
Abstract: Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.
  In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06759v1</guid></item><item><title>[arXiv-CR 2026] Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs</title><link>https://arxiv.org/abs/2602.06777</link><description>arXiv:2602.06777v1 Announce Type: new 
Abstract: This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06777v1</guid></item><item><title>[arXiv-CR 2026] Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations</title><link>https://arxiv.org/abs/2602.06887</link><description>arXiv:2602.06887v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in security-sensitive applications, yet remain vulnerable to backdoor attacks. However, existing backdoor defenses are difficult to operationalize for Backdoor Defense-as-a-Service (BDaaS), as they require unrealistic side information (e.g., downstream clean data, known triggers/targets, or task domain specifics), and lack reusable, scalable purification across diverse backdoored models. In this paper, we present PROTOPURIFY, a backdoor purification framework via parameter edits under minimal assumptions. PROTOPURIFY first builds a backdoor vector pool from clean and backdoored model pairs, aggregates vectors into candidate prototypes, and selects the most aligned candidate for the target model via similarity matching. PROTOPURIFY then identifies a boundary layer through layer-wise prototype alignment and performs targeted purification by suppressing prototype-aligned components in the affected layers, achieving fine-grained mitigation with minimal impact on benign utility. Designed as a BDaaS-ready primitive, PROTOPURIFY supports reusability, customizability, interpretability, and runtime efficiency. Experiments across various LLMs on both classification and generation tasks show that PROTOPURIFY consistently outperforms 6 representative defenses against 6 diverse attacks, including single-trigger, multi-trigger, and triggerless backdoor settings. PROTOPURIFY reduces ASR to below 10%, and even as low as 1.6% in some cases, while incurring less than a 3% drop in clean utility. PROTOPURIFY further demonstrates robustness against adaptive backdoor variants and stability on non-backdoored models.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06887v1</guid></item><item><title>[arXiv-CR 2026] TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking</title><link>https://arxiv.org/abs/2602.06440</link><description>arXiv:2602.06440v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06440v1</guid></item><item><title>[arXiv-CR 2026] Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2507.02735</link><description>arXiv:2507.02735v3 Announce Type: replace 
Abstract: Prompt injection attacks, where untrusted data contains an injected prompt to manipulate the system, have been listed as the top security threat to LLM-integrated applications. Model-level prompt injection defenses have shown strong effectiveness, but the strongest defenses are proprietary. Open-source secure models are needed by the AI security community so that co-development of attacks and defenses through open research can drive scientific progress in mitigating prompt injection attacks. To this end, we develop Meta SecAlign, the first fully open-source LLM with built-in model-level defense that achieves commercial-grade performance and is powerful enough for complex agentic tasks. We provide complete details of our training recipe. We perform the most comprehensive evaluation to date on 9 utility benchmarks (measuring general knowledge, instruction following, and agentic workflows) and 7 security benchmarks. Results show that Meta SecAlign, despite being trained only on generic instruction-tuning samples, surprisingly confers security in unseen downstream tasks, including tool-calling and web-navigation, in addition to general instruction-following. Our best model -- Meta-SecAlign-70B -- establishes a new frontier of utility-security trade-off for open-source LLMs, and is more secure than several flagship proprietary models with prompt injection defense. Below are links for the code (https://github.com/facebookresearch/Meta_SecAlign), Meta-SecAlign-70B (https://huggingface.co/facebook/Meta-SecAlign-70B), and Meta-SecAlign-8B (https://huggingface.co/facebook/Meta-SecAlign-8B) models.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.02735v3</guid></item><item><title>[arXiv-CR 2026] Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness</title><link>https://arxiv.org/abs/2511.12085</link><description>arXiv:2511.12085v2 Announce Type: replace 
Abstract: Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.12085v2</guid></item><item><title>[arXiv-CR 2026] CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses</title><link>https://arxiv.org/abs/2602.01438</link><description>arXiv:2602.01438v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit secure prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01438v2</guid></item><item><title>[arXiv-CR 2026] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening</title><link>https://arxiv.org/abs/2602.05386</link><description>arXiv:2602.05386v2 Announce Type: replace 
Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05386v2</guid></item><item><title>[arXiv-SE 2026] SVRepair: Structured Visual Reasoning for Automated Program Repair</title><link>https://arxiv.org/abs/2602.06090</link><description>arXiv:2602.06090v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06090v1</guid></item><item><title>[arXiv-SE 2026] Coding Agents with Environment Interaction: A Theoretical Perspective</title><link>https://arxiv.org/abs/2602.06098</link><description>arXiv:2602.06098v1 Announce Type: new 
Abstract: Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06098v1</guid></item><item><title>[arXiv-SE 2026] AgentStepper: Interactive Debugging of Software Development Agents</title><link>https://arxiv.org/abs/2602.06593</link><description>arXiv:2602.06593v1 Announce Type: new 
Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06593v1</guid></item><item><title>[arXiv-SE 2026] Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience</title><link>https://arxiv.org/abs/2602.06831</link><description>arXiv:2602.06831v1 Announce Type: new 
Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06831v1</guid></item><item><title>[arXiv-SE 2026] TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code</title><link>https://arxiv.org/abs/2602.06875</link><description>arXiv:2602.06875v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06875v1</guid></item><item><title>[arXiv-SE 2026] SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development</title><link>https://arxiv.org/abs/2505.16975</link><description>arXiv:2505.16975v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks. However, feature-driven development, a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world end-to-end feature-driven software development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. We evaluated SWE-Dev across 17 base LLMs, 10 reasoning-focused LLMs, 10 multi-agent systems, and 8 tool-augmented LLM agents. Results show substantial headroom: the best single-turn model reaches only 22.51\% Pass@1 on the hard split, while OpenHands agents improve to 56.44\% but still leave many tasks unsolved. Code is available here https://github.com/DorothyDUUU/SWE-Dev.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.16975v3</guid></item><item><title>[arXiv-SE 2026] LLM-Based Repair of Static Nullability Errors</title><link>https://arxiv.org/abs/2507.20674</link><description>arXiv:2507.20674v2 Announce Type: replace 
Abstract: Modern Java projects increasingly adopt static analysis tools that prevent null-pointer exceptions by treating nullness as a type property. However, integrating such tools into large, existing codebases remains a significant challenge. While annotation inference can eliminate many errors automatically, a subset of residual errors -- typically a mix of real bugs and false positives -- often persist and can only be resolved via code changes. Manually addressing these errors is tedious and error-prone. Large language models (LLMs) offer a promising path toward automating these repairs, but naively-prompted LLMs often generate incorrect, contextually-inappropriate edits. We present NullRepair, a system that integrates LLMs into a structured workflow for resolving the errors from a nullability checker. NullRepair's decision process follows a flowchart derived from manual analysis of 200 real-world errors. It leverages static analysis to identify safe and unsafe usage regions of symbols, using error-free usage examples to contextualize model prompts. Patches are generated through an iterative interaction with the LLM that incorporates project-wide context and decision logic. Our evaluation on 12 real-world Java projects shows that NullRepair resolves 63% of the 1,119 nullability errors that remain after applying a state-of-the-art annotation inference technique. Unlike two baselines (single-shot prompt and mini-SWE-agent), NullRepair also largely preserves program semantics, with all unit tests passing in 10/12 projects after applying every edit proposed by NullRepair, and 98% or more tests passing in the remaining two projects.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.20674v2</guid></item><item><title>[arXiv-SE 2026] Context Engineering for AI Agents in Open-Source Software</title><link>https://arxiv.org/abs/2510.21413</link><description>arXiv:2510.21413v4 Announce Type: replace 
Abstract: GenAI-based coding assistants have disrupted software development. The next generation of these tools is agent-based, operating with more autonomy and potentially without human oversight. Like human developers, AI agents require contextual information to develop solutions that are in line with the standards, policies, and workflows of the software projects they operate in. Vendors of popular agentic tools (e.g., Claude Code) recommend maintaining version-controlled Markdown files that describe aspects such as the project structure, code style, or building and testing. The content of these files is then automatically added to each prompt. Recently, AGENTS$.$md has emerged as a potential standard that consolidates existing tool-specific formats. However, little is known about whether and how developers adopt this format. Therefore, in this paper, we present the results of a preliminary study investigating the adoption of AI context files in 466 open-source software projects. We analyze the information that developers provide in AGENTS$.$md files, how they present that information, and how the files evolve over time. Our findings indicate that there is no established content structure yet and that there is a lot of variation in terms of how context is provided (descriptive, prescriptive, prohibitive, explanatory, conditional). Our commit-level analysis provides first insights into the evolution of the provided context. AI context files provide a unique opportunity to study real-world context engineering. In particular, we see great potential in studying which structural or presentational modifications can positively affect the quality of the generated content.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.21413v4</guid></item><item><title>[arXiv-SE 2026] KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation</title><link>https://arxiv.org/abs/2511.14224</link><description>arXiv:2511.14224v2 Announce Type: replace 
Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.14224v2</guid></item><item><title>[arXiv-SE 2026] SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</title><link>https://arxiv.org/abs/2512.16956</link><description>arXiv:2512.16956v2 Announce Type: replace 
Abstract: Retrieving code functions, classes or files that are relevant in order to solve a given user query, bug report or feature request from large codebases is a fundamental challenge for Large Language Model (LLM)-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify semantically relevant units. While embedding-based approaches can outperform BM25 by large margins, they often don't take into consideration the underlying graph-structured characteristics of the codebase. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that integrates LLM-based reasoning along with auxiliary information obtained from graph-based exploration of the codebase. We further introduce SpIDER-Bench, a graph-structured evaluation benchmark curated from SWE-PolyBench, SWEBench-Verified and Multi-SWEBench, spanning codebases from Python, Java, JavaScript and TypeScript programming languages. Empirical results show that SpIDER consistently improves dense retrieval performance by at least 13% across programming languages and benchmarks in SpIDER-Bench.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.16956v2</guid></item><item><title>[arXiv-SE 2026] OmniCode: A Benchmark for Evaluating Software Engineering Agents</title><link>https://arxiv.org/abs/2602.02262</link><description>arXiv:2602.02262v2 Announce Type: replace 
Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02262v2</guid></item><item><title>[arXiv-SE 2026] SEAL: Symbolic Execution with Separation Logic (Competition Contribution)</title><link>https://arxiv.org/abs/2602.05703</link><description>arXiv:2602.05703v2 Announce Type: replace 
Abstract: SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05703v2</guid></item><item><title>[arXiv-SE 2026] Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs</title><link>https://arxiv.org/abs/2504.20406</link><description>arXiv:2504.20406v2 Announce Type: replace-cross 
Abstract: Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.20406v2</guid></item><item><title>[arXiv-SE 2026] code_transformed: The Influence of Large Language Models on Code</title><link>https://arxiv.org/abs/2506.12014</link><description>arXiv:2506.12014v2 Announce Type: replace-cross 
Abstract: Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 20,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake_case function names in Python code increased from 40.7% in Q1 2023 to 49.8% in Q3 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Our experimental results may provide the first large-scale empirical evidence that LLMs affect real-world programming style. We release all the experimental dataset and source code at: https://github.com/ignorancex/LLM_code</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.12014v2</guid></item><item><title>[arXiv-SE 2026] Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis</title><link>https://arxiv.org/abs/2510.10216</link><description>arXiv:2510.10216v2 Announce Type: replace-cross 
Abstract: Language models have shown remarkable proficiency in code generation; nevertheless, ensuring type correctness remains a challenge. Although traditional methods, such as constrained decoding, alleviate this problem by externally rejecting untypable code, the model itself does not effectively learn type reasoning internally, which ultimately limits its overall performance. This paper introduces TyFlow, a novel system that internalizes type reasoning within code generation to guide the model to learn the type system. The core of our approach is a novel type-guided program synthesis system that maintains an isomorphism between type derivation trees and synthesis derivation trees, enabling a new code representation based on synthesis decision sequences rather than traditional text-based token sequences. By offloading the complexity of type system learning to the representation itself, models can redirect their computational resources toward higher-level program semantics. Our evaluation shows that TyFlow not only eliminates type errors but also significantly improves functional correctness, highlighting the importance of aligning LMs with type systems internally.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.10216v2</guid></item><item><title>[arXiv-SE 2026] ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling</title><link>https://arxiv.org/abs/2602.03070</link><description>arXiv:2602.03070v3 Announce Type: replace-cross 
Abstract: Growing renewable penetration introduces substantial uncertainty into power system operations, necessitating frequent adaptation of dispatch objectives and constraints and challenging expertise-intensive, near-real-time modeling workflows. Large Language Models (LLMs) provide a promising avenue for automating this process by translating natural-language (NL) operational requirements into executable optimization models via semantic reasoning and code synthesis. Yet existing LLM datasets and benchmarks for optimization modeling primarily target coarse-grained cross-domain generalization, offering limited, rigorous evaluation in power-system settings, particularly for Optimal Power Flow (OPF). We therefore introduce \textbf{ProOPF-D} and \textbf{ProOPF-B}, a dataset and benchmark for professional-grade OPF modeling: ProOPF-D contains 12K instances pairing NL requests with parameter adjustments and structural extensions to a canonical OPF, together with executable implementations; ProOPF-B provides 121 expert-annotated test cases with ground-truth code, enabling end-to-end evaluation under both concrete and abstract OPF modeling regimes.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03070v3</guid></item><item><title>[arXiv-PL 2026] Uniqueness is Separation</title><link>https://arxiv.org/abs/2602.06386</link><description>arXiv:2602.06386v1 Announce Type: new 
Abstract: Value independence is enormously beneficial for reasoning about software systems at scale. These benefits carry over into the world of formal verification. Reasoning about programs algebraically is a simple affair in a proof assistant, whereas programs with unconstrained mutation necessitate much more complex techniques, such as Separation Logic, where invariants about memory safety, aliasing, and state changes must be established by manual proof. Uniqueness type systems allow programs to be compiled to code that uses mutation for efficiency, while retaining a semantics that enjoys value independence for reasoning. The restrictions of these type systems, however, are often too onerous for realistic software. Thus, most uniqueness type systems include some "escape hatch" where the benefits of value independence for reasoning are lost, but the restrictions of uniqueness types are lifted. To formally verify a system with such mixed guarantees, the value independence guarantees from uniqueness types must be expressed in terms of imperative, mutable semantics. In other words, we ought to express value independence as an assertion in Separation Logic.</description><author>cs.PL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06386v1</guid></item><item><title>[arXiv-PL 2026] Auditing Rust Crates Effectively</title><link>https://arxiv.org/abs/2602.06466</link><description>arXiv:2602.06466v1 Announce Type: new 
Abstract: We introduce Cargo Scan, the first interactive program analysis tool designed to help developers audit third-party Rust code. Real systems written in Rust rely on thousands of transitive dependencies. These dependencies are as dangerous in Rust as they are in other languages (e.g., C or JavaScript) -- and auditing these dependencies today means manually inspecting every line of code. Unlike for most industrial languages, though, we can take advantage of Rust's type and module system to minimize the amount of code that developers need to inspect to the code that is potentially dangerous. Cargo Scan models such potentially dangerous code as effects and performs a side-effects analysis, tailored to Rust, to identify effects and track them across crate and module boundaries. In most cases (69.2%) developers can inspect flagged effects and decide whether the code is potentially dangerous locally. In some cases, however, the safety of an effect depends on the calling context -- how a function is called, potentially by a crate the developer imports later. Hence, Cargo Scan tracks context-dependent information using a call-graph, and collects audit results into composable and reusable audit files. In this paper, we describe our experience auditing Rust crates with Cargo Scan. In particular, we audit the popular client and server HTTP crate, hyper, and all of its dependencies; our experience shows that Cargo Scan can reduce the auditing burden of potentially dangerous code to a median of 0.2% of lines of code when compared to auditing whole crates. Looking at the Rust ecosystem more broadly, we find that Cargo Scan can automatically classify ~3.5K of the top 10K crates on crates.io as safe; of the crates that do require manual inspection, we find that most of the potentially dangerous side-effects are concentrated in roughly 3% of these crates.</description><author>cs.PL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06466v1</guid></item><item><title>[arXiv-PL 2026] Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)</title><link>https://arxiv.org/abs/2602.06680</link><description>arXiv:2602.06680v1 Announce Type: new 
Abstract: Fixpoint iteration constitutes the algorithmic core of static analyzers. Parallelizing the fixpoint engine can significantly reduce analysis times. Previous approaches typically fix the granularity of tasks upfront, e.g., at the level of program threads or procedures - yielding an engine permanently stuck in one gear. Instead, we propose to parallelize a generic fixpoint engine in a way that is parametric in the task granularity - meaning that our engine can be run in different gears. We build on the top-down solver TD, extended with support for mixed-flow sensitivity, and realize two competing philosophies for parallelization, both building on a task pool that schedules tasks to a fixed number of workers. The nature of tasks differs between the philosophies. In the immediate approach, all tasks access a single thread-safe hash table maintaining solver state, while in the independent approach, each task has its own state and exchanges data with other tasks via a publish/subscribe data structure. We have equipped the fixpoint engine of the static analysis framework Goblint with implementations following both philosophies and report on our results for large real-world programs.</description><author>cs.PL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06680v1</guid></item><item><title>[arXiv-AI 2026] Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems</title><link>https://arxiv.org/abs/2602.06319</link><description>arXiv:2602.06319v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06319v1</guid></item><item><title>[arXiv-AI 2026] Difficulty-Estimated Policy Optimization</title><link>https://arxiv.org/abs/2602.06375</link><description>arXiv:2602.06375v1 Announce Type: new 
Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06375v1</guid></item><item><title>[arXiv-AI 2026] Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution</title><link>https://arxiv.org/abs/2602.06413</link><description>arXiv:2602.06413v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06413v1</guid></item><item><title>[arXiv-AI 2026] JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks</title><link>https://arxiv.org/abs/2602.06486</link><description>arXiv:2602.06486v1 Announce Type: new 
Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06486v1</guid></item><item><title>[arXiv-AI 2026] HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction</title><link>https://arxiv.org/abs/2602.06527</link><description>arXiv:2602.06527v1 Announce Type: new 
Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06527v1</guid></item><item><title>[arXiv-AI 2026] Same Answer, Different Representations: Hidden instability in VLMs</title><link>https://arxiv.org/abs/2602.06652</link><description>arXiv:2602.06652v1 Announce Type: new 
Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06652v1</guid></item><item><title>[arXiv-AI 2026] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</title><link>https://arxiv.org/abs/2602.06855</link><description>arXiv:2602.06855v1 Announce Type: new 
Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06855v1</guid></item><item><title>[arXiv-AI 2026] Agentic Uncertainty Reveals Agentic Overconfidence</title><link>https://arxiv.org/abs/2602.06948</link><description>arXiv:2602.06948v1 Announce Type: new 
Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06948v1</guid></item><item><title>[arXiv-AI 2026] Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions</title><link>https://arxiv.org/abs/2602.06256</link><description>arXiv:2602.06256v1 Announce Type: cross 
Abstract: Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to finetuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of whether interventions alter only the intended property remain limited, especially with respect to unintended changes in behaviors related to the target property. We call this notion specificity. We propose a framework that distinguishes three dimensions of specificity: general (preserving fluency and unrelated abilities), control (preserving related control properties), and robustness (preserving control properties under distribution shifts). We study two safety-critical use cases: steering models to reduce overrefusal and faithfulness hallucinations, and show that while steering achieves high efficacy and largely maintains general and control specificity, it consistently fails to preserve robustness specificity. In the case of overrefusal steering, for example, all steering methods reduce overrefusal without harming general abilities and refusal on harmful queries; however, they substantially increase vulnerability to jailbreaks. Our work provides the first systematic evaluation of specificity in model steering, showing that standard efficacy and specificity checks are insufficient, because without robustness evaluation, steering methods may appear reliable even when they compromise model safety.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06256v1</guid></item><item><title>[arXiv-AI 2026] Improve Large Language Model Systems with User Logs</title><link>https://arxiv.org/abs/2602.06470</link><description>arXiv:2602.06470v1 Announce Type: cross 
Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06470v1</guid></item><item><title>[arXiv-AI 2026] Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs</title><link>https://arxiv.org/abs/2602.06920</link><description>arXiv:2602.06920v1 Announce Type: cross 
Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06920v1</guid></item><item><title>[arXiv-AI 2026] Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics</title><link>https://arxiv.org/abs/2506.19385</link><description>arXiv:2506.19385v3 Announce Type: replace 
Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike traditional RAG systems that rely solely on semantic similarity (Conversation RAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic intent transition graphs from goal achieved historical dialogues and implements a dual-retrieval mechanism that adaptively balances intent-based graph traversal with semantic search. This approach enables the system to simultaneously leverage both conversional intent flow patterns and contextual semantics, significantly improving retrieval quality and response quality. In extensive experiments on real-world customer service dialogues, we employ both automatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG significantly outperforms both semantic-based Conversation RAG and intent-based GraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG demonstrates substantial improvements over Conversation RAG across automatic metrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and most notably, a 58% improvement in response quality according to LLM-as-judge evaluations. These results demonstrate that the integration of intent transition structures with semantic retrieval creates a synergistic effect that neither approach achieves independently, establishing CID-GraphRAG as an effective framework for addressing the challenges of maintaining contextual coherence and goal-oriented progression in knowledge-intensive multi-turn dialogues.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.19385v3</guid></item><item><title>[arXiv-AI 2026] GATSim: Urban Mobility Simulation with Generative Agents</title><link>https://arxiv.org/abs/2506.23306</link><description>arXiv:2506.23306v3 Announce Type: replace 
Abstract: Traditional agent-based urban mobility simulations often rely on rigid rulebased systems that struggle to capture the complexity, adaptability, and behavioral diversity inherent in human travel decision making. Inspired by recent advancements in large language models and AI agent technologies, we introduce GATSim, a novel framework that leverages these advancements to simulate urban mobility using generative agents with dedicated cognitive structures. GATSim agents are characterized by diverse socioeconomic profiles, individual lifestyles, and evolving preferences shaped through psychologically informed memory systems and lifelong learning. The main contributions of this work are: 1) a comprehensive architecture that integrates urban mobility foundation model with agent cognitive systems and transport simulation environment; 2) a hierarchical memory designed for efficient retrieval of contextually relevant information, incorporating spatial and temporal associations; 3) planning and reactive mechanisms for modeling adaptive mobility behaviors which integrate a multi-scale reflection process to transform specific travel experiences into generalized behavioral insights. Experiments indicate that generative agents perform competitively with human annotators in role-playing scenarios, while naturally producing realistic macroscopic traffic patterns. The code for the prototype implementation is publicly available at https://github.com/qiliuchn/gatsim.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.23306v3</guid></item><item><title>[arXiv-AI 2026] Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer</title><link>https://arxiv.org/abs/2511.17673</link><description>arXiv:2511.17673v4 Announce Type: replace 
Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Soft Symbolic Control constitutes a dedicated governance layer within SCL, applying symbolic constraints to probabilistic inference while preserving the flexibility of neural reasoning and restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.17673v4</guid></item><item><title>[arXiv-AI 2026] Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems</title><link>https://arxiv.org/abs/2512.15922</link><description>arXiv:2512.15922v2 Announce Type: replace 
Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution, as it depends on high-quality graph representations of the corpus. Such representations usually rely on manually curated knowledge graphs, which are costly to construct and update, or on automated graph-construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval. In this paper, we propose a novel RAG framework that uses a spreading activation algorithm to retrieve information from a corpus of documents connected by an automatically constructed heterogeneous knowledge graph. This approach reduces reliance on semantic knowledge graphs, which are often incomplete due to information loss during information extraction, avoids LLM-guided graph traversal, and improves performance on multi-hop question answering. Experiments show that our method achieves better or comparable performance to several state-of-the-art RAG methods and can be integrated as a plug-and-play module with different iterative RAG pipelines. When combined with chain-of-thought iterative retrieval, it yields up to a 39% absolute improvement in answer correctness over naive RAG, while achieving these results with small open-weight language models.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.15922v2</guid></item><item><title>[arXiv-AI 2026] UniRel: Relation-Centric Knowledge Graph Question Answering with RL-Tuned LLM Reasoning</title><link>https://arxiv.org/abs/2512.17043</link><description>arXiv:2512.17043v2 Announce Type: replace 
Abstract: Knowledge Graph Question Answering (KGQA) has largely focused on entity-centric queries that return a single answer entity. However, many real-world questions are inherently relational, aiming to understand how entities are associated rather than which entity satisfies a query. In this work, we introduce relation-centric KGQA, a complementary setting in which the answer is a subgraph that represents the semantic relations among entities. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel, a unified modular framework that combines a subgraph retriever with an LLM fine-tuned using reinforcement learning. The framework uses a reward function to prefer compact and specific subgraphs with informative relations and low-degree intermediate entities. Experiments show that UniRel improves connectivity and reward over Prompting baselines and generalizes well to unseen entities and relations. Moreover, UniRel can be applied to conventional entity-centric KGQA, achieving competitive or improved performance in several settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17043v2</guid></item><item><title>[arXiv-AI 2026] Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations</title><link>https://arxiv.org/abs/2602.00731</link><description>arXiv:2602.00731v2 Announce Type: replace 
Abstract: In this document we perform a systematic review of the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizability to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00731v2</guid></item><item><title>[arXiv-AI 2026] MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</title><link>https://arxiv.org/abs/2602.01539</link><description>arXiv:2602.01539v2 Announce Type: replace 
Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01539v2</guid></item><item><title>[arXiv-AI 2026] ExpressivityBench: Can LLMs Communicate Implicitly?</title><link>https://arxiv.org/abs/2411.08010</link><description>arXiv:2411.08010v2 Announce Type: replace-cross 
Abstract: Human communication is often implicit, conveying tone, identity, and intent beyond literal meanings. While large language models have achieved strong performance on explicit tasks such as summarization and reasoning, their capacity for expressivity, or implicit communication, remains underexplored. We introduce \textbf{ExpressivityBench}, a framework for evaluating the expressivity of LLMs using information-theoretic communication models. Our approach quantifies how well LLM-generated text communicates target properties without explicit mention, across nine tasks spanning emotion, identity, and tone. To enable scalable and reproducible evaluation, we employ LLM-based graders validated against human judgments. Our results reveal that while models are adept at expressing affective content, they struggle with sociolinguistic signals, lagging behind human baselines. This study provides a necessary step to evaluate human-like implicit communication, with implications for applications such as education, mental health support, and socially-aware dialogue systems. We provide code and data for our benchmark alongside our paper.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2411.08010v2</guid></item><item><title>[arXiv-AI 2026] Physics vs Distributions: Pareto Optimal Flow Matching with Physics Constraints</title><link>https://arxiv.org/abs/2506.08604</link><description>arXiv:2506.08604v3 Announce Type: replace-cross 
Abstract: Physics-constrained generative modeling aims to produce high-dimensional samples that are both physically consistent and distributionally accurate, a task that remains challenging due to often conflicting optimization objectives. Recent advances in flow matching and diffusion models have enabled efficient generative modeling, but integrating physical constraints often degrades generative fidelity or requires costly inference-time corrections. Our work is the first to recognize the trade-off between distributional and physical accuracy. Based on the insight of inherently conflicting objectives, we introduce Physics-Based Flow Matching (PBFM) a method that enforces physical constraints at training time using conflict-free gradient updates and unrolling to mitigate Jensen's gap. Our approach avoids manual loss balancing and enables simultaneous optimization of generative and physical objectives. As a consequence, physics constraints do not impede inference performance. We benchmark our method across three representative PDE benchmarks. PBFM achieves a Pareto-optimal trade-off, competitive inference speed, and generalizes to a wide range of physics-constrained generative tasks, providing a practical tool for scientific machine learning. Code and datasets available at https://github.com/tum-pbs/PBFM.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.08604v3</guid></item><item><title>[arXiv-AI 2026] Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine</title><link>https://arxiv.org/abs/2509.20975</link><description>arXiv:2509.20975v2 Announce Type: replace-cross 
Abstract: The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an in silico surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge - such as medical textbooks and biomedical knowledge graphs - can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce LLM-based Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.20975v2</guid></item><item><title>[arXiv-AI 2026] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning</title><link>https://arxiv.org/abs/2512.21446</link><description>arXiv:2512.21446v2 Announce Type: replace-cross 
Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies, limiting their parallel generation potential. Existing acceleration methods either rely on fixed confidence-based heuristics or use distillation-based approaches that finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra achieves superior accuracy-efficiency trade-offs compared to state-of-the-art heuristic (Fast-dLLM) and distillation baselines (d3LLM, dParallel), demonstrating that learned unmasking trajectories through on-policy RL enable better exploitation of parallel generation in MDLMs. Code and checkpoints are released at https://github.com/chinsengi/dUltra-os.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.21446v2</guid></item><item><title>[arXiv-AI 2026] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</title><link>https://arxiv.org/abs/2602.05885</link><description>arXiv:2602.05885v2 Announce Type: replace-cross 
Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05885v2</guid></item><item><title>[arXiv-LG 2026] Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems</title><link>https://arxiv.org/abs/2602.06426</link><description>arXiv:2602.06426v1 Announce Type: new 
Abstract: Open source software (OSS) projects rely on complex networks of contributors whose interactions drive innovation and sustainability. This study presents a comprehensive analysis of OSS contributor networks using advanced graph neural networks and temporal network analysis on data spanning 25 years from the Cloud Native Computing Foundation ecosystem, encompassing sandbox, incubating, and graduated projects. Our analysis of thousands of contributors across hundreds of repositories reveals that OSS networks exhibit strong power-law distributions in influence, with the top 1\% of contributors controlling a substantial portion of network influence. Using GPU-accelerated PageRank, betweenness centrality, and custom LSTM models, we identify five distinct contributor roles: Core, Bridge, Connector, Regular, and Peripheral, each with unique network positions and structural importance. Statistical analysis reveals significant correlations between specific action types (commits, pull requests, issues) and contributor influence, with multiple regression models explaining substantial variance in influence metrics. Temporal analysis shows that network density, clustering coefficients, and modularity exhibit statistically significant temporal trends, with distinct regime changes coinciding with major project milestones. Structural integrity simulations show that Bridge contributors, despite representing a small fraction of the network, have a disproportionate impact on network cohesion when removed. Our findings provide empirical evidence for strategic contributor retention policies and offer actionable insights into community health metrics.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06426v1</guid></item><item><title>[arXiv-LG 2026] Evolutionary Generation of Multi-Agent Systems</title><link>https://arxiv.org/abs/2602.06511</link><description>arXiv:2602.06511v1 Announce Type: new 
Abstract: Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06511v1</guid></item><item><title>[arXiv-LG 2026] MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs</title><link>https://arxiv.org/abs/2602.06268</link><description>arXiv:2602.06268v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06268v1</guid></item><item><title>[arXiv-LG 2026] VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model</title><link>https://arxiv.org/abs/2502.01989</link><description>arXiv:2502.01989v5 Announce Type: replace 
Abstract: Inspired by human SYSTEM 2 thinking, LLMs excel at complex reasoning tasks via extended Chain-of-Thought. However, similar test-time scaling for diffusion models to tackle complex reasoning remains largely unexplored. From existing work, two primary challenges emerge in this setting: (i) the dependence on an external verifier indicating a notable gap from intrinsic reasoning of human intelligence without any external feedback, and (ii) the lack of an efficient search algorithm. In this paper, we introduce the Verifier-free Test-time Scalable Diffusion Model (VFScale) to achieve scalable intrinsic reasoning, which equips number-of-sample test-time scaling with the intrinsic energy function of diffusion models as the verifier. Concretely, VFScale comprises two key innovations to address the aforementioned challenges. On the training side, VFScale consists of a novel MRNCL loss and a KL regularization to improve the energy landscape, ensuring that the learned energy function itself serves as a reliable verifier. On the inference side, VFScale integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS) to improve search efficiency. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of VFScale's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our VFScale solves 88% of Maze problems with much larger sizes of $15\times15$, while standard diffusion models completely fail. The code can be found at https://github.com/AI4Science-WestlakeU/VFScale.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2502.01989v5</guid></item><item><title>[arXiv-LG 2026] Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow</title><link>https://arxiv.org/abs/2504.02275</link><description>arXiv:2504.02275v2 Announce Type: replace 
Abstract: Credit card fraud has been a persistent issue since the last century, causing significant financial losses to the industry. The most effective way to prevent fraud is by contacting customers to verify suspicious transactions. However, while these systems are designed to detect fraudulent activity, they often mistakenly flag legitimate transactions, leading to unnecessary declines that disrupt the user experience and erode customer trust. Frequent false positives can frustrate customers, resulting in dissatisfaction, increased complaints, and a diminished sense of security. To address these limitations, we propose a fraud detection framework incorporating Relational Graph Convolutional Networks (RGCN) to enhance the accuracy and efficiency of identifying fraudulent transactions. By leveraging the relational structure of transaction data, our model reduces the need for direct customer confirmation while maintaining high detection performance. Our experiments are conducted using the IBM credit card transaction dataset to evaluate the effectiveness of this approach.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.02275v2</guid></item><item><title>[arXiv-LG 2026] CORE: Context-Robust Remasking for Diffusion Language Models</title><link>https://arxiv.org/abs/2602.04096</link><description>arXiv:2602.04096v2 Announce Type: replace 
Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CORE), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CORE identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CORE delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04096v2</guid></item><item><title>[arXiv-LG 2026] Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics</title><link>https://arxiv.org/abs/2602.04928</link><description>arXiv:2602.04928v2 Announce Type: replace 
Abstract: While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle to discover high-reward samples, resulting in data-inefficient and slow optimization. To address these limitations, we propose Euphonium, a novel framework that steers generation via process reward gradient guided dynamics. Our key insight is to formulate the sampling process as a theoretically principled Stochastic Differential Equation that explicitly incorporates the gradient of a Process Reward Model into the flow drift. This design enables dense, step-by-step steering toward high-reward regions, advancing beyond the unguided exploration in prior works, and theoretically encompasses existing sampling methods (e.g., Flow-GRPO, DanceGRPO) as special cases. We further derive a distillation objective that internalizes the guidance signal into the flow network, eliminating inference-time dependency on the reward model. We instantiate this framework with a Dual-Reward Group Relative Policy Optimization algorithm, combining latent process rewards for efficient credit assignment with pixel-level outcome rewards for final visual fidelity. Experiments on text-to-video generation show that Euphonium achieves better alignment compared to existing methods while accelerating training convergence by 1.66x. Our code is available at https://github.com/zerzerzerz/Euphonium</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04928v2</guid></item><item><title>[arXiv-LG 2026] BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs</title><link>https://arxiv.org/abs/2602.05448</link><description>arXiv:2602.05448v2 Announce Type: replace 
Abstract: Selecting the top $m$ from $n$ items via expensive $k$-wise comparisons is fundamental to settings ranging from LLM-based document reranking to crowdsourced evaluation and tournament design. Existing methods either rely on heuristics that fail to fully exploit the information each comparison reveals, or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise ranking. Our key observation is that each $k$-item comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences; aggregating these into a global preference graph and computing its transitive closure yields many additional orderings without further oracle calls. We formalize when an item's rank is certifiably determined and design a greedy query schedule that maximizes information gain towards identifying the top-$m$ items. The framework also gracefully handles non-transitive preferences (cycles induced by real-world oracles) by collapsing them into equivalence classes that yield principled tiered rankings. Applied to LLM reranking across 14 benchmarks and 5 models, our method achieves Pareto dominance over existing approaches: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable methods, and $7\times$ fewer than pairwise reranking at near-identical quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05448v2</guid></item><item><title>[arXiv-LG 2026] Estimating Semantic Alphabet Size for LLM Uncertainty Quantification</title><link>https://arxiv.org/abs/2509.14478</link><description>arXiv:2509.14478v2 Announce Type: replace-cross 
Abstract: Many black-box techniques for quantifying the uncertainty of large language models (LLMs) rely on repeated LLM sampling, which can be computationally expensive. Therefore, practical applicability demands reliable estimation from few samples. Semantic entropy (SE) is a popular sample-based uncertainty estimator with a discrete formulation attractive for the black-box setting. Recent extensions of SE exhibit improved LLM hallucination detection, but do so with less interpretable methods that admit additional hyperparameters. For this reason, we revisit the canonical discrete semantic entropy (DSE) estimator, finding that it underestimates the "true" semantic entropy, as expected from theory. We propose a modified semantic alphabet size estimator, and illustrate that using it to adjust DSE for sample coverage results in more accurate SE estimation in our setting of interest. Furthermore, we find that two semantic alphabet size estimators, including our proposed, flag incorrect LLM responses as well or better than many top-performing alternatives, with the added benefit of remaining highly interpretable.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.14478v2</guid></item><item><title>[arXiv-LG 2026] MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</title><link>https://arxiv.org/abs/2601.02075</link><description>arXiv:2601.02075v4 Announce Type: replace-cross 
Abstract: Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.02075v4</guid></item><item><title>[arXiv-CL 2026] Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering</title><link>https://arxiv.org/abs/2602.06050</link><description>arXiv:2602.06050v1 Announce Type: new 
Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06050v1</guid></item><item><title>[arXiv-CL 2026] CAST: Character-and-Scene Episodic Memory for Agents</title><link>https://arxiv.org/abs/2602.06051</link><description>arXiv:2602.06051v1 Announce Type: new 
Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06051v1</guid></item><item><title>[arXiv-CL 2026] Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production</title><link>https://arxiv.org/abs/2602.06370</link><description>arXiv:2602.06370v1 Announce Type: new 
Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06370v1</guid></item><item><title>[arXiv-CL 2026] Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning</title><link>https://arxiv.org/abs/2602.06600</link><description>arXiv:2602.06600v1 Announce Type: new 
Abstract: Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $\Delta\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06600v1</guid></item><item><title>[arXiv-CL 2026] Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion</title><link>https://arxiv.org/abs/2602.06724</link><description>arXiv:2602.06724v1 Announce Type: new 
Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06724v1</guid></item><item><title>[arXiv-CL 2026] DAWN: Dependency-Aware Fast Inference for Diffusion LLMs</title><link>https://arxiv.org/abs/2602.06953</link><description>arXiv:2602.06953v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06953v1</guid></item><item><title>[arXiv-CL 2026] A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering</title><link>https://arxiv.org/abs/2602.05512</link><description>arXiv:2602.05512v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.</description><author>cs.CL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05512v2</guid></item><item><title>[ASE 2026] Vul-R2: A Reasoning LLM for Automated Vulnerability Repair.</title><link>https://doi.org/10.1109/ASE63991.2025.00011</link><description>Authors: Xin-Cheng Wen, Zirui Lin, Yijun Yang, Cuiyun Gao 0001, Deheng Ye
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WenLYGY25</guid></item><item><title>[ASE 2026] Wired for Reuse: Automating Context-Aware Code Adaptation in IDEs via LLM-Based Agent.</title><link>https://doi.org/10.1109/ASE63991.2025.00023</link><description>Authors: Taiming Wang, Yanjie Jiang, Chunhao Dong, Yuxia Zhang, Hui Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangJDZL25</guid></item><item><title>[ASE 2026] Towards More Accurate Static Analysis for Taint-Style Bug Detection in Linux Kernel.</title><link>https://doi.org/10.1109/ASE63991.2025.00039</link><description>Authors: Haonan Li, Hang Zhang 0012, Kexin Pei, Zhiyun Qian
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiZPQ25</guid></item><item><title>[ASE 2026] GlassWing: A Tailored Static Analysis Approach for Flutter Android Apps.</title><link>https://doi.org/10.1109/ASE63991.2025.00050</link><description>Authors: Xiangyu Zhang, Yucheng Su, Lingling Fan 0003, Miaoying Cai, Sen Chen 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangSFCC25</guid></item><item><title>[ASE 2026] Security Debt in LLM Agent Applications: A Measurement Study of Vulnerabilities and Mitigation Trade-offs.</title><link>https://doi.org/10.1109/ASE63991.2025.00053</link><description>Authors: Zhuoxiang Shen, Jiarun Dai, Yuan Zhang 0009, Min Yang 0002
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ShenDZY25</guid></item><item><title>[ASE 2026] RustRepoTrans: Repository-level Context Code Translation Benchmark Targeting Rust.</title><link>https://doi.org/10.1109/ASE63991.2025.00057</link><description>Authors: Guangsheng Ou, Mingwei Liu 0002, Yuxuan Chen, Yanlin Wang 0001, Xin Peng 0001, Zibin Zheng
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/OuLCWPZ25</guid></item><item><title>[ASE 2026] BinStruct: Binary Structure Recovery Combining Static Analysis and Semantics.</title><link>https://doi.org/10.1109/ASE63991.2025.00060</link><description>Authors: Yiran Zhang, Zhengzi Xu, Zhe Lang, Chengyue Liu, Yuqiang Sun 0001, Wenbo Guo, Chengwei Liu, Weisong Sun, Yang Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangXLLSGLSL25</guid></item><item><title>[ASE 2026] Triangle: Empowering Incident Triage with Multi-Agent.</title><link>https://doi.org/10.1109/ASE63991.2025.00062</link><description>Authors: Zhaoyang Yu 0002, Aoyang Fang, Minghua Ma, Jaskaran Singh Walia, Chaoyun Zhang, Shu Chi, Ze Li 0005, Murali Chintalapati, Xuchao Zhang, Rujia Wang, Chetan Bansal, Saravan Rajmohan, Qingwei Lin, Shenglin Zhang, Dan Pei, Pinjia He
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/YuFMWZCLCZWBRLZPH25</guid></item><item><title>[ASE 2026] Hit The Bullseye On The First Shot: Improving LLMs Using Multi-Sample Self-Reward Feedback for Vulnerability Repair.</title><link>https://doi.org/10.1109/ASE63991.2025.00071</link><description>Authors: Rui Jiao, Yue Zhang, Jinku Li, Jianfeng Ma
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiaoZLM25</guid></item><item><title>[ASE 2026] Exploring Static Taint Analysis in LLMs: A Dynamic Benchmarking Framework for Measurement and Enhancement.</title><link>https://doi.org/10.1109/ASE63991.2025.00082</link><description>Authors: Haoran Zhao, Lei Zhang 0006, Keke Lian, Fute Sun, Bofei Chen, Yongheng Liu, Zhiyu Wu, Yuan Zhang 0009, Min Yang 0002
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoZLSCLWZY25</guid></item><item><title>[ASE 2026] AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion.</title><link>https://doi.org/10.1109/ASE63991.2025.00085</link><description>Authors: Tianyue Jiang, Yanlin Wang 0001, Yanli Wang 0001, Daya Guo, Ensheng Shi, Yuchi Ma, Jiachi Chen, Zibin Zheng
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiangWWGSMCZ25</guid></item><item><title>[ASE 2026] Not Every Patch is an Island: LLM-Enhanced Identification of Multiple Vulnerability Patches.</title><link>https://doi.org/10.1109/ASE63991.2025.00087</link><description>Authors: Yi Song, Dongchen Xie, Lin Xu, He Zhang, Chunying Zhou, Xiaoyuan Xie
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/SongXXZZX25</guid></item><item><title>[ASE 2026] LOSVER: Line-Level Modifiability Signal-Guided Vulnerability Detection and Classification.</title><link>https://doi.org/10.1109/ASE63991.2025.00092</link><description>Authors: Doha Nam, Jongmoon Baik
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/NamB25</guid></item><item><title>[ASE 2026] Aligning LLMs to Fully Utilize the Cross-file Context in Repository-level Code Completion.</title><link>https://doi.org/10.1109/ASE63991.2025.00125</link><description>Authors: Jia Li 0012, Hao Zhu, Huanyu Liu 0001, Xianjie Shi, He Zong, Yihong Dong, Kechi Zhang, Siyuan Jiang, Zhi Jin 0001, Ge Li 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiZLSZDZJJL25</guid></item><item><title>[ASE 2026] Belief Propagation with Local Structure and Its Applications in Program Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00132</link><description>Authors: Yiqian Wu, Yifan Chen, Yingfei Xiong 0001, Xin Zhang 0035
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuCXZ25</guid></item><item><title>[ASE 2026] Leveraging Mixture-of-Experts Framework for Smart Contract Vulnerability Repair with Large Language Model.</title><link>https://doi.org/10.1109/ASE63991.2025.00140</link><description>Authors: Hang Yuan, Xizhi Hou, Lei Yu, Li Yang, Jiayue Tang, Jiadong Xu, Yifei Liu, Fengjun Zhang, Chun Zuo
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/YuanHYYTXLZZ25</guid></item><item><title>[ASE 2026] Fixing Broken Graphs: LLM-Powered Automatic Code Optimization for DNN Programs.</title><link>https://doi.org/10.1109/ASE63991.2025.00144</link><description>Authors: Haotian Wang, Yicheng Sui, Yudong Xie, Yicong Liu, Yufei Sun, Changqing Shi, Yuzhi Zhang
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangSXLSSZ25</guid></item><item><title>[ASE 2026] PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing.</title><link>https://doi.org/10.1109/ASE63991.2025.00153</link><description>Authors: Xiaoxue Ren, Jun Wan, Yun Peng, Zhongxin Liu 0002, Ming Liang, Dajun Chen, Wei Jiang, Yong Li
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/RenWPLLCJL25</guid></item><item><title>[ASE 2026] Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning.</title><link>https://doi.org/10.1109/ASE63991.2025.00161</link><description>Authors: Xin Wang, Zhenhao Li, Zishuo Ding
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangLD25</guid></item><item><title>[ASE 2026] Have We Solved Access Control Vulnerability Detection in Smart Contracts? A Benchmark Study.</title><link>https://doi.org/10.1109/ASE63991.2025.00166</link><description>Authors: Han Liu 0012, Daoyuan Wu, Yuqiang Sun 0001, Shuai Wang 0011, Yang Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiuWSWL25</guid></item><item><title>[ASE 2026] Interpretable Vulnerability Detection Reports.</title><link>https://doi.org/10.1109/ASE63991.2025.00168</link><description>Authors: Cludia Mamede, Jos Campos 0001, Claire Le Goues, Rui Abreu 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/MamedeCGA25</guid></item><item><title>[ASE 2026] Towards Generalizable Instruction Vulnerability Prediction via LLM-Enhanced Code Representation.</title><link>https://doi.org/10.1109/ASE63991.2025.00177</link><description>Authors: Bao Wen, Jingjing Gu, Jingxuan Zhang, Yang Liu, Pengfei Yu, Yanchao Zhao
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WenGZLYZ25</guid></item><item><title>[ASE 2026] FastCoder: Accelerating Repository-level Code Generation via Efficient Retrieval and Verification.</title><link>https://doi.org/10.1109/ASE63991.2025.00190</link><description>Authors: Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Jia Li, Lin Shi
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoZLLMJZLS25</guid></item><item><title>[ASE 2026] SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation.</title><link>https://doi.org/10.1109/ASE63991.2025.00192</link><description>Authors: Gustavo Ansaldi Oliva, Gopi Krishnan Rajbahadur, Aaditya Bhatia, Haoxiang Zhang 0001, Yihao Chen, Zhilong Chen, Arthur Leung, Dayi Lin, Boyuan Chen 0002, Ahmed E. Hassan
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/OlivaRBZCCLLCH25</guid></item><item><title>[ASE 2026] Incremental Program Analysis in the Wild: An Empirical Study on Real-World Program Changes.</title><link>https://doi.org/10.1109/ASE63991.2025.00194</link><description>Authors: Xizao Wang, Xiangrong Bin, Lanxin Huang, Shangqing Liu, Jianhua Zhao, Lei Bu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangBHLZB25</guid></item><item><title>[ASE 2026] An Agent-based Evaluation Framework for Complex Code Generation.</title><link>https://doi.org/10.1109/ASE63991.2025.00200</link><description>Authors: Xinchen Wang, Ruida Hu, Pengfei Gao, Chao Peng 0002, Cuiyun Gao 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangHGPG25</guid></item><item><title>[ASE 2026] ACTaint: Agent-Based Taint Analysis for Access Control Vulnerabilities in Smart Contracts.</title><link>https://doi.org/10.1109/ASE63991.2025.00210</link><description>Authors: Huarui Lin, Zhipeng Gao 0002, Jiachi Chen, Xiang Chen 0005, Xiaohu Yang 0001, Lingfeng Bao
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LinGCCYB25</guid></item><item><title>[ASE 2026] PALM: Synergizing Program Analysis and LLMs to Enhance Rust Unit Test Coverage.</title><link>https://doi.org/10.1109/ASE63991.2025.00223</link><description>Authors: Bei Chu, Yang Feng 0003, Kui Liu 0001, Hange Shi, Zifan Nan, Zhaoqiang Guo, Baowen Xu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ChuFLSNGX25</guid></item><item><title>[ASE 2026] LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM.</title><link>https://doi.org/10.1109/ASE63991.2025.00245</link><description>Authors: Yuxin Zhang, Yuxia Zhang, Zeyu Sun 0004, Yanjie Jiang, Hui Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangZSJL25</guid></item><item><title>[ASE 2026] Issue Localization via LLM-Driven Iterative Code Graph Searching.</title><link>https://doi.org/10.1109/ASE63991.2025.00249</link><description>Authors: Zhonghao Jiang, Xiaoxue Ren, Meng Yan 0001, Wei Jiang, Yong Li, Zhongxin Liu 0002
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiangRYJLL25</guid></item><item><title>[ASE 2026] HarmoBridge: Bridging ArkTS and C/C++ for Cross-Language Static Analysis on HarmonyOS.</title><link>https://doi.org/10.1109/ASE63991.2025.00261</link><description>Authors: Jiale Wu, Jiapeng Deng, Yanjie Zhao, Li Li, Haoyu Wang
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuDZLW25</guid></item><item><title>[ASE 2026] iCodeReviewer: Improving Secure Code Review with Mixture of Prompts.</title><link>https://doi.org/10.1109/ASE63991.2025.00264</link><description>Authors: Yun Peng, Kisub Kim, Linghan Meng, Kui Liu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/PengKML25</guid></item><item><title>[ASE 2026] Should We Evaluate LLM Based Security Analysis Approaches on Open Source Systems?</title><link>https://doi.org/10.1109/ASE63991.2025.00265</link><description>Authors: Kohei Dozono, Jonas Engesser, Benjamin Hummel, Tobias Roehm, Alexander Pretschner
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/DozonoEHRP25</guid></item><item><title>[ASE 2026] Securing Millions of Decentralized Identities in Alipay Super App with End-to-End Formal Verification.</title><link>https://doi.org/10.1109/ASE63991.2025.00305</link><description>Authors: Ziyu Mao, Xiaolin Ma, Lin Huang, Huan Yang, Wu Zhang, Weichao Sun, Yongtao Wang, Jingling Xue, Jingyi Wang
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/MaoMHYZSWXW25</guid></item><item><title>[ASE 2026] What Types of Code Review Comments Do Developers Most Frequently Resolve?</title><link>https://doi.org/10.1109/ASE63991.2025.00312</link><description>Authors: Saul Goldman, Hong Yi Lin, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Kla Tantithamthavorn, Zhe Wang, Ray Zhang 0004, Ali Behnaz, Fan Jiang, Michael Siers, Ryan Jiang, Mike Buller, Minwoo Jeong, Ming Wu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/GoldmanLPTTWZBJSJBJW25</guid></item><item><title>[ASE 2026] SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review.</title><link>https://doi.org/10.1109/ASE63991.2025.00315</link><description>Authors: Kai Wang, Bingcheng Mao, Shuai Jia, Yujie Ding, Dongming Han, Tianyi Ma, Bin Cao
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangMJDHMC25</guid></item><item><title>[ASE 2026] ConfuseTaint: Exploiting Vulnerabilities to Bypass Dynamic Taint Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00340</link><description>Authors: Yufei Wu, Alexandre Bartel
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuB25</guid></item><item><title>[ASE 2026] Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision.</title><link>https://doi.org/10.1109/ASE63991.2025.00345</link><description>Authors: Xu Lu, Weisong Sun, Yiran Zhang, Ming Hu 0003, Cong Tian, Zhi Jin 0001, Yang Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LuSZHTJL25</guid></item><item><title>[ASE 2026] STaint: Detecting Second-Order Vulnerabilities in PHP Applications with LLM-Assisted Bi-Directional Static Taint Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00347</link><description>Authors: Yuchen Ji, Hongchen Cao, Jingzhu He
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiCH25</guid></item><item><title>[ASE 2026] VUSC: An Extensible Research Platform for Java-Based Static Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00354</link><description>Authors: Marc Miltenberger, Steven Arzt
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/MiltenbergerA25</guid></item><item><title>[ASE 2026] A Large-Scale Evolvable Dataset for Model Context Protocol Ecosystem and Security Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00356</link><description>Authors: Zhiwei Lin, Bonan Ruan, Jiahao Liu 0005, Weibo Zhao
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LinRLZ25</guid></item><item><title>[ASE 2026] Secure Transaction Semantics: Analysis, Vulnerability Detection, and Attack Modeling.</title><link>https://doi.org/10.1109/ASE63991.2025.00398</link><description>Authors: Yixuan Liu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Liu25</guid></item><item><title>[TDSC 2026] Interaction-Aware Vulnerability Detection in Smart Contract Bytecodes.</title><link>https://doi.org/10.1109/TDSC.2025.3605773</link><description>Authors: Wenkai Li, Xiaoqi Li 0001, Yingjie Mao, Yuqing Zhang 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/LiLMZ26</guid></item><item><title>[TDSC 2026] Pruning Attention Heads Based on Semantic and Code Structure for Smart Contract Vulnerability Detection.</title><link>https://doi.org/10.1109/TDSC.2025.3607471</link><description>Authors: Siyu Jiang, Yuwen Chen, Teng Ouyang, Xue Zhang, Shen Su
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/JiangCOZS26</guid></item><item><title>[TDSC 2026] Alert2Vec: Eliminating Alert Fatigue by Embedding Security Alerts Through Subgraph Learning.</title><link>https://doi.org/10.1109/TDSC.2025.3609834</link><description>Authors: Songyun Wu, Xiaoqing Sun, Enhuan Dong, Zhiliang Wang, Chen Zhao, Jiahai Yang 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/WuSDWZY26</guid></item><item><title>[TDSC 2026] Nonstandard Sinks Matter: A Comprehensive and Efficient Taint Analysis Framework for Vulnerability Detection in Embedded Firmware.</title><link>https://doi.org/10.1109/TDSC.2025.3619200</link><description>Authors: Enzhou Song, Yuhao Zhao, Can Zhang, Jinyuan Zhai, Ruijie Cai, Long Liu, Qichao Yang, Xiaokang Yin 0001, Shengli Liu 0003
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/SongZZZCLYYL26</guid></item><item><title>[C&amp;S 2026] SemTaint: A scalable taint analysis approach for JavaWeb frameworks and composite containers</title><link>https://www.sciencedirect.com/science/article/pii/S0167404825005103?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers &amp; Security, Volume 163&lt;/p&gt;&lt;p&gt;Author(s): Haotian Huang, Ruibin Yan, Jian Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Computers &amp; Security</author><pubDate>Sat, 07 Feb 2026 16:45:49 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167404825005103</guid></item><item><title>[SCP 2026] DATVD: A novel vulnerability detection method based on dynamic attention and hybrid convolutional pooling</title><link>https://www.sciencedirect.com/science/article/pii/S0167642326000080?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Science of Computer Programming, Volume 251&lt;/p&gt;&lt;p&gt;Author(s): Jinfu Chen, Jinyu Mu, Saihua Cai, Jiapeng Zhou, Ziyan Liu, Xinping Shi&lt;/p&gt;</description><author>ScienceDirect Publication: Science of Computer Programming</author><pubDate>Sat, 07 Feb 2026 16:45:23 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167642326000080</guid></item><item><title>[JSS 2026] RLV: LLM-based vulnerability detection by retrieving and refining contextual information</title><link>https://www.sciencedirect.com/science/article/pii/S016412122500425X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 235&lt;/p&gt;&lt;p&gt;Author(s): Fangcheng Qiu, Zhongxin Liu, Bingde Hu, Zhengong Cai, Lingfeng Bao, Xinyu Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Sat, 07 Feb 2026 16:45:22 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S016412122500425X</guid></item><item><title>[JSS 2026] Clash: Enhancing context-sensitivity in data-flow analysis for mitigating the impact of indirect calls</title><link>https://www.sciencedirect.com/science/article/pii/S0164121225004224?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 235&lt;/p&gt;&lt;p&gt;Author(s): Jinyan Xie, Yingzhou Zhang, Mingzhe Hu, Liping Han, Le Yu, Qiuran Ding&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Sat, 07 Feb 2026 16:45:22 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0164121225004224</guid></item><item><title>[IST 2026] COTVD: A function-level vulnerability detection framework using chain-of-thought reasoning with large language models</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000327?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 193&lt;/p&gt;&lt;p&gt;Author(s): Yinan Chen, Xiangping Chen, Yuan Huang, Changlin Yang, Lei Yun&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000327</guid></item><item><title>[IST 2026] EdgeSim: Firmware vulnerability detection with control transfer-enhanced binary code similarity detection</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000091?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Li Liu, Shen Wang, Xunzhi Jiang&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000091</guid></item><item><title>[IST 2026] CSVD-AES: Cross-project software vulnerability detection based on active learning with metric fusion</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Zhidan Yuan, Xiang Chen, Juan Zhang, Weiming Zeng&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000042</guid></item><item><title>[IST 2026] VulSEG: Enhanced graph-based vulnerability detection system with advanced text embedding</title><link>https://www.sciencedirect.com/science/article/pii/S0950584925003465?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Wenjing Cai, Xin Liu, Lipeng Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584925003465</guid></item><item><title>[arXiv-AI 2026] Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education</title><link>https://arxiv.org/abs/2602.05059</link><description>arXiv:2602.05059v1 Announce Type: new 
Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05059v1</guid></item><item><title>[arXiv-AI 2026] Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment</title><link>https://arxiv.org/abs/2602.05110</link><description>arXiv:2602.05110v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05110v1</guid></item><item><title>[arXiv-AI 2026] Hallucination-Resistant Security Planning with a Large Language Model</title><link>https://arxiv.org/abs/2602.05279</link><description>arXiv:2602.05279v1 Announce Type: new 
Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05279v1</guid></item><item><title>[arXiv-AI 2026] Aspect-Aware MOOC Recommendation in a Heterogeneous Network</title><link>https://arxiv.org/abs/2602.05297</link><description>arXiv:2602.05297v1 Announce Type: new 
Abstract: MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05297v1</guid></item><item><title>[arXiv-AI 2026] ProAct: Agentic Lookahead in Interactive Environments</title><link>https://arxiv.org/abs/2602.05327</link><description>arXiv:2602.05327v1 Announce Type: new 
Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05327v1</guid></item><item><title>[arXiv-AI 2026] Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation</title><link>https://arxiv.org/abs/2602.05381</link><description>arXiv:2602.05381v1 Announce Type: new 
Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05381v1</guid></item><item><title>[arXiv-AI 2026] Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning</title><link>https://arxiv.org/abs/2602.05464</link><description>arXiv:2602.05464v1 Announce Type: new 
Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05464v1</guid></item><item><title>[arXiv-AI 2026] ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</title><link>https://arxiv.org/abs/2602.05472</link><description>arXiv:2602.05472v1 Announce Type: new 
Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05472v1</guid></item><item><title>[arXiv-AI 2026] Graph-based Agent Memory: Taxonomy, Techniques, and Applications</title><link>https://arxiv.org/abs/2602.05665</link><description>arXiv:2602.05665v1 Announce Type: new 
Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05665v1</guid></item><item><title>[arXiv-AI 2026] LeakBoost: Perceptual-Loss-Based Membership Inference Attack</title><link>https://arxiv.org/abs/2602.05748</link><description>arXiv:2602.05748v1 Announce Type: new 
Abstract: Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05748v1</guid></item><item><title>[arXiv-AI 2026] RocqSmith: Can Automatic Optimization Forge Better Proof Agents?</title><link>https://arxiv.org/abs/2602.05762</link><description>arXiv:2602.05762v1 Announce Type: new 
Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05762v1</guid></item><item><title>[arXiv-AI 2026] TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05818</link><description>arXiv:2602.05818v1 Announce Type: new 
Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05818v1</guid></item><item><title>[arXiv-AI 2026] Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy</title><link>https://arxiv.org/abs/2602.05877</link><description>arXiv:2602.05877v1 Announce Type: new 
Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05877v1</guid></item><item><title>[arXiv-AI 2026] AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</title><link>https://arxiv.org/abs/2602.06008</link><description>arXiv:2602.06008v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06008v1</guid></item><item><title>[arXiv-AI 2026] DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</title><link>https://arxiv.org/abs/2602.06039</link><description>arXiv:2602.06039v1 Announce Type: new 
Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06039v1</guid></item><item><title>[arXiv-AI 2026] Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction</title><link>https://arxiv.org/abs/2602.04892</link><description>arXiv:2602.04892v1 Announce Type: cross 
Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04892v1</guid></item><item><title>[arXiv-AI 2026] A Causal Perspective for Enhancing Jailbreak Attack and Defense</title><link>https://arxiv.org/abs/2602.04893</link><description>arXiv:2602.04893v1 Announce Type: cross 
Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04893v1</guid></item><item><title>[arXiv-AI 2026] Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software</title><link>https://arxiv.org/abs/2602.04894</link><description>arXiv:2602.04894v1 Announce Type: cross 
Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04894v1</guid></item><item><title>[arXiv-AI 2026] Internalizing LLM Reasoning via Discovery and Replay of Latent Actions</title><link>https://arxiv.org/abs/2602.04925</link><description>arXiv:2602.04925v1 Announce Type: cross 
Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04925v1</guid></item><item><title>[arXiv-AI 2026] E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</title><link>https://arxiv.org/abs/2602.05068</link><description>arXiv:2602.05068v1 Announce Type: cross 
Abstract: Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $\epsilon-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05068v1</guid></item><item><title>[arXiv-AI 2026] EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering</title><link>https://arxiv.org/abs/2602.05242</link><description>arXiv:2602.05242v1 Announce Type: cross 
Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05242v1</guid></item><item><title>[arXiv-AI 2026] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening</title><link>https://arxiv.org/abs/2602.05386</link><description>arXiv:2602.05386v1 Announce Type: cross 
Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05386v1</guid></item><item><title>[arXiv-AI 2026] LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation</title><link>https://arxiv.org/abs/2602.05493</link><description>arXiv:2602.05493v1 Announce Type: cross 
Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05493v1</guid></item><item><title>[arXiv-AI 2026] Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations</title><link>https://arxiv.org/abs/2602.05523</link><description>arXiv:2602.05523v1 Announce Type: cross 
Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05523v1</guid></item><item><title>[arXiv-AI 2026] Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes</title><link>https://arxiv.org/abs/2602.05780</link><description>arXiv:2602.05780v1 Announce Type: cross 
Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05780v1</guid></item><item><title>[arXiv-AI 2026] DARWIN: Dynamic Agentically Rewriting Self-Improving Network</title><link>https://arxiv.org/abs/2602.05848</link><description>arXiv:2602.05848v1 Announce Type: cross 
Abstract: DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05848v1</guid></item><item><title>[arXiv-AI 2026] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</title><link>https://arxiv.org/abs/2602.05885</link><description>arXiv:2602.05885v1 Announce Type: cross 
Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05885v1</guid></item><item><title>[arXiv-AI 2026] CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</title><link>https://arxiv.org/abs/2602.06038</link><description>arXiv:2602.06038v1 Announce Type: cross 
Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06038v1</guid></item><item><title>[arXiv-AI 2026] Robust Answers, Fragile Logic: Probing the Decoupling Hypothesis in LLM Reasoning</title><link>https://arxiv.org/abs/2505.17406</link><description>arXiv:2505.17406v2 Announce Type: replace 
Abstract: While Chain-of-Thought (CoT) prompting has become a cornerstone for complex reasoning in Large Language Models (LLMs), the faithfulness of the generated reasoning remains an open question. We investigate the Decoupling Hypothesis: that correct answers often mask fragile, post-hoc rationalizations that are not causally tied to the model's prediction. To systematically verify this, we introduce MATCHA, a novel Answer-Conditioned Probing framework. Unlike standard evaluations that focus on final output accuracy, MATCHA isolates the reasoning phase by conditioning generation on the model's predicted answer, allowing us to stress-test the stability of the rationale itself. Our experiments reveal a critical vulnerability: under imperceptible input perturbations, LLMs frequently maintain the correct answer while generating inconsistent or nonsensical reasoning - effectively being ``Right for the Wrong Reasons''. Using LLM judges to quantify this robustness gap, we find that multi-step and commonsense tasks are significantly more susceptible to this decoupling than logical tasks. Furthermore, we demonstrate that adversarial examples generated by MATCHA transfer non-trivially to black-box models. Our findings expose the illusion of CoT robustness and underscore the need for future architectures that enforce genuine answer-reasoning consistency rather than mere surface-level accuracy.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17406v2</guid></item><item><title>[arXiv-AI 2026] Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers</title><link>https://arxiv.org/abs/2506.13342</link><description>arXiv:2506.13342v2 Announce Type: replace 
Abstract: Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend that future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.13342v2</guid></item><item><title>[arXiv-AI 2026] How Catastrophic is Your LLM? Certifying Risk in Conversation</title><link>https://arxiv.org/abs/2510.03969</link><description>arXiv:2510.03969v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose C$^3$LLM, a novel, principled statistical Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions--random node, graph path, and adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.03969v3</guid></item><item><title>[arXiv-AI 2026] DeepAgent: A General Reasoning Agent with Scalable Toolsets</title><link>https://arxiv.org/abs/2510.21618</link><description>arXiv:2510.21618v3 Announce Type: replace 
Abstract: Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To manage long-horizon interactions, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.21618v3</guid></item><item><title>[arXiv-AI 2026] The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution</title><link>https://arxiv.org/abs/2601.15075</link><description>arXiv:2601.15075v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining \textbf{the reason behind agent behaviors}. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems. Codes are available at https://github.com/AI45Lab/AgentDoG.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15075v2</guid></item><item><title>[arXiv-AI 2026] A Study of Adaptive Modeling Towards Robust Generalization</title><link>https://arxiv.org/abs/2602.02780</link><description>arXiv:2602.02780v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly support reasoning over biomolecular structures, but most existing approaches remain modality-specific and rely on either sequence-style encodings or fixed-length connector tokens for structural inputs. These designs can under-expose explicit geometric cues and impose rigid fusion bottlenecks, leading to over-compression and poor token allocation as structural complexity grows. We present a unified all-atom framework that grounds language reasoning in geometric information while adaptively scaling structural tokens. The method first constructs variable-size structural patches on molecular graphs using an instruction-conditioned gating policy, enabling complexity-aware allocation of query tokens. It then refines the resulting patch tokens via cross-attention with modality embeddings and injects geometry-informed tokens into the language model to improve structure grounding and reduce structural hallucinations. Across diverse all-atom benchmarks, the proposed approach yields consistent gains in heterogeneous structure-grounded reasoning. An anonymized implementation is provided in the supplementary material.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02780v2</guid></item><item><title>[arXiv-AI 2026] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2503.06749</link><description>arXiv:2503.06749v3 Announce Type: replace-cross 
Abstract: DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.06749v3</guid></item><item><title>[arXiv-AI 2026] SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?</title><link>https://arxiv.org/abs/2505.20295</link><description>arXiv:2505.20295v4 Announce Type: replace-cross 
Abstract: The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables. To support the development of this universal form of LLM uncertainties, we publish the code that implements our metric for arbitrary LLMs under https://github.com/apple/ml-selfreflect .</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.20295v4</guid></item><item><title>[arXiv-AI 2026] Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title><link>https://arxiv.org/abs/2506.17208</link><description>arXiv:2506.17208v3 Announce Type: replace-cross 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.17208v3</guid></item><item><title>[arXiv-AI 2026] STACK: Adversarial Attacks on LLM Safeguard Pipelines</title><link>https://arxiv.org/abs/2506.24068</link><description>arXiv:2506.24068v3 Announce Type: replace-cross 
Abstract: Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic and OpenAI guard their latest Opus 4 model and GPT-5 models using such defense pipelines, and other frontier developers including Google DeepMind pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.24068v3</guid></item><item><title>[arXiv-AI 2026] CellForge: Agentic Design of Virtual Cell Models</title><link>https://arxiv.org/abs/2508.02276</link><description>arXiv:2508.02276v2 Announce Type: replace-cross 
Abstract: Virtual cell modeling aims to predict cellular responses to diverse perturbations but faces challenges from biological complexity, multimodal data heterogeneity, and the need for interdisciplinary expertise. We introduce CellForge, a multi-agent framework that autonomously designs and synthesizes neural network architectures tailored to specific single-cell datasets and perturbation tasks. Given raw multi-omics data and task descriptions, CellForge discovers candidate architectures through collaborative reasoning among specialized agents, then generates executable implementations. Our core contribution is the framework itself: showing that multi-agent collaboration mechanisms - rather than manual human design or single-LLM prompting - can autonomously produce executable, high-quality computational methods. This approach goes beyond conventional hyperparameter tuning by enabling entirely new architectural components such as trajectory-aware encoders and perturbation diffusion modules to emerge from agentic deliberation. We evaluate CellForge on six datasets spanning gene knockouts, drug treatments, and cytokine stimulations across multiple modalities (scRNA-seq, scATAC-seq, CITE-seq). The results demonstrate that the models generated by CellForge are highly competitive with established baselines, while revealing systematic patterns of architectural innovation. CellForge highlights the scientific value of multi-agent frameworks: collaboration among specialized agents enables genuine methodological innovation and executable solutions that single agents or human experts cannot achieve. This represents a paradigm shift toward autonomous scientific method development in computational biology. Code is available at https://github.com/gersteinlab/CellForge.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.02276v2</guid></item><item><title>[arXiv-AI 2026] Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>arXiv:2509.03531v2 Announce Type: replace-cross 
Abstract: Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets entity-level hallucinations-e.g., fabricated names, dates, citations-rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Despite being trained only to detect hallucinated entities, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.03531v2</guid></item><item><title>[arXiv-AI 2026] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs</title><link>https://arxiv.org/abs/2510.00031</link><description>arXiv:2510.00031v2 Announce Type: replace-cross 
Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.00031v2</guid></item><item><title>[arXiv-AI 2026] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</title><link>https://arxiv.org/abs/2511.11007</link><description>arXiv:2511.11007v2 Announce Type: replace-cross 
Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.11007v2</guid></item><item><title>[arXiv-AI 2026] CARL: Focusing Agentic Reinforcement Learning on Critical Actions</title><link>https://arxiv.org/abs/2512.04949</link><description>arXiv:2512.04949v2 Announce Type: replace-cross 
Abstract: Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for long-horizon agentic reasoning. CARL leverages entropy as a heuristic proxy for action criticality and achieves focused training by assigning rewards to high-criticality actions while excluding low-criticality actions from model updates, avoiding noisy credit assignment and redundant computation. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency across diverse evaluation settings. The source code will be publicly available.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.04949v2</guid></item><item><title>[arXiv-AI 2026] Learning to Discover at Test Time</title><link>https://arxiv.org/abs/2601.16175</link><description>arXiv:2601.16175v2 Announce Type: replace-cross 
Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\H{o}s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16175v2</guid></item><item><title>[arXiv-AI 2026] STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification</title><link>https://arxiv.org/abs/2601.19903</link><description>arXiv:2601.19903v2 Announce Type: replace-cross 
Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19903v2</guid></item><item><title>[arXiv-AI 2026] SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</title><link>https://arxiv.org/abs/2601.22129</link><description>arXiv:2601.22129v2 Announce Type: replace-cross 
Abstract: Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22129v2</guid></item><item><title>[arXiv-AI 2026] Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints</title><link>https://arxiv.org/abs/2602.00166</link><description>arXiv:2602.00166v2 Announce Type: replace-cross 
Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00166v2</guid></item><item><title>[arXiv-AI 2026] Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</title><link>https://arxiv.org/abs/2602.03190</link><description>arXiv:2602.03190v2 Announce Type: replace-cross 
Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 45.2 per-benchmark accuracy and 51.8 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03190v2</guid></item><item><title>[TSE 2026] The Power of Small LLMs: A Multi-Agent for Code Generation via Dynamic Precaution Tuning.</title><link>https://doi.org/10.1109/TSE.2025.3632508</link><description>Authors: Junfeng Zhang, Jinzhi Liao, Jiuyang Tang, Xiang Zhao 0002
Venue: IEEE Trans. Software Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/ZhangLTZ26</guid></item><item><title>[TSE 2026] Shield Broken: Black-Box Adversarial Attacks on LLM-Based Vulnerability Detectors.</title><link>https://doi.org/10.1109/TSE.2025.3638998</link><description>Authors: Yuan Jiang, Shan Huang, Christoph Treude, Xiaohong Su, Tiantian Wang 0001
Venue: IEEE Trans. Software Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/JiangHTSW26</guid></item><item><title>[TIFS 2026] MCLPF: Malware Collaborative Detection With LLM-Enhanced Pruning for Attributed Interpretable Flow Graphs.</title><link>https://doi.org/10.1109/TIFS.2025.3648113</link><description>Authors: Jun Tang, Zijun Li, Haiping Huang, Le Yu, Fu Xiao 0001, Ruilong Deng
Venue: IEEE Trans. Inf. Forensics Secur.
Year: 2026</description><author>dblp: new issues for streams/journals/tifs</author><pubDate>Sat, 24 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tifs/TangLHYXD26</guid></item><item><title>[TOSEM 2026] PonziHunter: Hunting Ethereum Ponzi Contract via Static Analysis and Contrastive Learning on the Bytecode Level</title><link>https://dl.acm.org/doi/abs/10.1145/3735971?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-21, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Wed, 21 Jan 2026 04:18:39 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3735971?af=R</guid></item><item><title>[TOSEM 2026] Abundant Modalities Offer More Nutrients: Multi-Modal-Based Function-Level Vulnerability Detection</title><link>https://dl.acm.org/doi/abs/10.1145/3731557?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-31, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Tue, 20 Jan 2026 02:02:29 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3731557?af=R</guid></item><item><title>[ESE 2026] Code review as decision-making - building a cognitive model from the questions asked during code review.</title><link>https://doi.org/10.1007/s10664-025-10791-2</link><description>Authors: Lo Gullstrand Heander, Emma Sderberg, Christofer Rydenflt
Venue: Empir. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ese</author><pubDate>Thu, 08 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ese/HeanderSR26</guid></item><item><title>[TDSC 2025] HgtJIT: Just-in-Time Vulnerability Detection Based on Heterogeneous Graph Transformer.</title><link>https://doi.org/10.1109/TDSC.2025.3586669</link><description>Authors: Xiaobing Sun 0001, Mingxuan Zhou, Sicong Cao, Xiaoxue Wu 0001, Lili Bo, Di Wu 0050, Bin Li 0006, Yang Xiang 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2025</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/SunZCWBWLX25</guid></item><item><title>[TDSC 2025] Tacco: A Framework for Ensuring the Security of Real-World TEEs via Formal Verification.</title><link>https://doi.org/10.1109/TDSC.2025.3594594</link><description>Authors: Jilin Hu, Yongwang Zhao, Shuangquan Pan, Zuohua Ding, Kui Ren 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2025</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/HuZPDR25</guid></item><item><title>[TDSC 2025] Real-World Code Vulnerability Detection Framework: From Data Preprocessing to Multi-Feature Fusion Detection.</title><link>https://doi.org/10.1109/TDSC.2025.3601228</link><description>Authors: Jixian Zhang, Qingfeng Du, Sheng Li, Zhongda Lu, Ting He, Chengwei Liu
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2025</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/ZhangDLLHL25</guid></item><item><title>[CCS 2025] Autonomous Vulnerability Analysis, Triaging, and Repair: A Historical Perspective.</title><link>https://doi.org/10.1145/3719027.3748270</link><description>Authors: Giovanni Vigna
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Vigna25</guid></item><item><title>[CCS 2025] SyzSpec: Specification Generation for Linux Kernel Fuzzing via Under-Constrained Symbolic Execution.</title><link>https://doi.org/10.1145/3719027.3744811</link><description>Authors: Yu Hao 0006, Juefei Pu, Xingyu Li, Zhiyun Qian, Ardalan Amiri Sani
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/0006PLQS25</guid></item><item><title>[CCS 2025] Protocols to Code: Formal Verification of a Secure Next-Generation Internet Router.</title><link>https://doi.org/10.1145/3719027.3765104</link><description>Authors: Joo C. Pereira, Tobias Klenze, Sofia Giampietro, Markus Limbeck, Dionysios Spiliopoulos, Felix A. Wolf, Marco Eilers, Christoph Sprenger 0001, David A. Basin, Peter Mller 0001, Adrian Perrig
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/PereiraKGLSWE0B25</guid></item><item><title>[CCS 2025] Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator.</title><link>https://doi.org/10.1145/3719027.3765193</link><description>Authors: R. Spencer Hallyburton, Miroslav Pajic
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/HallyburtonP25</guid></item><item><title>[CCS 2025] ZVDetector: State-Guided Vulnerability Detection System for Zigbee Devices.</title><link>https://doi.org/10.1145/3719027.3765035</link><description>Authors: Hai Lin, Chenglong Li 0006, Jiahai Yang 0001, Zhiliang Wang, Jiaqi Bai
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lin00WB25</guid></item><item><title>[CCS 2025] Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection.</title><link>https://doi.org/10.1145/3719027.3765027</link><description>Authors: Nils Bars, Lukas Bernhard, Moritz Schloegel, Thorsten Holz
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/BarsBSH25</guid></item><item><title>[CCS 2025] Security Analysis of Privately Verifiable Privacy Pass.</title><link>https://doi.org/10.1145/3719027.3765172</link><description>Authors: Konrad Hanff, Anja Lehmann, Cavit zbay
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/HanffLO25</guid></item><item><title>[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.</title><link>https://doi.org/10.1145/3719027.3765049</link><description>Authors: Bo Lin 0011, Shangwen Wang, Yihao Qin, Liqian Chen, Xiaoguang Mao
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/LinWQCM25</guid></item><item><title>[CCS 2025] Augmenting Search-based Program Synthesis with Local Inference Rules to Improve Black-box Deobfuscation.</title><link>https://doi.org/10.1145/3719027.3765134</link><description>Authors: Vidal Attias, Nicolas Bellec 0001, Grgoire Menguy, Sbastien Bardin, Jean-Yves Marion
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Attias0MBM25</guid></item><item><title>[CCS 2025] Poster: Leveraging Large Language Models to Effectively and Efficiently Identify Vulnerability Patches for WordPress Plugins.</title><link>https://doi.org/10.1145/3719027.3760720</link><description>Authors: Xue Leng, Hai Zhang, Tiantian Zhu 0001, Jianguo Sun
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/LengZZS25</guid></item><item><title>[CCS 2025] Poster: LogCraft: Crafting CVE-Aware Synthetic Worlds (Logs).</title><link>https://doi.org/10.1145/3719027.3760736</link><description>Authors: Kai-Xian Wong, Chan-Jien Tan, Yi-Ting Huang, Ying-Ren Guo, Yu-Zih Jheng, Guo-Wei Wong, Meng Chang Chen
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/WongTHGJWC25</guid></item><item><title>[CCS 2025] AI-Augmented Static Analysis: Bridging Heuristics and Completeness for Practical Reverse Engineering.</title><link>https://doi.org/10.1145/3719027.3765565</link><description>Authors: Monika Santra
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Santra25</guid></item><item><title>[CCS 2025] LAMPS '25: ACM CCS Workshop on Large AI Systems and Models with Privacy and Security Analysis.</title><link>https://doi.org/10.1145/3719027.3767670</link><description>Authors: Kwok-Yan Lam, Xiaoning Liu 0002, Derui Wang, Bo Li 0026, Wenyuan Xu 0001, Jieshan Chen, Minhui Xue 0001, Xingliang Yuan, Guangdong Bai, Shuo Wang 0012
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lam0W0XC0YBW25</guid></item><item><title>[USENIXSec 2025] Malicious LLM-Based Conversational AI Makes Users Reveal Personal Information.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/zhan</link><description>Authors: Xiao Zhan, Juan Carlos Carrillo, William Seymour, Jose Such
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/ZhanCSS25</guid></item><item><title>[USENIXSec 2025] HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/shen</link><description>Authors: Xinyue Shen 0001, Yixin Wu 0001, Yiting Qu, Michael Backes 0001, Savvas Zannettou, Yang Zhang 0016
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00010Q0Z025</guid></item><item><title>[USENIXSec 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/yu-jiahao</link><description>Authors: Jiahao Yu 0001, Haozheng Luo, Jerry Yao-Chieh Hu, Yan Chen 0004, Wenbo Guo 0002, Han Liu 0001, Xinyu Xing 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0001LHC00025</guid></item><item><title>[USENIXSec 2025] Game of Arrows: On the (In-)Security of Weight Obfuscation for On-Device TEE-Shielded LLM Partition Algorithms.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-pengli</link><description>Authors: Pengli Wang, Bingyou Dong, Yifeng Cai, Zheng Zhang, Junlin Liu, Huanran Xue, Ye Wu, Yao Zhang, Ziqi Zhang
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangDCZLXWZZ25</guid></item><item><title>[USENIXSec 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/zhang-jiawen</link><description>Authors: Jiawen Zhang 0005, Kejia Chen 0007, Lipeng He, Jian Lou 0001, Dan Li 0032, Zunlei Feng, Mingli Song, Jian Liu 0012, Kui Ren 0001, Xiaohu Yang 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00050H0LFS00025</guid></item><item><title>[USENIXSec 2025] LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lekssays</link><description>Authors: Ahmed Lekssays, Hamza Mouhcine, Khang Tran, Ting Yu 0001, Issa Khalil
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LekssaysMT0K25</guid></item><item><title>[USENIXSec 2025] Unlocking the Power of Differentially Private Zeroth-order Optimization for Fine-tuning LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/bao-ergute</link><description>Authors: Ergute Bao, Yangfan Jiang 0001, Fei Wei, Xiaokui Xiao, Zitao Li, Yaliang Li, Bolin Ding
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Bao0WXLLD25</guid></item><item><title>[USENIXSec 2025] Depth Gives a False Sense of Privacy: LLM Internal States Inversion.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/dong-tian</link><description>Authors: Tian Dong, Yan Meng 0001, Shaofeng Li 0001, Guoxing Chen, Zhen Liu 0008, Haojin Zhu
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Dong00C0Z25</guid></item><item><title>[USENIXSec 2025] Evaluating LLM-based Personal Information Extraction and Countermeasures.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/liu-yupei</link><description>Authors: Yupei Liu, Yuqi Jia, Jinyuan Jia 0001, Neil Zhenqiang Gong
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LiuJ0G25</guid></item><item><title>[USENIXSec 2025] Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wu-yixin-auditing</link><description>Authors: Yixin Wu 0001, Ziqing Yang 0002, Yun Shen, Michael Backes 0001, Yang Zhang 0016
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00010S0025</guid></item><item><title>[USENIXSec 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kim-hanna</link><description>Authors: Hanna Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin 0001, Kimin Lee
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KimSN0L25</guid></item><item><title>[USENIXSec 2025] zkGPT: An Efficient Non-interactive Zero-knowledge Proof Framework for LLM Inference.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/qu-zkgpt</link><description>Authors: Wenjie Qu 0001, Yijun Sun, Xuanming Liu, Tao Lu, Yanpei Guo, Kai Chen, Jiaheng Zhang
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0001SLLGCZ25</guid></item><item><title>[USENIXSec 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/krauss</link><description>Authors: Torsten Krau, Hamid Dashtbani, Alexandra Dmitrienko
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KraussDD25</guid></item><item><title>[USENIXSec 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/zhang-lan</link><description>Authors: Lan Zhang 0002, Xinben Gao, Liuyi Yao, Jinke Song, Yaliang Li
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0002GYSL25</guid></item><item><title>[USENIXSec 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/gong-xueluan</link><description>Authors: Xueluan Gong, Mingzhe Li, Yilin Zhang, Fengyuan Ran, Chen Chen, Yanjiao Chen, Qian Wang 0002, Kwok-Yan Lam
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/GongLZRCC0L25</guid></item><item><title>[USENIXSec 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/russinovich</link><description>Authors: Mark Russinovich, Ahmed Salem 0001, Ronen Eldan
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Russinovich0E25</guid></item><item><title>[USENIXSec 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-xunguang</link><description>Authors: Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma 0004, Shuai Wang 0011, Yingjiu Li, Yang Liu 0003, Ning Liu, Juergen Rahmel
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangWJL00L0LR25</guid></item><item><title>[USENIXSec 2025] Confusing Value with Enumeration: Studying the Use of CVEs in Academia.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/schloegel</link><description>Authors: Moritz Schloegel, Daniel Klischies, Simon Koch 0001, David Klein 0001, Lukas Gerlach 0001, Malte Wessels, Leon Trampert, Martin Johns, Mathy Vanhoef, Michael Schwarz 0001, Thorsten Holz, Jo Van Bulck
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/SchloegelK000WT25</guid></item><item><title>[USENIXSec 2025] We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/spracklen</link><description>Authors: Joseph Spracklen, Raveen Wijewickrama, A. H. M. Nazmus Sakib, Anindya Maiti, Bimal Viswanath, Murtuza Jadliwala
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/SpracklenWSMV25</guid></item><item><title>[USENIXSec 2025] Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based Agents.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/liu-fengyu</link><description>Authors: Fengyu Liu, Yuan Zhang 0009, Jiaqi Luo, Jiarun Dai, Tian Chen, Letian Yuan, Zhengmin Yu, Youkun Shi, Ke Li, Chengyuan Zhou, Hao Chen 0003, Min Yang 0002
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Liu0LDCYYSLZ0025</guid></item><item><title>[USENIXSec 2025] Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/shafran</link><description>Authors: Avital Shafran, Roei Schuster, Vitaly Shmatikov
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/ShafranSS25</guid></item><item><title>[USENIXSec 2025] Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/gong-yuyang</link><description>Authors: Yuyang Gong, Zhuo Chen, Jiawei Liu 0002, Miaokun Chen, Fengchang Yu, Wei Lu 0019, XiaoFeng Wang 0001, Xiaozhong Liu
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/GongC0CY00L25</guid></item><item><title>[USENIXSec 2025] PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/zou-poisonedrag</link><description>Authors: Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/ZouGW025</guid></item><item><title>[USENIXSec 2025] TracLLM: A Generic Framework for Attributing Long Context LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-yanting</link><description>Authors: Yanting Wang 0001, Wei Zou, Runpeng Geng, Jinyuan Jia 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangZG025</guid></item><item><title>[USENIXSec 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM Analysis.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kim-youngjoon</link><description>Authors: Youngjoon Kim 0001, Sunguk Shin 0001, Hyoungshick Kim, Jiwon Yoon 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Kim0KY25</guid></item><item><title>[USENIXSec 2025] APPATCH: Automated Adaptive Prompting Large Language Models for Real-World Software Vulnerability Patching.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/nong</link><description>Authors: Yu Nong, Haoran Yang, Long Cheng 0005, Hongxin Hu, Haipeng Cai
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/NongY0HC25</guid></item><item><title>[USENIXSec 2025] Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/jin-weifei</link><description>Authors: Weifei Jin, Yuxin Cao, Junjie Su, Derui Wang, Yedi Zhang, Minhui Xue 0001, Jie Hao 0001, Jin Song Dong 0001, Yixian Yang
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/JinCSWZ00DY25</guid></item><item><title>[USENIXSec 2025] A Thorough Security Analysis of BLE Proximity Tracking Protocols.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/liu-xiaofeng</link><description>Authors: Xiaofeng Liu 0013, Chaoshun Zuo, Qinsheng Hou, Pengcheng Ren, Jianliang Wu 0002, Qingchuan Zhao, Shanqing Guo
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0013ZHR0ZG25</guid></item><item><title>[USENIXSec 2025] SCASE: Automated Secret Recovery via Side-Channel-Assisted Symbolic Execution.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/weber</link><description>Authors: Daniel Weber 0007, Lukas Gerlach 0001, Leon Trampert, Youheng L, Jo Van Bulck, Michael Schwarz 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00070TLB025</guid></item><item><title>[USENIXSec 2025] EchoLLM: LLM-Augmented Acoustic Eavesdropping Attack on Bone Conduction Headphones with mmWave Radar.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/yao-xin</link><description>Authors: Xin Yao 0002, Kecheng Huang, Yimin Chen 0004, Jiawei Guo, Jie Tang, Ming Zhao 0007
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0002H0GT025</guid></item><item><title>[USENIXSec 2025] Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kwesi</link><description>Authors: Jabari Kwesi, Jiaxun Cao, Riya Manchanda, Pardis Emami Naeini
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KwesiCMN25</guid></item><item><title>[USENIXSec 2025] ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/chen-chuyang</link><description>Authors: Chuyang Chen 0001, Brendan Dolan-Gavitt, Zhiqiang Lin 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/ChenDC25</guid></item><item><title>[USENIXSec 2025] Hybrid Language Processor Fuzzing via LLM-Based Constraint Solving.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/yang-yupeng</link><description>Authors: Yupeng Yang, Shenglong Yao, Jizhou Chen, Wenke Lee
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/YangYCL25</guid></item><item><title>[USENIXSec 2025] Evaluating Privacy Policies under Modern Privacy Laws At Scale: An LLM-Based Automated Approach.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/xie</link><description>Authors: Qinge Xie, Karthik Ramakrishnan, Frank Li 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/XieR025</guid></item><item><title>[USENIXSec 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for Static Analysis Result Verification.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/bao-andrew</link><description>Authors: Andrew Bao, Wenjia Zhao, Yanhao Wang, Yueqiang Cheng, Stephen McCamant, Pen-Chung Yew
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/BaoZWCMY25</guid></item><item><title>[USENIXSec 2025] Low-Cost and Comprehensive Non-textual Input Fuzzing with LLM-Synthesized Input Generators.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/zhang-kunpeng</link><description>Authors: Kunpeng Zhang, Zongjie Li, Daoyuan Wu, Shuai Wang 0011, Xin Xia 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/ZhangLW0025</guid></item><item><title>[USENIXSec 2025] A Comprehensive Formal Security Analysis of OPC UA.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/diemunsch</link><description>Authors: Vincent Diemunsch, Lucca Hirschi, Steve Kremer
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/DiemunschHK25</guid></item><item><title>[USENIXSec 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/luo-zeren</link><description>Authors: Zeren Luo, Zifan Peng, Yule Liu, Zhen Sun 0001, Mingchen Li, Jingyi Zheng, Xinlei He 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LuoPL0LZ025</guid></item><item><title>[USENIXSec 2025] Cloak, Honey, Trap: Proactive Defenses Against LLM Agents.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/ayzenshteyn</link><description>Authors: Daniel Ayzenshteyn, Roy Weiss, Yisroel Mirsky
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/AyzenshteynWM25</guid></item><item><title>[USENIXSec 2025] SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/zhang-kaiyuan</link><description>Authors: Kaiyuan Zhang 0002, Siyuan Cheng 0005, Hanxi Guo, Yuetian Chen, Zian Su, Shengwei An, Yuntao Du 0002, Charles Fleming, Ashish Kundu, Xiangyu Zhang 0001, Ninghui Li
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00020GCSA0FK0L25</guid></item><item><title>[USENIXSec 2025] Effective PII Extraction from LLMs through Augmented Few-Shot Learning.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/cheng-shuai</link><description>Authors: Shuai Cheng, Shu Meng, Haitao Xu 0002, Haoran Zhang, Shuai Hao 0001, Chuan Yue, Wenrui Ma, Meng Han, Fan Zhang 0010, Zhao Li 0007
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/ChengM0ZHYMHZL25</guid></item><item><title>[USENIXSec 2025] PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/he-jinwen</link><description>Authors: Jinwen He, Yiyang Lu, Zijin Lin, Kai Chen 0012, Yue Zhao 0018
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/HeLL0025</guid></item><item><title>[USENIXSec 2025] ZIPPER: Static Taint Analysis for PHP Applications with Precision and Efficiency.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-xinyi</link><description>Authors: Xinyi Wang 0013, Yeting Li, Jie Lu 0009, Shizhe Cui, Chenghang Shi, Qin Mai, Yunpei Zhang, Yang Xiao 0011, Feng Li 0045, Wei Huo
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangLLCSMZ00H25</guid></item><item><title>[USENIXSec 2025] Effective Directed Fuzzing with Hierarchical Scheduling for Web Vulnerability Detection.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lin-zihan</link><description>Authors: Zihan Lin, Yuan Zhang 0009, Jiarun Dai, Xinyou Huang, Bocheng Xiang, Guangliang Yang 0001, Letian Yuan, Lei Zhang 0096, Tian Chen, Min Yang 0002
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Lin0DHX0Y0C025</guid></item><item><title>[ASE 2025] GPTVD: vulnerability detection and analysis method based on LLM's chain of thoughts.</title><link>https://doi.org/10.1007/s10515-025-00550-4</link><description>Authors: Yinan Chen, Yuan Huang, Xiangping Chen, Pengfei Shen, Lei Yun
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/ChenHCSY26</guid></item><item><title>[ASE 2025] HMF: Enhancing reentrancy vulnerability detection and repair with a hybrid model framework.</title><link>https://doi.org/10.1007/s10515-025-00546-0</link><description>Authors: Mengliang Li, Qiang Shen, Xiaoxue Ren, Han Fu, Zhuo Li 0014, Jianling Sun
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/LiSRFLS26</guid></item><item><title>[ASE 2025] Graph neural networks for precise bug localization through structural program analysis.</title><link>https://doi.org/10.1007/s10515-025-00556-y</link><description>Authors: Leila Yousofvand, Seyfollah Soleimani, Vahid Rafe, Amin Nikanjam
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YousofvandSRN26</guid></item><item><title>[ASE 2025] ByteEye: A smart contract vulnerability detection framework at bytecode level with graph neural networks.</title><link>https://doi.org/10.1007/s10515-025-00559-9</link><description>Authors: Jinni Yang, Shuang Liu 0007, Surong Dai, Yaozheng Fang, Kunpeng Xie, Ye Lu 0004
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YangLDFXL26</guid></item><item><title>[ASE 2025] SPVR: syntax-to-prompt vulnerability repair based on large language models.</title><link>https://doi.org/10.1007/s10515-025-00579-5</link><description>Authors: Ruoke Wang, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Yang Xiao, Xuan Wang
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/WangLGWXW26</guid></item><item><title>[SOSP 2025] KNighter: Transforming Static Analysis with LLM-Synthesized Checkers.</title><link>https://doi.org/10.1145/3731569.3764827</link><description>Authors: Chenyuan Yang, Zijie Zhao, Zichen Xie, Haoyu Li, Lingming Zhang 0001
Venue: SOSP
Year: 2025</description><author>dblp: new volumes for streams/conf/sosp</author><pubDate>Wed, 01 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sosp/YangZXLZ25</guid></item><item><title>[ACL 2025] Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation.</title><link>https://doi.org/10.18653/v1/2025.acl-short.93</link><description>Authors: Chengwei Qin, Wenxuan Zhou, Karthik Abinav Sankararaman, Nanshu Wang, Tengyu Xu, Alexander Radovic, Eryk Helenowski, Arya Talebzadeh, Aditya Tayade, Sinong Wang, Shafiq Joty, Han Fang, Hao Ma 0001
Venue: ACL (2)
Year: 2025</description><author>dblp: new volumes for streams/conf/acl</author><pubDate>Wed, 24 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acl/QinZSWXRHTTWJF025</guid></item><item><title>[IJCAI 2025] AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.</title><link>https://doi.org/10.24963/ijcai.2025/2</link><description>Authors: Petr Anokhin, Nikita Semenov, Artyom Y. Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev 0001, Evgeny Burnaev
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/AnokhinSSEK0B25</guid></item><item><title>[IJCAI 2025] Relational Decomposition for Program Synthesis.</title><link>https://doi.org/10.24963/ijcai.2025/504</link><description>Authors: Cline Hocquette, Andrew Cropper
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HocquetteC25</guid></item><item><title>[IJCAI 2025] POLO: An LLM-Powered Project-Level Code Performance Optimization Framework.</title><link>https://doi.org/10.24963/ijcai.2025/814</link><description>Authors: Jiameng Bai, Ruoyi Xu, Sai Wu, Dingyu Yang, Junbo Zhao 0002, Gang Chen 0001
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/BaiXWY0025</guid></item><item><title>[IJCAI 2025] APIMig: A Project-Level Cross-Multi-Version API Migration Framework Based on Evolution Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/829</link><description>Authors: Li Kuang, Qi Xie, Haiyang Yang, Yang Yang, Xiang Wei, HaoYue Kang, Yingjie Xia
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/KuangXYYWKX25</guid></item><item><title>[IJCAI 2025] Can We Translate Code Better with LLMs and Call Graph Analysis?</title><link>https://doi.org/10.24963/ijcai.2025/848</link><description>Authors: Yang Luo
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Luo25</guid></item><item><title>[IJCAI 2025] SecV: LLM-based Secure Verilog Generation with Clue-Guided Exploration on Hardware-CWE Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/895</link><description>Authors: Fanghao Fan, Yingjie Xia, Li Kuang
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/FanXK25</guid></item><item><title>[IJCAI 2025] Learn to Think: Bootstrapping LLM Logic Through Graph Representation Learning.</title><link>https://doi.org/10.24963/ijcai.2025/896</link><description>Authors: Hang Gao 0004, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu 0001
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0004ZWZWZ025</guid></item><item><title>[IJCAI 2025] SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation.</title><link>https://doi.org/10.24963/ijcai.2025/965</link><description>Authors: Bin Xu, Yiguan Lin, Yinghao Li, Yang Gao
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/XuLLG25</guid></item><item><title>[IJCAI 2025] The Graph's Apprentice: Teaching an LLM Low-Level Knowledge for Circuit Quality Estimation.</title><link>https://doi.org/10.24963/ijcai.2025/1033</link><description>Authors: Reza Moravej, Saurabh Bodhe, Zhanguang Zhang, Didier Chtelat, Dimitrios Tsaras, Yingxue Zhang 0001, Hui-Ling Zhen, Jianye Hao, Mingxuan Yuan
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/MoravejBZCT0ZHY25</guid></item><item><title>[IJCAI 2025] AI-Assisted Triage and Decision Support of Head and Neck Cancer Screening and Diagnosis in Low-Resourced Settings.</title><link>https://doi.org/10.24963/ijcai.2025/1087</link><description>Authors: Min Hun Lee, Sean Shao Wei Lam, Shaun Xin Hong Liew, Michael Dorosan, Nicholas Graves, Jonas Karlstrm, Hiang Khoon Tan, Walter Tsong Lee
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LeeLLDGKTL25</guid></item><item><title>[IJCAI 2025] GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs.</title><link>https://doi.org/10.24963/ijcai.2025/1256</link><description>Authors: Longchao Da, Parth Mitesh Shah, Kuanru Liou, Jiaxing Zhang 0002, Hua Wei 0001
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/DaSLZ025</guid></item><item><title>[EuroS&amp;P 2025] Mitigating Information Leakage in Large Language Models: Evaluating the Impact of Code Obfuscation on Vulnerability Detection.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00007</link><description>Authors: Beng Glay, Cemal Yilmaz 0001
Venue: EuroS&amp;amp;P (Workshops)
Year: 2025</description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/GulayY25</guid></item><item><title>[EuroS&amp;P 2025] CFA-Bench: Cybersecurity Forensic Llm Agent Benchmark and Testing.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00031</link><description>Authors: Francesco De Santis, Kai Huang, Rodolfo V. Valentim, Danilo Giordano, Marco Mellia, Zied Ben-Houidi, Dario Rossi 0001
Venue: EuroS&amp;amp;P (Workshops)
Year: 2025</description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/SantisHVGMBR25</guid></item><item><title>[S&amp;P 2025] Code Vulnerability Repair with Large Language Model Using Context-Aware Prompt Tuning.</title><link>https://doi.org/10.1109/SPW67851.2025.00040</link><description>Authors: Arshiya Khan, Guannan Liu, Xing Gao
Venue: SP (Workshops)
Year: 2025</description><author>dblp: new volumes for streams/conf/sp</author><pubDate>Sun, 20 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sp/KhanLG25</guid></item><item><title>[OSDI 2025] Paralegal: Practical Static Analysis for Privacy Bugs.</title><link>https://www.usenix.org/conference/osdi25/presentation/adam</link><description>Authors: Justus Adam, Carolyn Zech, Livia Zhu, Sreshtaa Rajesh, Nathan Harbison, Mithi Jethwa, Will Crichton, Shriram Krishnamurthi, Malte Schwarzkopf
Venue: OSDI
Year: 2025</description><author>dblp: new volumes for streams/conf/osdi</author><pubDate>Wed, 16 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/osdi/AdamZZRHJCKS25</guid></item><item><title>[ISSTA 2025] TBFV4J: An Automated Testing-Based Formal Verification Tool for Java.</title><link>https://doi.org/10.1145/3713081.3731740</link><description>Authors: Ai Liu, Yang Liu 0003, Shaoying Liu
Venue: ISSTA Companion
Year: 2025</description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/LiuLL25</guid></item><item><title>[ISSTA 2025] Revisiting the Combination of Static Analysis Error Traces and Dynamic Symbolic Execution: A Potential Approach for True Positive Confirmation (Registered Report).</title><link>https://doi.org/10.1145/3713081.3731720</link><description>Authors: Yihua Xu, Chengyu Zhang 0001, Geguang Pu
Venue: ISSTA Companion
Year: 2025</description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/Xu0P25</guid></item><item><title>[ISSTA 2025] A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection.</title><link>https://doi.org/10.1145/3713081.3731746</link><description>Authors: Junji Yu, Honglin Shu, Michael Fu, Dong Wang 0044, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen 0003
Venue: ISSTA Companion
Year: 2025</description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/YuSF0TK025</guid></item><item><title>[ICLR 2025] Diffusion On Syntax Trees For Program Synthesis.</title><link>https://openreview.net/forum?id=wN3KaUXA5X</link><description>Authors: Shreyas Kapur, Erik Jenner, Stuart Russell 0001
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/KapurJ025</guid></item><item><title>[ICLR 2025] MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code.</title><link>https://openreview.net/forum?id=1Iuw1jcIrf</link><description>Authors: Zimu Lu, Aojun Zhou, Ke Wang 0036, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li 0001
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/LuZ0RSPZL25</guid></item><item><title>[ICLR 2025] EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing.</title><link>https://openreview.net/forum?id=Y2Dh8rWwlb</link><description>Authors: Kaizhi Zheng, Xiaotong Chen, Xuehai He, Jing Gu, Linjie Li, Zhengyuan Yang, Kevin Lin, Jianfeng Wang, Lijuan Wang, Xin Eric Wang
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhengCHGLYLWWW25</guid></item><item><title>[ICLR 2025] SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?</title><link>https://openreview.net/forum?id=riTiq3i21b</link><description>Authors: John Yang 0002, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida Wang 0001, Ofir Press
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/YangJZLYWPMSNY025</guid></item><item><title>[ICLR 2025] GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation.</title><link>https://openreview.net/forum?id=5RUM1aIdok</link><description>Authors: Tao Feng, Yihang Sun, Jiaxuan You
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/FengSY25</guid></item><item><title>[ICLR 2025] GraphRouter: A Graph-based Router for LLM Selections.</title><link>https://openreview.net/forum?id=eU39PDsZtT</link><description>Authors: Tao Feng, Yanzhen Shen, Jiaxuan You
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/FengSY25a</guid></item><item><title>[ICLR 2025] Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment.</title><link>https://openreview.net/forum?id=kN25ggeq1J</link><description>Authors: Yuze Zhao, Tianyun Ji, Wenjun Feng, Zhenya Huang, Qi Liu 0003, Zhiding Liu, Yixiao Ma, Kai Zhang 0038, Enhong Chen
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhaoJFH0LM0C25</guid></item><item><title>[ICLR 2025] CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &amp; Reasoning Capabilities of CodeLLMs.</title><link>https://openreview.net/forum?id=CahIEKCu5Q</link><description>Authors: Dung Manh Nguyen, Thang Chau Phan, Nam Le Hai, Tien-Thong Doan, Nam V. Nguyen 0001, Quang Pham, Nghi D. Q. Bui
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/NguyenPHDNPB25</guid></item><item><title>[ICLR 2025] Steering Large Language Models between Code Execution and Textual Reasoning.</title><link>https://openreview.net/forum?id=5X5Z7Ffrjb</link><description>Authors: Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ChenJSFW25</guid></item><item><title>[ICLR 2025] RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph.</title><link>https://openreview.net/forum?id=dw9VUsSHGB</link><description>Authors: Siru Ouyang, Wenhao Yu 0002, Kaixin Ma, Zilin Xiao, Zhihan Zhang 0001, Mengzhao Jia, Jiawei Han 0001, Hongming Zhang 0009, Dong Yu 0001
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Ouyang0MX0J00025</guid></item><item><title>[ICLR 2025] Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents.</title><link>https://openreview.net/forum?id=V4y0CpX4hK</link><description>Authors: Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang 0001, Yongfeng Zhang 0003
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhangHMYWZWZ25</guid></item><item><title>[ICLR 2025] Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control.</title><link>https://openreview.net/forum?id=l32IrJtpOP</link><description>Authors: Sunguk Shin 0001, Youngjoon Kim 0001
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ShinK25</guid></item><item><title>[ICLR 2025] IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities.</title><link>https://openreview.net/forum?id=9LdJDU7E91</link><description>Authors: Ziyang Li, Saikat Dutta 0001, Mayur Naik
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Li0N25</guid></item><item><title>[ICLR 2025] RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code.</title><link>https://openreview.net/forum?id=NiNIthntx7</link><description>Authors: Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, Roshanak Zilouchian Moghaddam
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/GautamGJSM25</guid></item><item><title>[ICLR 2025] CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning.</title><link>https://openreview.net/forum?id=dCPF1wlqj8</link><description>Authors: Jiaxin Wen, Jian Guan 0002, Hongning Wang, Wei Wu 0014, Minlie Huang
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Wen0W0H25</guid></item><item><title>[ICLR 2025] Safety Layers in Aligned Large Language Models: The Key to LLM Security.</title><link>https://openreview.net/forum?id=kUH1yPMAn7</link><description>Authors: Shen Li, Liuyi Yao, Lan Zhang 0002, Yaliang Li
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/LiY0L25</guid></item><item><title>[ICLR 2025] HaDeMiF: Hallucination Detection and Mitigation in Large Language Models.</title><link>https://openreview.net/forum?id=VwOYxPScxB</link><description>Authors: Xiaoling Zhou, Mingjie Zhang, Zhemg Lee, Wei Ye 0004, Shikun Zhang
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhouZL0Z25</guid></item><item><title>[ICLR 2025] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation.</title><link>https://openreview.net/forum?id=sGpCzsfd1K</link><description>Authors: Cheng Yang 0002, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang 0011, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai 0002, Yujiu Yang 0001
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/0002SLS0JXZLZLN25</guid></item><item><title>[ACSAC 2025] Learning to Unfix: Towards ML Robustness in Vulnerability Detection via Structure-Aware Code Generation.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00014</link><description>Authors: Muhammad Fakhrur Rozi, Takeshi Takahashi 0001
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/Rozi024</guid></item><item><title>[ACSAC 2025] AdVul: Adversarial Attack against ML-based Vulnerability Detection.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00018</link><description>Authors: Marina Katoh, Weiping Pei, Youye Xie
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/KatohPX24</guid></item><item><title>[ACSAC 2025] Software Vulnerability Detection Using LLM: Does Additional Information Help?</title><link>https://doi.org/10.1109/ACSACW65225.2024.00031</link><description>Authors: Samiha Shimmi, Yash Saini, Mark Schaefer, Hamed Okhravi, Mona Rahimi
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/ShimmiSSOR24</guid></item><item><title>[ACSAC 2025] Automated Vulnerability Detection in Smart Contracts using Control Flow Graphs and Machine Learning.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00037</link><description>Authors: Charles Lohest, Samy Bettaieb, Axel Legay
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/LohestBL24</guid></item><item><title>[NDSS 2025] Generating API Parameter Security Rules with LLM for API Misuse Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/generating-api-parameter-security-rules-with-llm-for-api-misuse-detection/</link><description>Authors: Jinghua Liu, Yi Yang 0100, Kai Chen 0012, Miaoqian Lin
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LiuY0L25</guid></item><item><title>[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/from-large-to-mammoth-a-comparative-evaluation-of-large-language-models-in-vulnerability-detection/</link><description>Authors: Jie Lin, David Mohaisen
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LinM25</guid></item><item><title>[NDSS 2025] PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation.</title><link>https://www.ndss-symposium.org/ndss-paper/propertygpt-llm-driven-formal-verification-of-smart-contracts-through-retrieval-augmented-property-generation/</link><description>Authors: Ye Liu 0012, Yue Xue, Daoyuan Wu, Yuqiang Sun 0001, Yi Li 0008, Miaolei Shi, Yang Liu 0003
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/0012XW00S025</guid></item><item><title>[NDSS 2025] Uncovering the iceberg from the tip: Generating API Specifications for Bug Detection via Specification Propagation Analysis.</title><link>https://www.ndss-symposium.org/ndss-paper/uncovering-the-iceberg-from-the-tip-generating-api-specifications-for-bug-detection-via-specification-propagation-analysis/</link><description>Authors: Miaoqian Lin, Kai Chen 0012, Yi Yang 0100, Jinghua Liu
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/Lin0YL25</guid></item><item><title>[NDSS 2025] You Can Rand but You Can't Hide: A Holistic Security Analysis of Google Fuchsia's (and gVisor's) Network Stack.</title><link>https://www.ndss-symposium.org/ndss-paper/you-can-rand-but-you-cant-hide-a-holistic-security-analysis-of-google-fuchsias-and-gvisors-network-stack/</link><description>Authors: Inon Kaplan, Ron Even, Amit Klein 0001
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/KaplanE025</guid></item><item><title>[NeurIPS 2025] SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html</link><description>Authors: Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou 0001, Jiwen Lu
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/YinXWZL24</guid></item><item><title>[NeurIPS 2025] Can Graph Learning Improve Planning in LLM-based Agents?</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/098d1bd3eb6156a4c2f834563cdcf617-Abstract-Conference.html</link><description>Authors: Xixi Wu, Yifei Shen, Caihua Shan, Kaitao Song, Siwei Wang, Bohang Zhang, Jiarui Feng, Hong Cheng 0001, Wei Chen, Yun Xiong, Dongsheng Li
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WuSSSWZFCCXL24</guid></item><item><title>[NeurIPS 2025] LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0b77d3a82b59e9d9899370b378087faf-Abstract-Conference.html</link><description>Authors: Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu 0002
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangZL024</guid></item><item><title>[NeurIPS 2025] HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1c9c85bae6161d52182d0fe2f3640512-Abstract-Conference.html</link><description>Authors: Shraddha Barke, Emmanuel Anaya Gonzalez, Saketh Ram Kasibatla, Taylor Berg-Kirkpatrick, Nadia Polikarpova
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BarkeGKBP24</guid></item><item><title>[NeurIPS 2025] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/62c6d7893b13a13c659cb815852dd00d-Abstract-Datasets_and_Benchmarks_Track.html</link><description>Authors: Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LinCHWLLSL24</guid></item><item><title>[NeurIPS 2025] NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/69d97a6493fbf016fff0a751f253ad18-Abstract-Datasets_and_Benchmarks_Track.html</link><description>Authors: Minghao Shao, Sofija Jancheska, Meet Udeshi, Brendan Dolan-Gavitt, Haoran Xi, Kimberly Milner, Boyuan Chen 0004, Max Yin, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique 0001
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ShaoJUDxM0YGKKK24</guid></item><item><title>[NeurIPS 2025] SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/6efcc7fd8efeee29a050a79c843c90e0-Abstract-Conference.html</link><description>Authors: Yangruibo Ding, Jinjun Peng, Marcus J. Min, Gail E. Kaiser, Junfeng Yang, Baishakhi Ray
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DingPMKYR24</guid></item><item><title>[NeurIPS 2025] GraphVis: Boosting LLMs with Visual Knowledge Graph Integration.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7cb04f510593c9ba30da398f5e0a7e7b-Abstract-Conference.html</link><description>Authors: Yihe Deng, Chenchen Ye 0001, Zijie Huang 0002, Mingyu Derek Ma, Yiwen Kou, Wei Wang 0010
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Deng00MK024</guid></item><item><title>[NeurIPS 2025] LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/8196be81e68289d7a9ece21ed7f5750a-Abstract-Datasets_and_Benchmarks_Track.html</link><description>Authors: Bowen Li 0007, Zhaoyu Li, Qiwei Du, Jinqi Luo, Wenshan Wang, Yaqi Xie 0001, Simon Stepputtis, Chen Wang 0033, Katia P. Sycara, Pradeep Ravikumar, Alexander G. Gray, Xujie Si, Sebastian A. Scherer
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiLDLWXSWSRGSS24</guid></item><item><title>[NeurIPS 2025] WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/820c61a0cd419163ccbd2c33b268816e-Abstract-Conference.html</link><description>Authors: Hao Tang 0008, Darren Key, Kevin Ellis
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0008KE24</guid></item><item><title>[NeurIPS 2025] SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/94f093b41fc2666376fb1f667fe282f3-Abstract-Conference.html</link><description>Authors: Niels Mndler, Mark Niklas Mller, Jingxuan He, Martin T. Vechev
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MundlerMHV24</guid></item><item><title>[NeurIPS 2025] HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ba92705991cfbbcedc26e27e833ebbae-Abstract-Conference.html</link><description>Authors: Xuefeng Du, Chaowei Xiao, Sharon Li 0001
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DuX024</guid></item><item><title>[NeurIPS 2025] RedCode: Risky Code Execution and Generation Benchmark for Code Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/bfd082c452dffb450d5a5202b0419205-Abstract-Datasets_and_Benchmarks_Track.html</link><description>Authors: Chengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng 0005, Zinan Lin 0001, Dawn Song, Bo Li 0026
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/GuoLXZZ0SL24</guid></item><item><title>[NeurIPS 2025] Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html</link><description>Authors: Hongbo Wang, Jie Cao 0002, Jin Liu, Xiaoqiang Zhou, Huaibo Huang, Ran He 0001
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Wang0LZH024</guid></item><item><title>[NeurIPS 2025] Suitable is the Best: Task-Oriented Knowledge Fusion in Vulnerability Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/db7a81128155d6f14970c12d0b5e7a4c-Abstract-Conference.html</link><description>Authors: Jingjing Wang, Minhuan Huang, Yuanping Nie, Xiang Li 0078, Qianjin Du, Wei Kong, Huan Deng, Xiaohui Kuang
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangHN0DKDK24</guid></item><item><title>[ASE 2025] Language-Agnostic Static Analysis of Probabilistic Programs.</title><link>https://doi.org/10.1145/3691620.3695031</link><description>Authors: Markus Bck, Michael Schrder 0005, Jrgen Cito
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Bock0C24</guid></item><item><title>[ASE 2025] GPP: A Graph-Powered Prioritizer for Code Review Requests.</title><link>https://doi.org/10.1145/3691620.3694990</link><description>Authors: Lanxin Yang, Jinwei Xu, He Zhang 0001, Fanghao Wu, Jun Lyu, Yue Li 0047, Alberto Bacchelli
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/YangX0WLLB24</guid></item><item><title>[ASE 2025] Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models.</title><link>https://doi.org/10.1145/3691620.3695013</link><description>Authors: Yulun Wu, Ming Wen 0001, Zeliang Yu, Xiaochen Guo, Hai Jin 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Wu0YG024</guid></item><item><title>[ASE 2025] Program Synthesis Meets Visual What-Comes-Next Puzzles.</title><link>https://doi.org/10.1145/3691620.3695015</link><description>Authors: Sumit Lahiri, Pankaj Kumar Kalita, Akshay Kumar Chittora, Varun Vankudre, Subhajit Roy 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LahiriKCV024</guid></item><item><title>[ASE 2025] GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph.</title><link>https://doi.org/10.1145/3691620.3695054</link><description>Authors: Wei Liu 0189, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang 0004, Haiyan Zhao 0001, Zhi Jin 0001, Qianxiang Wang
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiuYZS00JW24</guid></item><item><title>[ASE 2025] Snopy: Bridging Sample Denoising with Causal Graph Learning for Effective Vulnerability Detection.</title><link>https://doi.org/10.1145/3691620.3695057</link><description>Authors: Sicong Cao, Xiaobing Sun 0001, Xiaoxue Wu 0001, David Lo 0001, Lili Bo, Bin Li 0006, Xiaolei Liu 0001, Xingwei Lin, Wei Liu 0010
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Cao000B00L024</guid></item><item><title>[ASE 2025] Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network.</title><link>https://doi.org/10.1145/3691620.3695068</link><description>Authors: Di Cui, Jiaqi Wang, Qiangqiang Wang, Peng Ji, Minglang Qiao, Yutong Zhao, Jingzhao Hu, Luqiao Wang, Qingshan Li
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/CuiWWJQZHWL24</guid></item><item><title>[ASE 2025] AdvSCanner: Generating Adversarial Smart Contracts to Exploit Reentrancy Vulnerabilities Using LLM and Static Analysis.</title><link>https://doi.org/10.1145/3691620.3695482</link><description>Authors: Yin Wu, Xiaofei Xie, Chenyang Peng, Dijun Liu, Hao Wu 0100, Ming Fan 0002, Ting Liu 0002, Haijun Wang 0002
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuXPLW00W24</guid></item><item><title>[ASE 2025] Compositional Security Analysis of Dynamic Component-based Systems.</title><link>https://doi.org/10.1145/3691620.3695499</link><description>Authors: Narges Khakpour, Charilaos Skandylas
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/KhakpourS24</guid></item><item><title>[ASE 2025] HITS: High-coverage LLM-based Unit Test Generation via Method Slicing.</title><link>https://doi.org/10.1145/3691620.3695501</link><description>Authors: Zejun Wang, Kaibo Liu, Ge Li 0001, Zhi Jin 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangL0J24</guid></item><item><title>[ASE 2025] TypeFSL: Type Prediction from Binaries via Inter-procedural Data-flow Analysis and Few-shot Learning.</title><link>https://doi.org/10.1145/3691620.3695502</link><description>Authors: Zirui Song, Yutong Zhou, Shuaike Dong, Ke Zhang 0039, Kehuan Zhang
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/SongZDZZ24</guid></item><item><title>[ASE 2025] Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?</title><link>https://doi.org/10.1145/3691620.3695539</link><description>Authors: Yu Zhao, Lina Gong, Zhiqiu Huang, Yongwei Wang, Mingqiang Wei, Fei Wu 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoGHWW024</guid></item><item><title>[ASE 2025] STASE: Static Analysis Guided Symbolic Execution for UEFI Vulnerability Signature Generation.</title><link>https://doi.org/10.1145/3691620.3695543</link><description>Authors: Md Shafiuzzaman, Achintya Desai, Laboni Sarker, Tevfik Bultan
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ShafiuzzamanDSB24</guid></item><item><title>[ASE 2025] Understanding Developer-Analyzer Interactions in Code Reviews.</title><link>https://doi.org/10.1145/3691620.3695257</link><description>Authors: Martin Schf, Berk irisci, Linghui Luo, Muhammad Numair Mansur, Omer Tripp, Daniel Sanchez, Qiang Zhou 0009, Muhammad Bilal Zafar
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/SchafCLMTSZZ24</guid></item><item><title>[ASE 2025] DeployFix: Dynamic Repair of Software Deployment Failures via Constraint Solving.</title><link>https://doi.org/10.1145/3691620.3695268</link><description>Authors: Haoyu Liao, Jianmei Guo, Bo Huang 0002, Yujie Han, Dingyu Yang, Kai Shi 0006, Jonathan Ding, Guoyao Xu, Guodong Yang, Liping Zhang 0013
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiaoG0HY0DXYZ24</guid></item><item><title>[ASE 2025] Experience Report on Applying Program Analysis Techniques for Mainframe Application Understanding.</title><link>https://doi.org/10.1145/3691620.3695270</link><description>Authors: Shivali Agarwal, Hiroaki Nakamura, Rami Katan
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/AgarwalNK24</guid></item><item><title>[ASE 2025] Enhancing Compositional Static Analysis with Dynamic Analysis.</title><link>https://doi.org/10.1145/3691620.3695599</link><description>Authors: Dino Distefano, Matteo Marescotti, Cons T. hs, Sopot Cela, Gabriela Cunha Sampaio, Radu Grigore, kos Hajdu, Timotej Kapus, Ke Mao, Thibault Suzanne
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/DistefanoMACSGH24</guid></item><item><title>[ASE 2025] Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation.</title><link>https://doi.org/10.1145/3691620.3695291</link><description>Authors: Luqiao Wang, Yangtao Zhou, Huiying Zhuang, Qingshan Li, Di Cui, Yutong Zhao, Lu Wang 0014
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZZLCZ024</guid></item><item><title>[ASE 2025] Oracle-Guided Vulnerability Diversity and Exploit Synthesis of Smart Contracts Using LLMs.</title><link>https://doi.org/10.1145/3691620.3695292</link><description>Authors: Mojtaba Eshghie, Cyrille Artho
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/EshghieA24</guid></item><item><title>[ASE 2025] ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts.</title><link>https://doi.org/10.1145/3691620.3695349</link><description>Authors: Che Wang, Jiashuo Zhang 0001, Jianbo Gao 0003, Libin Xia, Zhi Guan, Zhong Chen 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZGXG024</guid></item><item><title>[ASE 2025] Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers.</title><link>https://doi.org/10.1145/3691620.3695322</link><description>Authors: Yang Luo, Richard Yu, Fajun Zhang, Ling Liang, Yongqiang Xiong
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LuoYZLX24</guid></item><item><title>[ASE 2025] RepoGenix: Dual Context-Aided Repository-Level Code Completion with Language Models.</title><link>https://doi.org/10.1145/3691620.3695331</link><description>Authors: Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, Wei Jiang 0011, Hongwei Chen, Chengpeng Wang 0001, Gang Fan
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiangXZZD0CWF24</guid></item><item><title>[RAID 2024] Large-Scale Security Analysis of Real-World Backend Deployments Speaking IoT-Focused Protocols.</title><link>https://doi.org/10.1145/3678890.3678899</link><description>Authors: Carlotta Tagliaro, Martina Komsic, Andrea Continella, Kevin Borgolte, Martina Lindorfer
Venue: RAID
Year: 2024</description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Mon, 30 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/TagliaroKCBL24</guid></item><item><title>[RAID 2024] A Comprehensive, Automated Security Analysis of the Uptane Automotive Over-the-Air Update Framework.</title><link>https://doi.org/10.1145/3678890.3678927</link><description>Authors: Robert Lorch, Daniel Larraz, Cesare Tinelli, Omar Chowdhury
Venue: RAID
Year: 2024</description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Mon, 30 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/LorchLTC24</guid></item><item><title>[FM 2024] Learning Branching-Time Properties in CTL and ATL via Constraint Solving.</title><link>https://doi.org/10.1007/978-3-031-71162-6_16</link><description>Authors: Benjamin Bordais, Daniel Neider, Rajarshi Roy 0002
Venue: FM (1)
Year: 2024</description><author>dblp: new volumes for streams/conf/fm</author><pubDate>Mon, 16 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/fm/BordaisNR24</guid></item></channel></rss>