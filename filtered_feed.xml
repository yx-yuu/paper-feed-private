<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Fri, 06 Feb 2026 20:45:14 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[C&amp;S 2026] SemTaint: A scalable taint analysis approach for JavaWeb frameworks and composite containers</title><link>https://www.sciencedirect.com/science/article/pii/S0167404825005103?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers &amp; Security, Volume 163&lt;/p&gt;&lt;p&gt;Author(s): Haotian Huang, Ruibin Yan, Jian Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Computers &amp; Security</author><pubDate>Fri, 06 Feb 2026 20:45:14 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167404825005103</guid></item><item><title>[SCP 2026] DATVD: A novel vulnerability detection method based on dynamic attention and hybrid convolutional pooling</title><link>https://www.sciencedirect.com/science/article/pii/S0167642326000080?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Science of Computer Programming, Volume 251&lt;/p&gt;&lt;p&gt;Author(s): Jinfu Chen, Jinyu Mu, Saihua Cai, Jiapeng Zhou, Ziyan Liu, Xinping Shi&lt;/p&gt;</description><author>ScienceDirect Publication: Science of Computer Programming</author><pubDate>Fri, 06 Feb 2026 20:44:41 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167642326000080</guid></item><item><title>[JSS 2026] RLV: LLM-based vulnerability detection by retrieving and refining contextual information</title><link>https://www.sciencedirect.com/science/article/pii/S016412122500425X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 235&lt;/p&gt;&lt;p&gt;Author(s): Fangcheng Qiu, Zhongxin Liu, Bingde Hu, Zhengong Cai, Lingfeng Bao, Xinyu Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Fri, 06 Feb 2026 20:44:40 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S016412122500425X</guid></item><item><title>[IST 2026] VulSEG: Enhanced graph-based vulnerability detection system with advanced text embedding</title><link>https://www.sciencedirect.com/science/article/pii/S0950584925003465?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Wenjing Cai, Xin Liu, Lipeng Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 20:44:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584925003465</guid></item><item><title>[IST 2026] CSVD-AES: Cross-project software vulnerability detection based on active learning with metric fusion</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Zhidan Yuan, Xiang Chen, Juan Zhang, Weiming Zeng&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 20:44:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000042</guid></item><item><title>[IST 2026] EdgeSim: Firmware vulnerability detection with control transfer-enhanced binary code similarity detection</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000091?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Li Liu, Shen Wang, Xunzhi Jiang&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 20:44:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000091</guid></item><item><title>[IST 2026] COTVD: A function-level vulnerability detection framework using chain-of-thought reasoning with large language models</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000327?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 193&lt;/p&gt;&lt;p&gt;Author(s): Yinan Chen, Xiangping Chen, Yuan Huang, Changlin Yang, Lei Yun&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 20:44:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000327</guid></item><item><title>[arXiv-CR 2026] Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software</title><link>https://arxiv.org/abs/2602.04894</link><description>arXiv:2602.04894v1 Announce Type: new 
Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04894v1</guid></item><item><title>[arXiv-CR 2026] SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions</title><link>https://arxiv.org/abs/2602.05329</link><description>arXiv:2602.05329v1 Announce Type: new 
Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05329v1</guid></item><item><title>[arXiv-CR 2026] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening</title><link>https://arxiv.org/abs/2602.05386</link><description>arXiv:2602.05386v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05386v1</guid></item><item><title>[arXiv-CR 2026] BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models</title><link>https://arxiv.org/abs/2602.05401</link><description>arXiv:2602.05401v1 Announce Type: new 
Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05401v1</guid></item><item><title>[arXiv-CR 2026] Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection</title><link>https://arxiv.org/abs/2602.05868</link><description>arXiv:2602.05868v1 Announce Type: new 
Abstract: Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05868v1</guid></item><item><title>[arXiv-CR 2026] A Causal Perspective for Enhancing Jailbreak Attack and Defense</title><link>https://arxiv.org/abs/2602.04893</link><description>arXiv:2602.04893v1 Announce Type: cross 
Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04893v1</guid></item><item><title>[arXiv-CR 2026] Hallucination-Resistant Security Planning with a Large Language Model</title><link>https://arxiv.org/abs/2602.05279</link><description>arXiv:2602.05279v1 Announce Type: cross 
Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05279v1</guid></item><item><title>[arXiv-CR 2026] Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration</title><link>https://arxiv.org/abs/2602.05759</link><description>arXiv:2602.05759v1 Announce Type: cross 
Abstract: The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05759v1</guid></item><item><title>[arXiv-CR 2026] From Sands to Mansions: Towards Automated Cyberattack Emulation with Classical Planning and Large Language Models</title><link>https://arxiv.org/abs/2407.16928</link><description>arXiv:2407.16928v4 Announce Type: replace 
Abstract: Evolving attacker capabilities demand realistic and continuously updated cyberattack emulation for threat-informed defense and security benchmarking. Towards automated attack emulation, this paper defines modular attack actions and a linking model to organize and chain heterogeneous attack tools into causality-preserving cyberattacks. Building on this foundation, we introduce Aurora: an automated cyberattack emulation system powered by symbolic planning and large language models (LLMs). Aurora crafts actionable, causality-preserving attack chains tailored to Cyber Threat Intelligence (CTI) reports and target environments, and automatically executes these emulations. Using Aurora, we generated an extensive cyberattack emulation dataset from 250 attack reports, 15 times larger than the leading expert-crafted dataset. Our evaluation shows that Aurora significantly outperforms existing methods in creating actionable, diverse, and realistic attack chains. We release the dataset and use it to evaluate three state-of-the-art intrusion detection systems, whose performance differed notably from results on older datasets, highlighting the need for up-to-date, automated attack emulation.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2407.16928v4</guid></item><item><title>[arXiv-CR 2026] Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates</title><link>https://arxiv.org/abs/2602.04653</link><description>arXiv:2602.04653v2 Announce Type: replace 
Abstract: Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04653v2</guid></item><item><title>[arXiv-CR 2026] How Catastrophic is Your LLM? Certifying Risk in Conversation</title><link>https://arxiv.org/abs/2510.03969</link><description>arXiv:2510.03969v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose C$^3$LLM, a novel, principled statistical Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions--random node, graph path, and adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.03969v3</guid></item><item><title>[arXiv-SE 2026] Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set</title><link>https://arxiv.org/abs/2602.04910</link><description>arXiv:2602.04910v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04910v1</guid></item><item><title>[arXiv-SE 2026] EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering</title><link>https://arxiv.org/abs/2602.05242</link><description>arXiv:2602.05242v1 Announce Type: new 
Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05242v1</guid></item><item><title>[arXiv-SE 2026] PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models</title><link>https://arxiv.org/abs/2602.05270</link><description>arXiv:2602.05270v1 Announce Type: new 
Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05270v1</guid></item><item><title>[arXiv-SE 2026] Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study</title><link>https://arxiv.org/abs/2602.05465</link><description>arXiv:2602.05465v1 Announce Type: new 
Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05465v1</guid></item><item><title>[arXiv-SE 2026] Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations</title><link>https://arxiv.org/abs/2602.05523</link><description>arXiv:2602.05523v1 Announce Type: new 
Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05523v1</guid></item><item><title>[arXiv-SE 2026] SEAL: Symbolic Execution with Separation Logic (Competition Contribution)</title><link>https://arxiv.org/abs/2602.05703</link><description>arXiv:2602.05703v1 Announce Type: new 
Abstract: SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05703v1</guid></item><item><title>[arXiv-SE 2026] A Dual-Loop Agent Framework for Automated Vulnerability Reproduction</title><link>https://arxiv.org/abs/2602.05721</link><description>arXiv:2602.05721v1 Announce Type: new 
Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \textit{Tactical Loop} for code-level refinement, while the \textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\% and 54.3\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\% and 20.4\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05721v1</guid></item><item><title>[arXiv-SE 2026] Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes</title><link>https://arxiv.org/abs/2602.05780</link><description>arXiv:2602.05780v1 Announce Type: new 
Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05780v1</guid></item><item><title>[arXiv-SE 2026] Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction</title><link>https://arxiv.org/abs/2602.04892</link><description>arXiv:2602.04892v1 Announce Type: cross 
Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04892v1</guid></item><item><title>[arXiv-SE 2026] RocqSmith: Can Automatic Optimization Forge Better Proof Agents?</title><link>https://arxiv.org/abs/2602.05762</link><description>arXiv:2602.05762v1 Announce Type: cross 
Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05762v1</guid></item><item><title>[arXiv-SE 2026] Multi-View Adaptive Contrastive Learning for Information Retrieval Based Fault Localization</title><link>https://arxiv.org/abs/2409.12519</link><description>arXiv:2409.12519v3 Announce Type: replace 
Abstract: Most studies focused on information retrieval-based techniques for fault localization, which built representations for bug reports and source code files and matched their semantic vectors through similarity measurement. However, such approaches often ignore some useful information that might help improve localization performance, such as 1) the interaction relationship between bug reports and source code files; 2) the similarity relationship between bug reports; and 3) the co-citation relationship between source code files. In this paper, we propose a novel approach named Multi-View Adaptive Contrastive Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the above-mentioned relationships for software fault localization. Specifically, we first generate data augmentations from report-code interaction view, report-report similarity view and code-code co-citation view separately, and adopt graph neural network to aggregate the information of bug reports or source code files from the three views in the embedding process. Moreover, we perform contrastive learning across these views. Our design of contrastive learning task will force the bug report representations to encode information shared by report-report and report-code views,and the source code file representations shared by code-code and report-code views, thereby alleviating the noise from auxiliary information. Finally, to evaluate the performance of our approach, we conduct extensive experiments on five open-source Java projects. The results show that our model can improve over the best baseline up to 28.93%, 25.57% and 20.35% on Accuracy@1, MAP and MRR, respectively.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2409.12519v3</guid></item><item><title>[arXiv-SE 2026] Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title><link>https://arxiv.org/abs/2506.17208</link><description>arXiv:2506.17208v3 Announce Type: replace 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.17208v3</guid></item><item><title>[arXiv-SE 2026] Code Clone Detection via an AlphaFold-Inspired Framework</title><link>https://arxiv.org/abs/2507.15226</link><description>arXiv:2507.15226v2 Announce Type: replace 
Abstract: Code clone detection plays a critical role in software maintenance and vulnerability analysis. Substantial methods have been proposed to detect code clones. However, they struggle to extract high-level program semantics directly from a single linear token sequence, leading to unsatisfactory detection performance. A similar single-sequence challenge has been successfully addressed in protein structure prediction by AlphaFold. Motivated by the successful resolution of the shared single-sequence challenge by AlphaFold, as well as the sequential similarities between proteins and code, we leverage AlphaFold for code clone detection. In particular, we propose AlphaCC, which represents code fragments as token sequences and adapts AlphaFold's sequence-to-structure modeling capability to infer code semantics. The pipeline of AlphaCC goes through three steps. First, AlphaCC transforms each input code fragment into a token sequence and, motivated by AlphaFold's use of multiple sequence alignment (MSA), novelly uses a retrieval-augmentation strategy to construct an MSA from lexically similar token sequences. Second, AlphaCC adopts a modified attention-based encoder based on AlphaFold to model dependencies within and across token sequences. Finally, unlike AlphaFold's protein structure prediction task, AlphaCC computes similarity scores between token sequences through a late interaction strategy and performs binary classification to determine code clone pairs. Comprehensive evaluations on three datasets, particularly two semantic clone detection datasets, show that AlphaCC consistently outperforms all baselines, demonstrating strong semantic understanding. AlphaCC further achieves strong performance on instances where tool-dependent methods fail, highlighting its tool-independence. Moreover, AlphaCC maintains competitive efficiency, enabling practical usage in large-scale clone detection tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.15226v2</guid></item><item><title>[arXiv-SE 2026] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs</title><link>https://arxiv.org/abs/2510.00031</link><description>arXiv:2510.00031v2 Announce Type: replace 
Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.00031v2</guid></item><item><title>[arXiv-SE 2026] SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</title><link>https://arxiv.org/abs/2601.22129</link><description>arXiv:2601.22129v2 Announce Type: replace 
Abstract: Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22129v2</guid></item><item><title>[arXiv-SE 2026] ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling</title><link>https://arxiv.org/abs/2602.03070</link><description>arXiv:2602.03070v2 Announce Type: replace-cross 
Abstract: Growing renewable penetration introduces substantial uncertainty into power system operations, necessitating frequent adaptation of dispatch objectives and constraints and challenging expertise-intensive, near-real-time modeling workflows. Large Language Models (LLMs) provide a promising avenue for automating this process by translating natural-language (NL) operational requirements into executable optimization models via semantic reasoning and code synthesis. Yet existing LLM datasets and benchmarks for optimization modeling primarily target coarse-grained cross-domain generalization, offering limited, rigorous evaluation in power-system settings, particularly for Optimal Power Flow (OPF). We therefore introduce \textbf{ProOPF-D} and \textbf{ProOPF-B}, a dataset and benchmark for professional-grade OPF modeling: ProOPF-D contains 12K instances pairing NL requests with parameter adjustments and structural extensions to a canonical OPF, together with executable implementations; ProOPF-B provides 121 expert-annotated test cases with ground-truth code, enabling end-to-end evaluation under both concrete and abstract OPF modeling regimes.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03070v2</guid></item><item><title>[arXiv-AI 2026] Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education</title><link>https://arxiv.org/abs/2602.05059</link><description>arXiv:2602.05059v1 Announce Type: new 
Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05059v1</guid></item><item><title>[arXiv-AI 2026] Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment</title><link>https://arxiv.org/abs/2602.05110</link><description>arXiv:2602.05110v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05110v1</guid></item><item><title>[arXiv-AI 2026] Aspect-Aware MOOC Recommendation in a Heterogeneous Network</title><link>https://arxiv.org/abs/2602.05297</link><description>arXiv:2602.05297v1 Announce Type: new 
Abstract: MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05297v1</guid></item><item><title>[arXiv-AI 2026] ProAct: Agentic Lookahead in Interactive Environments</title><link>https://arxiv.org/abs/2602.05327</link><description>arXiv:2602.05327v1 Announce Type: new 
Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05327v1</guid></item><item><title>[arXiv-AI 2026] Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning</title><link>https://arxiv.org/abs/2602.05464</link><description>arXiv:2602.05464v1 Announce Type: new 
Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05464v1</guid></item><item><title>[arXiv-AI 2026] ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</title><link>https://arxiv.org/abs/2602.05472</link><description>arXiv:2602.05472v1 Announce Type: new 
Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05472v1</guid></item><item><title>[arXiv-AI 2026] Graph-based Agent Memory: Taxonomy, Techniques, and Applications</title><link>https://arxiv.org/abs/2602.05665</link><description>arXiv:2602.05665v1 Announce Type: new 
Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05665v1</guid></item><item><title>[arXiv-AI 2026] TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05818</link><description>arXiv:2602.05818v1 Announce Type: new 
Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05818v1</guid></item><item><title>[arXiv-AI 2026] Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy</title><link>https://arxiv.org/abs/2602.05877</link><description>arXiv:2602.05877v1 Announce Type: new 
Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05877v1</guid></item><item><title>[arXiv-AI 2026] AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</title><link>https://arxiv.org/abs/2602.06008</link><description>arXiv:2602.06008v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06008v1</guid></item><item><title>[arXiv-AI 2026] DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</title><link>https://arxiv.org/abs/2602.06039</link><description>arXiv:2602.06039v1 Announce Type: new 
Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06039v1</guid></item><item><title>[arXiv-AI 2026] Internalizing LLM Reasoning via Discovery and Replay of Latent Actions</title><link>https://arxiv.org/abs/2602.04925</link><description>arXiv:2602.04925v1 Announce Type: cross 
Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04925v1</guid></item><item><title>[arXiv-AI 2026] E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</title><link>https://arxiv.org/abs/2602.05068</link><description>arXiv:2602.05068v1 Announce Type: cross 
Abstract: Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $\epsilon-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05068v1</guid></item><item><title>[arXiv-AI 2026] LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation</title><link>https://arxiv.org/abs/2602.05493</link><description>arXiv:2602.05493v1 Announce Type: cross 
Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05493v1</guid></item><item><title>[arXiv-AI 2026] DARWIN: Dynamic Agentically Rewriting Self-Improving Network</title><link>https://arxiv.org/abs/2602.05848</link><description>arXiv:2602.05848v1 Announce Type: cross 
Abstract: DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05848v1</guid></item><item><title>[arXiv-AI 2026] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</title><link>https://arxiv.org/abs/2602.05885</link><description>arXiv:2602.05885v1 Announce Type: cross 
Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05885v1</guid></item><item><title>[arXiv-AI 2026] CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</title><link>https://arxiv.org/abs/2602.06038</link><description>arXiv:2602.06038v1 Announce Type: cross 
Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06038v1</guid></item><item><title>[arXiv-AI 2026] Robust Answers, Fragile Logic: Probing the Decoupling Hypothesis in LLM Reasoning</title><link>https://arxiv.org/abs/2505.17406</link><description>arXiv:2505.17406v2 Announce Type: replace 
Abstract: While Chain-of-Thought (CoT) prompting has become a cornerstone for complex reasoning in Large Language Models (LLMs), the faithfulness of the generated reasoning remains an open question. We investigate the Decoupling Hypothesis: that correct answers often mask fragile, post-hoc rationalizations that are not causally tied to the model's prediction. To systematically verify this, we introduce MATCHA, a novel Answer-Conditioned Probing framework. Unlike standard evaluations that focus on final output accuracy, MATCHA isolates the reasoning phase by conditioning generation on the model's predicted answer, allowing us to stress-test the stability of the rationale itself. Our experiments reveal a critical vulnerability: under imperceptible input perturbations, LLMs frequently maintain the correct answer while generating inconsistent or nonsensical reasoning - effectively being ``Right for the Wrong Reasons''. Using LLM judges to quantify this robustness gap, we find that multi-step and commonsense tasks are significantly more susceptible to this decoupling than logical tasks. Furthermore, we demonstrate that adversarial examples generated by MATCHA transfer non-trivially to black-box models. Our findings expose the illusion of CoT robustness and underscore the need for future architectures that enforce genuine answer-reasoning consistency rather than mere surface-level accuracy.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17406v2</guid></item><item><title>[arXiv-AI 2026] Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers</title><link>https://arxiv.org/abs/2506.13342</link><description>arXiv:2506.13342v2 Announce Type: replace 
Abstract: Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend that future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.13342v2</guid></item><item><title>[arXiv-AI 2026] DeepAgent: A General Reasoning Agent with Scalable Toolsets</title><link>https://arxiv.org/abs/2510.21618</link><description>arXiv:2510.21618v3 Announce Type: replace 
Abstract: Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To manage long-horizon interactions, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.21618v3</guid></item><item><title>[arXiv-AI 2026] The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution</title><link>https://arxiv.org/abs/2601.15075</link><description>arXiv:2601.15075v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining \textbf{the reason behind agent behaviors}. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems. Codes are available at https://github.com/AI45Lab/AgentDoG.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15075v2</guid></item><item><title>[arXiv-AI 2026] A Study of Adaptive Modeling Towards Robust Generalization</title><link>https://arxiv.org/abs/2602.02780</link><description>arXiv:2602.02780v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly support reasoning over biomolecular structures, but most existing approaches remain modality-specific and rely on either sequence-style encodings or fixed-length connector tokens for structural inputs. These designs can under-expose explicit geometric cues and impose rigid fusion bottlenecks, leading to over-compression and poor token allocation as structural complexity grows. We present a unified all-atom framework that grounds language reasoning in geometric information while adaptively scaling structural tokens. The method first constructs variable-size structural patches on molecular graphs using an instruction-conditioned gating policy, enabling complexity-aware allocation of query tokens. It then refines the resulting patch tokens via cross-attention with modality embeddings and injects geometry-informed tokens into the language model to improve structure grounding and reduce structural hallucinations. Across diverse all-atom benchmarks, the proposed approach yields consistent gains in heterogeneous structure-grounded reasoning. An anonymized implementation is provided in the supplementary material.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02780v2</guid></item><item><title>[arXiv-AI 2026] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2503.06749</link><description>arXiv:2503.06749v3 Announce Type: replace-cross 
Abstract: DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.06749v3</guid></item><item><title>[arXiv-AI 2026] SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?</title><link>https://arxiv.org/abs/2505.20295</link><description>arXiv:2505.20295v4 Announce Type: replace-cross 
Abstract: The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables. To support the development of this universal form of LLM uncertainties, we publish the code that implements our metric for arbitrary LLMs under https://github.com/apple/ml-selfreflect .</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.20295v4</guid></item><item><title>[arXiv-AI 2026] STACK: Adversarial Attacks on LLM Safeguard Pipelines</title><link>https://arxiv.org/abs/2506.24068</link><description>arXiv:2506.24068v3 Announce Type: replace-cross 
Abstract: Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic and OpenAI guard their latest Opus 4 model and GPT-5 models using such defense pipelines, and other frontier developers including Google DeepMind pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.24068v3</guid></item><item><title>[arXiv-AI 2026] CellForge: Agentic Design of Virtual Cell Models</title><link>https://arxiv.org/abs/2508.02276</link><description>arXiv:2508.02276v2 Announce Type: replace-cross 
Abstract: Virtual cell modeling aims to predict cellular responses to diverse perturbations but faces challenges from biological complexity, multimodal data heterogeneity, and the need for interdisciplinary expertise. We introduce CellForge, a multi-agent framework that autonomously designs and synthesizes neural network architectures tailored to specific single-cell datasets and perturbation tasks. Given raw multi-omics data and task descriptions, CellForge discovers candidate architectures through collaborative reasoning among specialized agents, then generates executable implementations. Our core contribution is the framework itself: showing that multi-agent collaboration mechanisms - rather than manual human design or single-LLM prompting - can autonomously produce executable, high-quality computational methods. This approach goes beyond conventional hyperparameter tuning by enabling entirely new architectural components such as trajectory-aware encoders and perturbation diffusion modules to emerge from agentic deliberation. We evaluate CellForge on six datasets spanning gene knockouts, drug treatments, and cytokine stimulations across multiple modalities (scRNA-seq, scATAC-seq, CITE-seq). The results demonstrate that the models generated by CellForge are highly competitive with established baselines, while revealing systematic patterns of architectural innovation. CellForge highlights the scientific value of multi-agent frameworks: collaboration among specialized agents enables genuine methodological innovation and executable solutions that single agents or human experts cannot achieve. This represents a paradigm shift toward autonomous scientific method development in computational biology. Code is available at https://github.com/gersteinlab/CellForge.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.02276v2</guid></item><item><title>[arXiv-AI 2026] Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>arXiv:2509.03531v2 Announce Type: replace-cross 
Abstract: Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets entity-level hallucinations-e.g., fabricated names, dates, citations-rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Despite being trained only to detect hallucinated entities, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.03531v2</guid></item><item><title>[arXiv-AI 2026] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</title><link>https://arxiv.org/abs/2511.11007</link><description>arXiv:2511.11007v2 Announce Type: replace-cross 
Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.11007v2</guid></item><item><title>[arXiv-AI 2026] CARL: Focusing Agentic Reinforcement Learning on Critical Actions</title><link>https://arxiv.org/abs/2512.04949</link><description>arXiv:2512.04949v2 Announce Type: replace-cross 
Abstract: Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for long-horizon agentic reasoning. CARL leverages entropy as a heuristic proxy for action criticality and achieves focused training by assigning rewards to high-criticality actions while excluding low-criticality actions from model updates, avoiding noisy credit assignment and redundant computation. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency across diverse evaluation settings. The source code will be publicly available.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.04949v2</guid></item><item><title>[arXiv-AI 2026] Learning to Discover at Test Time</title><link>https://arxiv.org/abs/2601.16175</link><description>arXiv:2601.16175v2 Announce Type: replace-cross 
Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\H{o}s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16175v2</guid></item><item><title>[arXiv-AI 2026] STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification</title><link>https://arxiv.org/abs/2601.19903</link><description>arXiv:2601.19903v2 Announce Type: replace-cross 
Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19903v2</guid></item><item><title>[arXiv-AI 2026] Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints</title><link>https://arxiv.org/abs/2602.00166</link><description>arXiv:2602.00166v2 Announce Type: replace-cross 
Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00166v2</guid></item><item><title>[arXiv-AI 2026] Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</title><link>https://arxiv.org/abs/2602.03190</link><description>arXiv:2602.03190v2 Announce Type: replace-cross 
Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 45.2 per-benchmark accuracy and 51.8 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03190v2</guid></item><item><title>[arXiv-LG 2026] Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog</title><link>https://arxiv.org/abs/2602.04919</link><description>arXiv:2602.04919v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04919v1</guid></item><item><title>[arXiv-LG 2026] SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines</title><link>https://arxiv.org/abs/2602.05134</link><description>arXiv:2602.05134v1 Announce Type: new 
Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05134v1</guid></item><item><title>[arXiv-LG 2026] BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs</title><link>https://arxiv.org/abs/2602.05448</link><description>arXiv:2602.05448v1 Announce Type: new 
Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05448v1</guid></item><item><title>[arXiv-LG 2026] EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking</title><link>https://arxiv.org/abs/2602.05571</link><description>arXiv:2602.05571v1 Announce Type: new 
Abstract: Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\%, a +3.8 pp improvement over the prior state of the art (74.2\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05571v1</guid></item><item><title>[arXiv-LG 2026] ContextBench: A Benchmark for Context Retrieval in Coding Agents</title><link>https://arxiv.org/abs/2602.05892</link><description>arXiv:2602.05892v1 Announce Type: new 
Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05892v1</guid></item><item><title>[arXiv-LG 2026] Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training</title><link>https://arxiv.org/abs/2602.05933</link><description>arXiv:2602.05933v1 Announce Type: new 
Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$\chi^2$ regularizer. This additional $\chi^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05933v1</guid></item><item><title>[arXiv-LG 2026] Atomic Information Flow: A Network Flow Model for Tool Attributions in RAG Systems</title><link>https://arxiv.org/abs/2602.04912</link><description>arXiv:2602.04912v1 Announce Type: cross 
Abstract: Many tool-based Retrieval Augmented Generation (RAG) systems lack precise mechanisms for tracing final responses back to specific tool components -- a critical gap as systems scale to complex multi-agent architectures. We present \textbf{Atomic Information Flow (AIF)}, a graph-based network flow model that decomposes tool outputs and LLM calls into atoms: indivisible, self-contained units of information. By modeling LLM orchestration as a directed flow of atoms from tool and LLM nodes to a response super-sink, AIF enables granular attribution metrics for AI explainability.
  Motivated by the max-flow min-cut theorem in network flow theory, we train a lightweight Gemma3 (4B parameter) language model as a context compressor to approximate the minimum cut of tool atoms using flow signals computed offline by AIF. We note that the base Gemma3-4B model struggles to identify critical information with \textbf{54.7\%} accuracy on HotpotQA, barely outperforming lexical baselines (BM25). However, post-training on AIF signals boosts accuracy to \textbf{82.71\%} (+28.01 points) while achieving \textbf{87.52\%} (+1.85\%) context token compression -- bridging the gap with the Gemma3-27B variant, a model nearly $7\times$ larger.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04912v1</guid></item><item><title>[arXiv-LG 2026] Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2602.04926</link><description>arXiv:2602.04926v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04926v1</guid></item><item><title>[arXiv-LG 2026] VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model</title><link>https://arxiv.org/abs/2502.01989</link><description>arXiv:2502.01989v4 Announce Type: replace 
Abstract: Inspired by human SYSTEM 2 thinking, LLMs excel at complex reasoning tasks via extended Chain-of-Thought. However, similar test-time scaling for diffusion models to tackle complex reasoning remains largely unexplored. From existing work, two primary challenges emerge in this setting: (i) the dependence on an external verifier indicating a notable gap from intrinsic reasoning of human intelligence without any external feedback, and (ii) the lack of an efficient search algorithm. In this paper, we introduce the Verifier-free Test-time Scalable Diffusion Model (VFScale) to achieve scalable intrinsic reasoning, which equips number-of-sample test-time scaling with the intrinsic energy function of diffusion models as the verifier. Concretely, VFScale comprises two key innovations to address the aforementioned challenges. On the training side, VFScale consists of a novel MRNCL loss and a KL regularization to improve the energy landscape, ensuring that the learned energy function itself serves as a reliable verifier. On the inference side, VFScale integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS) to improve search efficiency. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of VFScale's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our VFScale solves 88% of Maze problems with much larger sizes of $15\times15$, while standard diffusion models completely fail. The code can be found at https://github.com/AI4Science-WestlakeU/VFScale.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2502.01989v4</guid></item><item><title>[arXiv-LG 2026] Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning</title><link>https://arxiv.org/abs/2510.00761</link><description>arXiv:2510.00761v4 Announce Type: replace 
Abstract: Large language model (LLM) unlearning aims to surgically remove the influence of undesired data or knowledge from an existing model while preserving its utility on unrelated tasks. This paradigm has shown promise in addressing privacy and safety concerns. However, recent findings reveal that unlearning effects are often fragile: post-unlearning manipulations such as weight quantization or fine-tuning can quickly neutralize the intended forgetting. Prior efforts to improve robustness primarily reformulate unlearning objectives by explicitly assuming the role of vulnerability sources. In this work, we take a different perspective by investigating the role of the optimizer, independent of unlearning objectives and formulations, in shaping unlearning robustness. We show that the 'grade' of the optimizer, defined by the level of information it exploits, ranging from zeroth-order (gradient-free) to first-order (gradient-based) to second-order (Hessian-based), is tightly linked to the resilience of unlearning. Surprisingly, we find that downgrading the optimizer, such as using zeroth-order methods or compressed-gradient variants (e.g., gradient sign-based optimizers), often leads to stronger robustness. While these optimizers produce noisier and less precise updates, they encourage convergence to harder-to-disturb basins in the loss landscape, thereby resisting post-training perturbations. By connecting zeroth-order methods with randomized smoothing, we further highlight their natural advantage for robust unlearning. Motivated by these insights, we propose a hybrid optimizer that combines first-order and zeroth-order updates, preserving unlearning efficacy while enhancing robustness. Extensive experiments on the MUSE and WMDP benchmarks, across multiple LLM unlearning algorithms, validate that our approach achieves more resilient forgetting without sacrificing unlearning quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.00761v4</guid></item><item><title>[arXiv-CL 2026] Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions</title><link>https://arxiv.org/abs/2602.05220</link><description>arXiv:2602.05220v1 Announce Type: new 
Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05220v1</guid></item><item><title>[arXiv-CL 2026] IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models</title><link>https://arxiv.org/abs/2602.05385</link><description>arXiv:2602.05385v1 Announce Type: new 
Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05385v1</guid></item><item><title>[arXiv-CL 2026] A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering</title><link>https://arxiv.org/abs/2602.05512</link><description>arXiv:2602.05512v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05512v1</guid></item><item><title>[arXiv-CL 2026] OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</title><link>https://arxiv.org/abs/2602.05843</link><description>arXiv:2602.05843v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., &gt; 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05843v1</guid></item><item><title>[arXiv-CL 2026] Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models</title><link>https://arxiv.org/abs/2602.05897</link><description>arXiv:2602.05897v1 Announce Type: new 
Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05897v1</guid></item><item><title>[arXiv-CL 2026] Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning</title><link>https://arxiv.org/abs/2503.16252</link><description>arXiv:2503.16252v4 Announce Type: replace 
Abstract: In recent years, general-purpose large language models (LLMs) such as GPT, Gemini, Claude, and DeepSeek have advanced at an unprecedented pace. Despite these achievements, their application to finance remains challenging, due to fragmented data sources, intransparent reasoning processes, and weak transferability to business applications. In response, we introduce Fin-R1, a reasoning LLM designed for financial scenarios. With a compact size of 7 billion parameters, Fin-R1 reduces deployment costs while addressing the aforementioned challenges. Its development follows a two-stage pipeline. First, we construct Fin-R1-Data, a high-quality financial dataset consisting of 60,091 chain-of-thought (CoT) samples, distilled and filtered from multiple authoritative benchmarks to ensure consistency and reliability. Second, we train Fin-R1 using Fin-R1-Data through supervised fine-tuning (SFT), followed by reinforcement learning (RL). This stage substantially improves the model's ability to solve complex financial reasoning tasks, yielding outputs that are both accurate and interpretable. Despite its relatively small parameter scale, Fin-R1 achieves competitive empirical performance across established financial benchmarks and demonstrates practical utility in compliance checking and robo-advisory. Our code is publicly available at https://github.com/SUFE-AIFLM-Lab/Fin-R1, and has already attracted over 700 stars.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.16252v4</guid></item><item><title>[arXiv-CL 2026] Horizon-LM: A RAM-Centric Architecture for LLM Training</title><link>https://arxiv.org/abs/2602.04816</link><description>arXiv:2602.04816v2 Announce Type: replace-cross 
Abstract: The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04816v2</guid></item><item><title>[arXiv-IR 2026] SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval</title><link>https://arxiv.org/abs/2602.04451</link><description>arXiv:2602.04451v2 Announce Type: replace 
Abstract: Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.</description><author>cs.IR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04451v2</guid></item><item><title>[arXiv-statML 2026] Quantum Circuit Generation via test-time learning with large language models</title><link>https://arxiv.org/abs/2602.03466</link><description>arXiv:2602.03466v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can generate structured artifacts, but using them as dependable optimizers for scientific design requires a mechanism for iterative improvement under black-box evaluation. Here, we cast quantum circuit synthesis as a closed-loop, test-time optimization problem: an LLM proposes edits to a fixed-length gate list, and an external simulator evaluates the resulting state with the Meyer-Wallach (MW) global entanglement measure. We introduce a lightweight test-time learning recipe that can reuse prior high-performing candidates as an explicit memory trace, augments prompts with a score-difference feedback, and applies restart-from-the-best sampling to escape potential plateaus. Across fixed 20-qubit settings, the loop without feedback and restart-from-the-best improves random initial circuits over a range of gate budgets. To lift up this performance and success rate, we use the full learning strategy. For the 25-qubit, it mitigates a pronounced performance plateau when naive querying is used. Beyond raw scores, we analyze the structure of synthesized states and find that high MW solutions can correspond to stabilizer or graph-state-like constructions, but full connectivity is not guaranteed due to the metric property and prompt design. These results illustrate both the promise and the pitfalls of memory evaluator-guided LLM optimization for circuit synthesis, highlighting the critical role of prior human-made theoretical theorems to optimally design a custom tool in support of research.</description><author>stat.ML updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03466v2</guid></item><item><title>[TSE 2026] The Power of Small LLMs: A Multi-Agent for Code Generation via Dynamic Precaution Tuning.</title><link>https://doi.org/10.1109/TSE.2025.3632508</link><description></description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/ZhangLTZ26</guid></item><item><title>[TSE 2026] Shield Broken: Black-Box Adversarial Attacks on LLM-Based Vulnerability Detectors.</title><link>https://doi.org/10.1109/TSE.2025.3638998</link><description></description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/JiangHTSW26</guid></item><item><title>[TOSEM 2026] PonziHunter: Hunting Ethereum Ponzi Contract via Static Analysis and Contrastive Learning on the Bytecode Level</title><link>https://dl.acm.org/doi/abs/10.1145/3735971?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-21, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Wed, 21 Jan 2026 04:18:39 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3735971?af=R</guid></item><item><title>[TOSEM 2026] Abundant Modalities Offer More Nutrients: Multi-Modal-Based Function-Level Vulnerability Detection</title><link>https://dl.acm.org/doi/abs/10.1145/3731557?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-31, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Tue, 20 Jan 2026 02:02:29 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3731557?af=R</guid></item><item><title>[TDSC 2025] HgtJIT: Just-in-Time Vulnerability Detection Based on Heterogeneous Graph Transformer.</title><link>https://doi.org/10.1109/TDSC.2025.3586669</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/SunZCWBWLX25</guid></item><item><title>[TDSC 2025] Tacco: A Framework for Ensuring the Security of Real-World TEEs via Formal Verification.</title><link>https://doi.org/10.1109/TDSC.2025.3594594</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/HuZPDR25</guid></item><item><title>[TDSC 2025] Real-World Code Vulnerability Detection Framework: From Data Preprocessing to Multi-Feature Fusion Detection.</title><link>https://doi.org/10.1109/TDSC.2025.3601228</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/ZhangDLLHL25</guid></item><item><title>[CCS 2025] Autonomous Vulnerability Analysis, Triaging, and Repair: A Historical Perspective.</title><link>https://doi.org/10.1145/3719027.3748270</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Vigna25</guid></item><item><title>[CCS 2025] SyzSpec: Specification Generation for Linux Kernel Fuzzing via Under-Constrained Symbolic Execution.</title><link>https://doi.org/10.1145/3719027.3744811</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/0006PLQS25</guid></item><item><title>[CCS 2025] Protocols to Code: Formal Verification of a Secure Next-Generation Internet Router.</title><link>https://doi.org/10.1145/3719027.3765104</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/PereiraKGLSWE0B25</guid></item><item><title>[CCS 2025] Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator.</title><link>https://doi.org/10.1145/3719027.3765193</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/HallyburtonP25</guid></item><item><title>[CCS 2025] ZVDetector: State-Guided Vulnerability Detection System for Zigbee Devices.</title><link>https://doi.org/10.1145/3719027.3765035</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lin00WB25</guid></item><item><title>[CCS 2025] Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection.</title><link>https://doi.org/10.1145/3719027.3765027</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/BarsBSH25</guid></item><item><title>[CCS 2025] Security Analysis of Privately Verifiable Privacy Pass.</title><link>https://doi.org/10.1145/3719027.3765172</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/HanffLO25</guid></item><item><title>[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.</title><link>https://doi.org/10.1145/3719027.3765049</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/LinWQCM25</guid></item><item><title>[CCS 2025] Augmenting Search-based Program Synthesis with Local Inference Rules to Improve Black-box Deobfuscation.</title><link>https://doi.org/10.1145/3719027.3765134</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Attias0MBM25</guid></item><item><title>[CCS 2025] Poster: LogCraft: Crafting CVE-Aware Synthetic Worlds (Logs).</title><link>https://doi.org/10.1145/3719027.3760736</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/WongTHGJWC25</guid></item><item><title>[CCS 2025] AI-Augmented Static Analysis: Bridging Heuristics and Completeness for Practical Reverse Engineering.</title><link>https://doi.org/10.1145/3719027.3765565</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Santra25</guid></item><item><title>[CCS 2025] LAMPS '25: ACM CCS Workshop on Large AI Systems and Models with Privacy and Security Analysis.</title><link>https://doi.org/10.1145/3719027.3767670</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lam0W0XC0YBW25</guid></item><item><title>[USENIXSec 2025] Game of Arrows: On the (In-)Security of Weight Obfuscation for On-Device TEE-Shielded LLM Partition Algorithms.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-pengli</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangDCZLXWZZ25</guid></item><item><title>[USENIXSec 2025] LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lekssays</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LekssaysMT0K25</guid></item><item><title>[USENIXSec 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/krauss</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KraussDD25</guid></item><item><title>[USENIXSec 2025] Confusing Value with Enumeration: Studying the Use of CVEs in Academia.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/schloegel</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/SchloegelK000WT25</guid></item><item><title>[USENIXSec 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM Analysis.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kim-youngjoon</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Kim0KY25</guid></item><item><title>[USENIXSec 2025] A Thorough Security Analysis of BLE Proximity Tracking Protocols.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/liu-xiaofeng</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0013ZHR0ZG25</guid></item><item><title>[USENIXSec 2025] SCASE: Automated Secret Recovery via Side-Channel-Assisted Symbolic Execution.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/weber</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00070TLB025</guid></item><item><title>[USENIXSec 2025] Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kwesi</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KwesiCMN25</guid></item><item><title>[USENIXSec 2025] Hybrid Language Processor Fuzzing via LLM-Based Constraint Solving.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/yang-yupeng</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/YangYCL25</guid></item><item><title>[USENIXSec 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for Static Analysis Result Verification.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/bao-andrew</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/BaoZWCMY25</guid></item><item><title>[USENIXSec 2025] A Comprehensive Formal Security Analysis of OPC UA.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/diemunsch</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/DiemunschHK25</guid></item><item><title>[USENIXSec 2025] ZIPPER: Static Taint Analysis for PHP Applications with Precision and Efficiency.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-xinyi</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangLLCSMZ00H25</guid></item><item><title>[USENIXSec 2025] Effective Directed Fuzzing with Hierarchical Scheduling for Web Vulnerability Detection.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lin-zihan</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Lin0DHX0Y0C025</guid></item><item><title>[ASE 2025] GPTVD: vulnerability detection and analysis method based on LLM's chain of thoughts.</title><link>https://doi.org/10.1007/s10515-025-00550-4</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/ChenHCSY26</guid></item><item><title>[ASE 2025] HMF: Enhancing reentrancy vulnerability detection and repair with a hybrid model framework.</title><link>https://doi.org/10.1007/s10515-025-00546-0</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/LiSRFLS26</guid></item><item><title>[ASE 2025] Graph neural networks for precise bug localization through structural program analysis.</title><link>https://doi.org/10.1007/s10515-025-00556-y</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YousofvandSRN26</guid></item><item><title>[ASE 2025] ByteEye: A smart contract vulnerability detection framework at bytecode level with graph neural networks.</title><link>https://doi.org/10.1007/s10515-025-00559-9</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YangLDFXL26</guid></item><item><title>[SOSP 2025] KNighter: Transforming Static Analysis with LLM-Synthesized Checkers.</title><link>https://doi.org/10.1145/3731569.3764827</link><description></description><author>dblp: new volumes for streams/conf/sosp</author><pubDate>Wed, 01 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sosp/YangZXLZ25</guid></item><item><title>[ACL 2025] Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation.</title><link>https://doi.org/10.18653/v1/2025.acl-short.93</link><description></description><author>dblp: new volumes for streams/conf/acl</author><pubDate>Wed, 24 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acl/QinZSWXRHTTWJF025</guid></item><item><title>[IJCAI 2025] AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.</title><link>https://doi.org/10.24963/ijcai.2025/2</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/AnokhinSSEK0B25</guid></item><item><title>[IJCAI 2025] Relational Decomposition for Program Synthesis.</title><link>https://doi.org/10.24963/ijcai.2025/504</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HocquetteC25</guid></item><item><title>[IJCAI 2025] POLO: An LLM-Powered Project-Level Code Performance Optimization Framework.</title><link>https://doi.org/10.24963/ijcai.2025/814</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/BaiXWY0025</guid></item><item><title>[IJCAI 2025] APIMig: A Project-Level Cross-Multi-Version API Migration Framework Based on Evolution Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/829</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/KuangXYYWKX25</guid></item><item><title>[IJCAI 2025] Can We Translate Code Better with LLMs and Call Graph Analysis?</title><link>https://doi.org/10.24963/ijcai.2025/848</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Luo25</guid></item><item><title>[IJCAI 2025] SecV: LLM-based Secure Verilog Generation with Clue-Guided Exploration on Hardware-CWE Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/895</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/FanXK25</guid></item><item><title>[IJCAI 2025] Learn to Think: Bootstrapping LLM Logic Through Graph Representation Learning.</title><link>https://doi.org/10.24963/ijcai.2025/896</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0004ZWZWZ025</guid></item><item><title>[IJCAI 2025] SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation.</title><link>https://doi.org/10.24963/ijcai.2025/965</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/XuLLG25</guid></item><item><title>[IJCAI 2025] The Graph's Apprentice: Teaching an LLM Low-Level Knowledge for Circuit Quality Estimation.</title><link>https://doi.org/10.24963/ijcai.2025/1033</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/MoravejBZCT0ZHY25</guid></item><item><title>[IJCAI 2025] GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs.</title><link>https://doi.org/10.24963/ijcai.2025/1256</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/DaSLZ025</guid></item><item><title>[EuroS&amp;P 2025] Mitigating Information Leakage in Large Language Models: Evaluating the Impact of Code Obfuscation on Vulnerability Detection.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00007</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/GulayY25</guid></item><item><title>[EuroS&amp;P 2025] CFA-Bench: Cybersecurity Forensic Llm Agent Benchmark and Testing.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00031</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/SantisHVGMBR25</guid></item><item><title>[OSDI 2025] Paralegal: Practical Static Analysis for Privacy Bugs.</title><link>https://www.usenix.org/conference/osdi25/presentation/adam</link><description></description><author>dblp: new volumes for streams/conf/osdi</author><pubDate>Wed, 16 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/osdi/AdamZZRHJCKS25</guid></item><item><title>[ISSTA 2025] TBFV4J: An Automated Testing-Based Formal Verification Tool for Java.</title><link>https://doi.org/10.1145/3713081.3731740</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/LiuLL25</guid></item><item><title>[ISSTA 2025] Revisiting the Combination of Static Analysis Error Traces and Dynamic Symbolic Execution: A Potential Approach for True Positive Confirmation (Registered Report).</title><link>https://doi.org/10.1145/3713081.3731720</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/Xu0P25</guid></item><item><title>[ISSTA 2025] A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection.</title><link>https://doi.org/10.1145/3713081.3731746</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/YuSF0TK025</guid></item><item><title>[ICLR 2025] Diffusion On Syntax Trees For Program Synthesis.</title><link>https://openreview.net/forum?id=wN3KaUXA5X</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/KapurJ025</guid></item><item><title>[ICLR 2025] MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code.</title><link>https://openreview.net/forum?id=1Iuw1jcIrf</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/LuZ0RSPZL25</guid></item><item><title>[ICLR 2025] EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing.</title><link>https://openreview.net/forum?id=Y2Dh8rWwlb</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhengCHGLYLWWW25</guid></item><item><title>[ICLR 2025] SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?</title><link>https://openreview.net/forum?id=riTiq3i21b</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/YangJZLYWPMSNY025</guid></item><item><title>[ICLR 2025] GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation.</title><link>https://openreview.net/forum?id=5RUM1aIdok</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/FengSY25</guid></item><item><title>[ICLR 2025] GraphRouter: A Graph-based Router for LLM Selections.</title><link>https://openreview.net/forum?id=eU39PDsZtT</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/FengSY25a</guid></item><item><title>[ICLR 2025] Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment.</title><link>https://openreview.net/forum?id=kN25ggeq1J</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhaoJFH0LM0C25</guid></item><item><title>[ICLR 2025] CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &amp; Reasoning Capabilities of CodeLLMs.</title><link>https://openreview.net/forum?id=CahIEKCu5Q</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/NguyenPHDNPB25</guid></item><item><title>[ICLR 2025] Steering Large Language Models between Code Execution and Textual Reasoning.</title><link>https://openreview.net/forum?id=5X5Z7Ffrjb</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ChenJSFW25</guid></item><item><title>[ICLR 2025] RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph.</title><link>https://openreview.net/forum?id=dw9VUsSHGB</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Ouyang0MX0J00025</guid></item><item><title>[ICLR 2025] Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents.</title><link>https://openreview.net/forum?id=V4y0CpX4hK</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhangHMYWZWZ25</guid></item><item><title>[ICLR 2025] Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control.</title><link>https://openreview.net/forum?id=l32IrJtpOP</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ShinK25</guid></item><item><title>[ICLR 2025] IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities.</title><link>https://openreview.net/forum?id=9LdJDU7E91</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Li0N25</guid></item><item><title>[ICLR 2025] RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code.</title><link>https://openreview.net/forum?id=NiNIthntx7</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/GautamGJSM25</guid></item><item><title>[ICLR 2025] CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning.</title><link>https://openreview.net/forum?id=dCPF1wlqj8</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Wen0W0H25</guid></item><item><title>[ICLR 2025] Safety Layers in Aligned Large Language Models: The Key to LLM Security.</title><link>https://openreview.net/forum?id=kUH1yPMAn7</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/LiY0L25</guid></item><item><title>[ICLR 2025] HaDeMiF: Hallucination Detection and Mitigation in Large Language Models.</title><link>https://openreview.net/forum?id=VwOYxPScxB</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/ZhouZL0Z25</guid></item><item><title>[ICLR 2025] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation.</title><link>https://openreview.net/forum?id=sGpCzsfd1K</link><description></description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/0002SLS0JXZLZLN25</guid></item><item><title>[ACSAC 2025] Learning to Unfix: Towards ML Robustness in Vulnerability Detection via Structure-Aware Code Generation.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00014</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/Rozi024</guid></item><item><title>[ACSAC 2025] AdVul: Adversarial Attack against ML-based Vulnerability Detection.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00018</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/KatohPX24</guid></item><item><title>[ACSAC 2025] Software Vulnerability Detection Using LLM: Does Additional Information Help?</title><link>https://doi.org/10.1109/ACSACW65225.2024.00031</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/ShimmiSSOR24</guid></item><item><title>[ACSAC 2025] Automated Vulnerability Detection in Smart Contracts using Control Flow Graphs and Machine Learning.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00037</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/LohestBL24</guid></item><item><title>[NDSS 2025] Generating API Parameter Security Rules with LLM for API Misuse Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/generating-api-parameter-security-rules-with-llm-for-api-misuse-detection/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LiuY0L25</guid></item><item><title>[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/from-large-to-mammoth-a-comparative-evaluation-of-large-language-models-in-vulnerability-detection/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LinM25</guid></item><item><title>[NDSS 2025] PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation.</title><link>https://www.ndss-symposium.org/ndss-paper/propertygpt-llm-driven-formal-verification-of-smart-contracts-through-retrieval-augmented-property-generation/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/0012XW00S025</guid></item><item><title>[NDSS 2025] You Can Rand but You Can't Hide: A Holistic Security Analysis of Google Fuchsia's (and gVisor's) Network Stack.</title><link>https://www.ndss-symposium.org/ndss-paper/you-can-rand-but-you-cant-hide-a-holistic-security-analysis-of-google-fuchsias-and-gvisors-network-stack/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/KaplanE025</guid></item><item><title>[NeurIPS 2025] SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/YinXWZL24</guid></item><item><title>[NeurIPS 2025] Can Graph Learning Improve Planning in LLM-based Agents?</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/098d1bd3eb6156a4c2f834563cdcf617-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WuSSSWZFCCXL24</guid></item><item><title>[NeurIPS 2025] LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0b77d3a82b59e9d9899370b378087faf-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangZL024</guid></item><item><title>[NeurIPS 2025] HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1c9c85bae6161d52182d0fe2f3640512-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BarkeGKBP24</guid></item><item><title>[NeurIPS 2025] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/62c6d7893b13a13c659cb815852dd00d-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LinCHWLLSL24</guid></item><item><title>[NeurIPS 2025] NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/69d97a6493fbf016fff0a751f253ad18-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ShaoJUDxM0YGKKK24</guid></item><item><title>[NeurIPS 2025] SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/6efcc7fd8efeee29a050a79c843c90e0-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DingPMKYR24</guid></item><item><title>[NeurIPS 2025] GraphVis: Boosting LLMs with Visual Knowledge Graph Integration.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7cb04f510593c9ba30da398f5e0a7e7b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Deng00MK024</guid></item><item><title>[NeurIPS 2025] LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/8196be81e68289d7a9ece21ed7f5750a-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiLDLWXSWSRGSS24</guid></item><item><title>[NeurIPS 2025] WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/820c61a0cd419163ccbd2c33b268816e-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0008KE24</guid></item><item><title>[NeurIPS 2025] SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/94f093b41fc2666376fb1f667fe282f3-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MundlerMHV24</guid></item><item><title>[NeurIPS 2025] HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ba92705991cfbbcedc26e27e833ebbae-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DuX024</guid></item><item><title>[NeurIPS 2025] RedCode: Risky Code Execution and Generation Benchmark for Code Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/bfd082c452dffb450d5a5202b0419205-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/GuoLXZZ0SL24</guid></item><item><title>[NeurIPS 2025] Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Wang0LZH024</guid></item><item><title>[NeurIPS 2025] Suitable is the Best: Task-Oriented Knowledge Fusion in Vulnerability Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/db7a81128155d6f14970c12d0b5e7a4c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangHN0DKDK24</guid></item><item><title>[ASE 2025] Language-Agnostic Static Analysis of Probabilistic Programs.</title><link>https://doi.org/10.1145/3691620.3695031</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Bock0C24</guid></item><item><title>[ASE 2025] GPP: A Graph-Powered Prioritizer for Code Review Requests.</title><link>https://doi.org/10.1145/3691620.3694990</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/YangX0WLLB24</guid></item><item><title>[ASE 2025] Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models.</title><link>https://doi.org/10.1145/3691620.3695013</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Wu0YG024</guid></item><item><title>[ASE 2025] Program Synthesis Meets Visual What-Comes-Next Puzzles.</title><link>https://doi.org/10.1145/3691620.3695015</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LahiriKCV024</guid></item><item><title>[ASE 2025] GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph.</title><link>https://doi.org/10.1145/3691620.3695054</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiuYZS00JW24</guid></item><item><title>[ASE 2025] Snopy: Bridging Sample Denoising with Causal Graph Learning for Effective Vulnerability Detection.</title><link>https://doi.org/10.1145/3691620.3695057</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Cao000B00L024</guid></item><item><title>[ASE 2025] Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network.</title><link>https://doi.org/10.1145/3691620.3695068</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/CuiWWJQZHWL24</guid></item><item><title>[ASE 2025] AdvSCanner: Generating Adversarial Smart Contracts to Exploit Reentrancy Vulnerabilities Using LLM and Static Analysis.</title><link>https://doi.org/10.1145/3691620.3695482</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuXPLW00W24</guid></item><item><title>[ASE 2025] Compositional Security Analysis of Dynamic Component-based Systems.</title><link>https://doi.org/10.1145/3691620.3695499</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/KhakpourS24</guid></item><item><title>[ASE 2025] HITS: High-coverage LLM-based Unit Test Generation via Method Slicing.</title><link>https://doi.org/10.1145/3691620.3695501</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangL0J24</guid></item><item><title>[ASE 2025] Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?</title><link>https://doi.org/10.1145/3691620.3695539</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoGHWW024</guid></item><item><title>[ASE 2025] STASE: Static Analysis Guided Symbolic Execution for UEFI Vulnerability Signature Generation.</title><link>https://doi.org/10.1145/3691620.3695543</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ShafiuzzamanDSB24</guid></item><item><title>[ASE 2025] Understanding Developer-Analyzer Interactions in Code Reviews.</title><link>https://doi.org/10.1145/3691620.3695257</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/SchafCLMTSZZ24</guid></item><item><title>[ASE 2025] DeployFix: Dynamic Repair of Software Deployment Failures via Constraint Solving.</title><link>https://doi.org/10.1145/3691620.3695268</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiaoG0HY0DXYZ24</guid></item><item><title>[ASE 2025] Experience Report on Applying Program Analysis Techniques for Mainframe Application Understanding.</title><link>https://doi.org/10.1145/3691620.3695270</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/AgarwalNK24</guid></item><item><title>[ASE 2025] Enhancing Compositional Static Analysis with Dynamic Analysis.</title><link>https://doi.org/10.1145/3691620.3695599</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/DistefanoMACSGH24</guid></item><item><title>[ASE 2025] Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation.</title><link>https://doi.org/10.1145/3691620.3695291</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZZLCZ024</guid></item><item><title>[ASE 2025] Oracle-Guided Vulnerability Diversity and Exploit Synthesis of Smart Contracts Using LLMs.</title><link>https://doi.org/10.1145/3691620.3695292</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/EshghieA24</guid></item><item><title>[ASE 2025] ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts.</title><link>https://doi.org/10.1145/3691620.3695349</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZGXG024</guid></item><item><title>[ASE 2025] Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers.</title><link>https://doi.org/10.1145/3691620.3695322</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LuoYZLX24</guid></item><item><title>[ASE 2025] RepoGenix: Dual Context-Aided Repository-Level Code Completion with Language Models.</title><link>https://doi.org/10.1145/3691620.3695331</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiangXZZD0CWF24</guid></item><item><title>[RAID 2024] Large-Scale Security Analysis of Real-World Backend Deployments Speaking IoT-Focused Protocols.</title><link>https://doi.org/10.1145/3678890.3678899</link><description></description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Mon, 30 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/TagliaroKCBL24</guid></item><item><title>[RAID 2024] A Comprehensive, Automated Security Analysis of the Uptane Automotive Over-the-Air Update Framework.</title><link>https://doi.org/10.1145/3678890.3678927</link><description></description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Mon, 30 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/LorchLTC24</guid></item><item><title>[FM 2024] Learning Branching-Time Properties in CTL and ATL via Constraint Solving.</title><link>https://doi.org/10.1007/978-3-031-71162-6_16</link><description></description><author>dblp: new volumes for streams/conf/fm</author><pubDate>Mon, 16 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/fm/BordaisNR24</guid></item></channel></rss>