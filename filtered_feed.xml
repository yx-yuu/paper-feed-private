<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Fri, 06 Feb 2026 18:20:01 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[C&amp;S 2026] SemTaint: A scalable taint analysis approach for JavaWeb frameworks and composite containers</title><link>https://www.sciencedirect.com/science/article/pii/S0167404825005103?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers &amp; Security, Volume 163&lt;/p&gt;&lt;p&gt;Author(s): Haotian Huang, Ruibin Yan, Jian Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Computers &amp; Security</author><pubDate>Fri, 06 Feb 2026 18:20:01 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167404825005103</guid></item><item><title>[SCP 2026] DATVD: A novel vulnerability detection method based on dynamic attention and hybrid convolutional pooling</title><link>https://www.sciencedirect.com/science/article/pii/S0167642326000080?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Science of Computer Programming, Volume 251&lt;/p&gt;&lt;p&gt;Author(s): Jinfu Chen, Jinyu Mu, Saihua Cai, Jiapeng Zhou, Ziyan Liu, Xinping Shi&lt;/p&gt;</description><author>ScienceDirect Publication: Science of Computer Programming</author><pubDate>Fri, 06 Feb 2026 18:19:21 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167642326000080</guid></item><item><title>[JSS 2026] Many hands make light work: An LLM-based multi-agent system for detecting malicious PyPI packages</title><link>https://www.sciencedirect.com/science/article/pii/S0164121226000269?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 236&lt;/p&gt;&lt;p&gt;Author(s): Muhammad Umar Zeshan, Motunrayo Ibiyo, Claudio Di Sipio, Phuong T. Nguyen, Davide Di Ruscio&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Fri, 06 Feb 2026 18:19:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0164121226000269</guid></item><item><title>[IST 2026] VulSEG: Enhanced graph-based vulnerability detection system with advanced text embedding</title><link>https://www.sciencedirect.com/science/article/pii/S0950584925003465?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Wenjing Cai, Xin Liu, Lipeng Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 18:19:19 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584925003465</guid></item><item><title>[IST 2026] CSVD-AES: Cross-project software vulnerability detection based on active learning with metric fusion</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Zhidan Yuan, Xiang Chen, Juan Zhang, Weiming Zeng&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 18:19:19 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000042</guid></item><item><title>[IST 2026] EdgeSim: Firmware vulnerability detection with control transfer-enhanced binary code similarity detection</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000091?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Li Liu, Shen Wang, Xunzhi Jiang&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 18:19:19 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000091</guid></item><item><title>[C&amp;S] Folded-tag: Enhancing memory safety with efficient hardware-supported memory tagging</title><link>https://www.sciencedirect.com/science/article/pii/S0167404825005115?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers &amp; Security, Volume 163&lt;/p&gt;&lt;p&gt;Author(s): Sumin Yang, Hongjoo Jin, Wonsuk Choi, Dong Hoon Lee&lt;/p&gt;</description><author>ScienceDirect Publication: Computers &amp; Security</author><pubDate>Fri, 06 Feb 2026 17:02:38 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167404825005115</guid></item><item><title>[SCP] Securing LLM code generation: Leveraging prompt engineering to mitigate vulnerabilities across models and languages</title><link>https://www.sciencedirect.com/science/article/pii/S0167642326000110?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Science of Computer Programming, Volume 251&lt;/p&gt;&lt;p&gt;Author(s): Shaykhah S. Aldosari, Layla S. Aldawsari&lt;/p&gt;</description><author>ScienceDirect Publication: Science of Computer Programming</author><pubDate>Fri, 06 Feb 2026 17:01:57 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167642326000110</guid></item><item><title>[JSS] RLV: LLM-based vulnerability detection by retrieving and refining contextual information</title><link>https://www.sciencedirect.com/science/article/pii/S016412122500425X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 235&lt;/p&gt;&lt;p&gt;Author(s): Fangcheng Qiu, Zhongxin Liu, Bingde Hu, Zhengong Cai, Lingfeng Bao, Xinyu Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Fri, 06 Feb 2026 17:01:57 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S016412122500425X</guid></item><item><title>[IST] COTVD: A function-level vulnerability detection framework using chain-of-thought reasoning with large language models</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000327?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 193&lt;/p&gt;&lt;p&gt;Author(s): Yinan Chen, Xiangping Chen, Yuan Huang, Changlin Yang, Lei Yun&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Fri, 06 Feb 2026 17:01:56 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000327</guid></item><item><title>[arXiv-CR] Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software</title><link>https://arxiv.org/abs/2602.04894</link><description>arXiv:2602.04894v1 Announce Type: new 
Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04894v1</guid></item><item><title>[arXiv-CR] SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions</title><link>https://arxiv.org/abs/2602.05329</link><description>arXiv:2602.05329v1 Announce Type: new 
Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05329v1</guid></item><item><title>[arXiv-CR] Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows</title><link>https://arxiv.org/abs/2602.05817</link><description>arXiv:2602.05817v1 Announce Type: new 
Abstract: The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05817v1</guid></item><item><title>[arXiv-CR] Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection</title><link>https://arxiv.org/abs/2602.05868</link><description>arXiv:2602.05868v1 Announce Type: new 
Abstract: Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05868v1</guid></item><item><title>[arXiv-CR] A Causal Perspective for Enhancing Jailbreak Attack and Defense</title><link>https://arxiv.org/abs/2602.04893</link><description>arXiv:2602.04893v1 Announce Type: cross 
Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04893v1</guid></item><item><title>[arXiv-CR] TxRay: Agentic Postmortem of Live Blockchain Attacks</title><link>https://arxiv.org/abs/2602.01317</link><description>arXiv:2602.01317v3 Announce Type: replace 
Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.
  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.
  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +22.9pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01317v3</guid></item><item><title>[arXiv-CR] Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.08859</link><description>arXiv:2510.08859v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories through distinct conversational approaches. Existing multi-turn methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles, defense to one pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.08859v2</guid></item><item><title>[arXiv-SE] Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set</title><link>https://arxiv.org/abs/2602.04910</link><description>arXiv:2602.04910v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04910v1</guid></item><item><title>[arXiv-SE] EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering</title><link>https://arxiv.org/abs/2602.05242</link><description>arXiv:2602.05242v1 Announce Type: new 
Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05242v1</guid></item><item><title>[arXiv-SE] PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models</title><link>https://arxiv.org/abs/2602.05270</link><description>arXiv:2602.05270v1 Announce Type: new 
Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05270v1</guid></item><item><title>[arXiv-SE] Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study</title><link>https://arxiv.org/abs/2602.05465</link><description>arXiv:2602.05465v1 Announce Type: new 
Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05465v1</guid></item><item><title>[arXiv-SE] A Dual-Loop Agent Framework for Automated Vulnerability Reproduction</title><link>https://arxiv.org/abs/2602.05721</link><description>arXiv:2602.05721v1 Announce Type: new 
Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \textit{Tactical Loop} for code-level refinement, while the \textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\% and 54.3\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\% and 20.4\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05721v1</guid></item><item><title>[arXiv-SE] Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes</title><link>https://arxiv.org/abs/2602.05780</link><description>arXiv:2602.05780v1 Announce Type: new 
Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05780v1</guid></item><item><title>[arXiv-SE] Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction</title><link>https://arxiv.org/abs/2602.04892</link><description>arXiv:2602.04892v1 Announce Type: cross 
Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04892v1</guid></item><item><title>[arXiv-SE] Multi-View Adaptive Contrastive Learning for Information Retrieval Based Fault Localization</title><link>https://arxiv.org/abs/2409.12519</link><description>arXiv:2409.12519v3 Announce Type: replace 
Abstract: Most studies focused on information retrieval-based techniques for fault localization, which built representations for bug reports and source code files and matched their semantic vectors through similarity measurement. However, such approaches often ignore some useful information that might help improve localization performance, such as 1) the interaction relationship between bug reports and source code files; 2) the similarity relationship between bug reports; and 3) the co-citation relationship between source code files. In this paper, we propose a novel approach named Multi-View Adaptive Contrastive Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the above-mentioned relationships for software fault localization. Specifically, we first generate data augmentations from report-code interaction view, report-report similarity view and code-code co-citation view separately, and adopt graph neural network to aggregate the information of bug reports or source code files from the three views in the embedding process. Moreover, we perform contrastive learning across these views. Our design of contrastive learning task will force the bug report representations to encode information shared by report-report and report-code views,and the source code file representations shared by code-code and report-code views, thereby alleviating the noise from auxiliary information. Finally, to evaluate the performance of our approach, we conduct extensive experiments on five open-source Java projects. The results show that our model can improve over the best baseline up to 28.93%, 25.57% and 20.35% on Accuracy@1, MAP and MRR, respectively.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2409.12519v3</guid></item><item><title>[arXiv-SE] LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models</title><link>https://arxiv.org/abs/2505.14759</link><description>arXiv:2505.14759v4 Announce Type: replace 
Abstract: Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens' importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS' tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.14759v4</guid></item><item><title>[arXiv-SE] Code Clone Detection via an AlphaFold-Inspired Framework</title><link>https://arxiv.org/abs/2507.15226</link><description>arXiv:2507.15226v2 Announce Type: replace 
Abstract: Code clone detection plays a critical role in software maintenance and vulnerability analysis. Substantial methods have been proposed to detect code clones. However, they struggle to extract high-level program semantics directly from a single linear token sequence, leading to unsatisfactory detection performance. A similar single-sequence challenge has been successfully addressed in protein structure prediction by AlphaFold. Motivated by the successful resolution of the shared single-sequence challenge by AlphaFold, as well as the sequential similarities between proteins and code, we leverage AlphaFold for code clone detection. In particular, we propose AlphaCC, which represents code fragments as token sequences and adapts AlphaFold's sequence-to-structure modeling capability to infer code semantics. The pipeline of AlphaCC goes through three steps. First, AlphaCC transforms each input code fragment into a token sequence and, motivated by AlphaFold's use of multiple sequence alignment (MSA), novelly uses a retrieval-augmentation strategy to construct an MSA from lexically similar token sequences. Second, AlphaCC adopts a modified attention-based encoder based on AlphaFold to model dependencies within and across token sequences. Finally, unlike AlphaFold's protein structure prediction task, AlphaCC computes similarity scores between token sequences through a late interaction strategy and performs binary classification to determine code clone pairs. Comprehensive evaluations on three datasets, particularly two semantic clone detection datasets, show that AlphaCC consistently outperforms all baselines, demonstrating strong semantic understanding. AlphaCC further achieves strong performance on instances where tool-dependent methods fail, highlighting its tool-independence. Moreover, AlphaCC maintains competitive efficiency, enabling practical usage in large-scale clone detection tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.15226v2</guid></item><item><title>[arXiv-SE] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs</title><link>https://arxiv.org/abs/2510.00031</link><description>arXiv:2510.00031v2 Announce Type: replace 
Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.00031v2</guid></item><item><title>[arXiv-SE] Fine-tuned LLM-based Code Migration Framework</title><link>https://arxiv.org/abs/2512.13515</link><description>arXiv:2512.13515v3 Announce Type: replace 
Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.13515v3</guid></item><item><title>[arXiv-SE] ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling</title><link>https://arxiv.org/abs/2602.03070</link><description>arXiv:2602.03070v2 Announce Type: replace-cross 
Abstract: Growing renewable penetration introduces substantial uncertainty into power system operations, necessitating frequent adaptation of dispatch objectives and constraints and challenging expertise-intensive, near-real-time modeling workflows. Large Language Models (LLMs) provide a promising avenue for automating this process by translating natural-language (NL) operational requirements into executable optimization models via semantic reasoning and code synthesis. Yet existing LLM datasets and benchmarks for optimization modeling primarily target coarse-grained cross-domain generalization, offering limited, rigorous evaluation in power-system settings, particularly for Optimal Power Flow (OPF). We therefore introduce \textbf{ProOPF-D} and \textbf{ProOPF-B}, a dataset and benchmark for professional-grade OPF modeling: ProOPF-D contains 12K instances pairing NL requests with parameter adjustments and structural extensions to a canonical OPF, together with executable implementations; ProOPF-B provides 121 expert-annotated test cases with ground-truth code, enabling end-to-end evaluation under both concrete and abstract OPF modeling regimes.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03070v2</guid></item><item><title>[arXiv-PL] Opportunistically Parallel Lambda Calculus</title><link>https://arxiv.org/abs/2405.11361</link><description>arXiv:2405.11361v4 Announce Type: replace 
Abstract: Scripting languages are widely used to compose external calls such as native libraries and network services. In such scripts, execution time is often dominated by waiting for these external calls, rendering traditional single-language optimizations ineffective. To address this, we propose a novel opportunistic evaluation strategy for scripting languages based on a core lambda calculus that automatically dispatches independent external calls in parallel and streams their results. We prove that our approach is confluent, ensuring that it preserves the programmer's original intent, and that it eventually executes every external call. We implement this approach in a scripting language called Opal. We demonstrate the versatility and performance of Opal, focusing on programs that invoke heavy external computation through the use of large language models (LLMs) and other APIs. Across five scripts, we compare to several state-of-the-art baselines and show that opportunistic evaluation improves total running time (up to $6.2\times$) and latency (up to $12.7\times$) compared to standard sequential Python, while performing very close (between $1.3\%$ and $18.5\%$ running time overhead) to hand-tuned manually optimized asynchronous Rust. For Tree-of-Thoughts, a prominent LLM reasoning approach, we achieve a $6.2\times$ performance improvement over the authors' own implementation.</description><author>cs.PL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2405.11361v4</guid></item><item><title>[arXiv-AI] Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment</title><link>https://arxiv.org/abs/2602.05110</link><description>arXiv:2602.05110v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05110v1</guid></item><item><title>[arXiv-AI] Aspect-Aware MOOC Recommendation in a Heterogeneous Network</title><link>https://arxiv.org/abs/2602.05297</link><description>arXiv:2602.05297v1 Announce Type: new 
Abstract: MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05297v1</guid></item><item><title>[arXiv-AI] ProAct: Agentic Lookahead in Interactive Environments</title><link>https://arxiv.org/abs/2602.05327</link><description>arXiv:2602.05327v1 Announce Type: new 
Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05327v1</guid></item><item><title>[arXiv-AI] Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning</title><link>https://arxiv.org/abs/2602.05464</link><description>arXiv:2602.05464v1 Announce Type: new 
Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05464v1</guid></item><item><title>[arXiv-AI] Generative Ontology: When Structured Knowledge Learns to Create</title><link>https://arxiv.org/abs/2602.05636</link><description>arXiv:2602.05636v1 Announce Type: new 
Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05636v1</guid></item><item><title>[arXiv-AI] Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions</title><link>https://arxiv.org/abs/2602.05709</link><description>arXiv:2602.05709v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05709v1</guid></item><item><title>[arXiv-AI] Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods</title><link>https://arxiv.org/abs/2602.06000</link><description>arXiv:2602.06000v1 Announce Type: new 
Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06000v1</guid></item><item><title>[arXiv-AI] AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</title><link>https://arxiv.org/abs/2602.06008</link><description>arXiv:2602.06008v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06008v1</guid></item><item><title>[arXiv-AI] DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</title><link>https://arxiv.org/abs/2602.06039</link><description>arXiv:2602.06039v1 Announce Type: new 
Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06039v1</guid></item><item><title>[arXiv-AI] Denoising diffusion networks for normative modeling in neuroimaging</title><link>https://arxiv.org/abs/2602.04886</link><description>arXiv:2602.04886v1 Announce Type: cross 
Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04886v1</guid></item><item><title>[arXiv-AI] Physics as the Inductive Bias for Causal Discovery</title><link>https://arxiv.org/abs/2602.04907</link><description>arXiv:2602.04907v1 Announce Type: cross 
Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04907v1</guid></item><item><title>[arXiv-AI] Internalizing LLM Reasoning via Discovery and Replay of Latent Actions</title><link>https://arxiv.org/abs/2602.04925</link><description>arXiv:2602.04925v1 Announce Type: cross 
Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04925v1</guid></item><item><title>[arXiv-AI] Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05183</link><description>arXiv:2602.05183v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05183v1</guid></item><item><title>[arXiv-AI] LMMRec: LLM-driven Motivation-aware Multimodal Recommendation</title><link>https://arxiv.org/abs/2602.05474</link><description>arXiv:2602.05474v1 Announce Type: cross 
Abstract: Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\% performance improvement.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05474v1</guid></item><item><title>[arXiv-AI] LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation</title><link>https://arxiv.org/abs/2602.05493</link><description>arXiv:2602.05493v1 Announce Type: cross 
Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05493v1</guid></item><item><title>[arXiv-AI] AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval</title><link>https://arxiv.org/abs/2602.05524</link><description>arXiv:2602.05524v1 Announce Type: cross 
Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05524v1</guid></item><item><title>[arXiv-AI] Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views</title><link>https://arxiv.org/abs/2602.05884</link><description>arXiv:2602.05884v1 Announce Type: cross 
Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05884v1</guid></item><item><title>[arXiv-AI] RISE-Video: Can Video Generators Decode Implicit World Rules?</title><link>https://arxiv.org/abs/2602.05986</link><description>arXiv:2602.05986v1 Announce Type: cross 
Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05986v1</guid></item><item><title>[arXiv-AI] CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</title><link>https://arxiv.org/abs/2602.06038</link><description>arXiv:2602.06038v1 Announce Type: cross 
Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06038v1</guid></item><item><title>[arXiv-AI] Robust Answers, Fragile Logic: Probing the Decoupling Hypothesis in LLM Reasoning</title><link>https://arxiv.org/abs/2505.17406</link><description>arXiv:2505.17406v2 Announce Type: replace 
Abstract: While Chain-of-Thought (CoT) prompting has become a cornerstone for complex reasoning in Large Language Models (LLMs), the faithfulness of the generated reasoning remains an open question. We investigate the Decoupling Hypothesis: that correct answers often mask fragile, post-hoc rationalizations that are not causally tied to the model's prediction. To systematically verify this, we introduce MATCHA, a novel Answer-Conditioned Probing framework. Unlike standard evaluations that focus on final output accuracy, MATCHA isolates the reasoning phase by conditioning generation on the model's predicted answer, allowing us to stress-test the stability of the rationale itself. Our experiments reveal a critical vulnerability: under imperceptible input perturbations, LLMs frequently maintain the correct answer while generating inconsistent or nonsensical reasoning - effectively being ``Right for the Wrong Reasons''. Using LLM judges to quantify this robustness gap, we find that multi-step and commonsense tasks are significantly more susceptible to this decoupling than logical tasks. Furthermore, we demonstrate that adversarial examples generated by MATCHA transfer non-trivially to black-box models. Our findings expose the illusion of CoT robustness and underscore the need for future architectures that enforce genuine answer-reasoning consistency rather than mere surface-level accuracy.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17406v2</guid></item><item><title>[arXiv-AI] Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers</title><link>https://arxiv.org/abs/2506.13342</link><description>arXiv:2506.13342v2 Announce Type: replace 
Abstract: Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend that future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.13342v2</guid></item><item><title>[arXiv-AI] DeepAgent: A General Reasoning Agent with Scalable Toolsets</title><link>https://arxiv.org/abs/2510.21618</link><description>arXiv:2510.21618v3 Announce Type: replace 
Abstract: Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To manage long-horizon interactions, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.21618v3</guid></item><item><title>[arXiv-AI] The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution</title><link>https://arxiv.org/abs/2601.15075</link><description>arXiv:2601.15075v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining \textbf{the reason behind agent behaviors}. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems. Codes are available at https://github.com/AI45Lab/AgentDoG.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15075v2</guid></item><item><title>[arXiv-AI] A Study of Adaptive Modeling Towards Robust Generalization</title><link>https://arxiv.org/abs/2602.02780</link><description>arXiv:2602.02780v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly support reasoning over biomolecular structures, but most existing approaches remain modality-specific and rely on either sequence-style encodings or fixed-length connector tokens for structural inputs. These designs can under-expose explicit geometric cues and impose rigid fusion bottlenecks, leading to over-compression and poor token allocation as structural complexity grows. We present a unified all-atom framework that grounds language reasoning in geometric information while adaptively scaling structural tokens. The method first constructs variable-size structural patches on molecular graphs using an instruction-conditioned gating policy, enabling complexity-aware allocation of query tokens. It then refines the resulting patch tokens via cross-attention with modality embeddings and injects geometry-informed tokens into the language model to improve structure grounding and reduce structural hallucinations. Across diverse all-atom benchmarks, the proposed approach yields consistent gains in heterogeneous structure-grounded reasoning. An anonymized implementation is provided in the supplementary material.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02780v2</guid></item><item><title>[arXiv-AI] WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting</title><link>https://arxiv.org/abs/2410.03159</link><description>arXiv:2410.03159v4 Announce Type: replace-cross 
Abstract: We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2410.03159v4</guid></item><item><title>[arXiv-AI] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2503.06749</link><description>arXiv:2503.06749v3 Announce Type: replace-cross 
Abstract: DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.06749v3</guid></item><item><title>[arXiv-AI] Guided Diffusion Sampling on Function Spaces with Applications to PDEs</title><link>https://arxiv.org/abs/2505.17004</link><description>arXiv:2505.17004v3 Announce Type: replace-cross 
Abstract: We propose a general framework for conditional sampling in PDE-based inverse problems, targeting the recovery of whole solutions from extremely sparse or noisy measurements. This is accomplished by a function-space diffusion model and plug-and-play guidance for conditioning. Our method first trains an unconditional, discretization-agnostic denoising model using neural operator architectures. At inference, we refine the samples to satisfy sparse observation data via a gradient-based guidance mechanism. Through rigorous mathematical analysis, we extend Tweedie's formula to infinite-dimensional Banach spaces, providing the theoretical foundation for our posterior sampling approach. Our method (FunDPS) accurately captures posterior distributions in function spaces under minimal supervision and severe data scarcity. Across five PDE tasks with only 3% observation, our method achieves an average 32% accuracy improvement over state-of-the-art fixed-resolution diffusion baselines while reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning ensures strong cross-resolution generalizability. To the best of our knowledge, this is the first diffusion-based framework to operate independently of discretization, offering a practical and flexible solution for forward and inverse problems in the context of PDEs. Code is available at https://github.com/neuraloperator/FunDPS</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17004v3</guid></item><item><title>[arXiv-AI] CellForge: Agentic Design of Virtual Cell Models</title><link>https://arxiv.org/abs/2508.02276</link><description>arXiv:2508.02276v2 Announce Type: replace-cross 
Abstract: Virtual cell modeling aims to predict cellular responses to diverse perturbations but faces challenges from biological complexity, multimodal data heterogeneity, and the need for interdisciplinary expertise. We introduce CellForge, a multi-agent framework that autonomously designs and synthesizes neural network architectures tailored to specific single-cell datasets and perturbation tasks. Given raw multi-omics data and task descriptions, CellForge discovers candidate architectures through collaborative reasoning among specialized agents, then generates executable implementations. Our core contribution is the framework itself: showing that multi-agent collaboration mechanisms - rather than manual human design or single-LLM prompting - can autonomously produce executable, high-quality computational methods. This approach goes beyond conventional hyperparameter tuning by enabling entirely new architectural components such as trajectory-aware encoders and perturbation diffusion modules to emerge from agentic deliberation. We evaluate CellForge on six datasets spanning gene knockouts, drug treatments, and cytokine stimulations across multiple modalities (scRNA-seq, scATAC-seq, CITE-seq). The results demonstrate that the models generated by CellForge are highly competitive with established baselines, while revealing systematic patterns of architectural innovation. CellForge highlights the scientific value of multi-agent frameworks: collaboration among specialized agents enables genuine methodological innovation and executable solutions that single agents or human experts cannot achieve. This represents a paradigm shift toward autonomous scientific method development in computational biology. Code is available at https://github.com/gersteinlab/CellForge.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.02276v2</guid></item><item><title>[arXiv-AI] Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression</title><link>https://arxiv.org/abs/2510.02345</link><description>arXiv:2510.02345v3 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs. Code is available at https://github.com/szdtzpj/Breaking_the_moe_trilemma</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.02345v3</guid></item><item><title>[arXiv-AI] Leveraging Whisper Embeddings for Audio-based Lyrics Matching</title><link>https://arxiv.org/abs/2510.08176</link><description>arXiv:2510.08176v2 Announce Type: replace-cross 
Abstract: Audio-based lyrics matching can be an appealing alternative to other content-based retrieval approaches, but existing methods often suffer from limited reproducibility and inconsistent baselines. In this work, we introduce WEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings for lyrics matching tasks. WEALY establishes robust and transparent baselines, while also exploring multimodal extensions that integrate textual and acoustic features. Through extensive experiments on standard datasets, we demonstrate that WEALY achieves a performance comparable to state-of-the-art methods that lack reproducibility. In addition, we provide ablation studies and analyses on language robustness, loss functions, and embedding strategies. This work contributes a reliable benchmark for future research, and underscores the potential of speech technologies for music information retrieval tasks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.08176v2</guid></item><item><title>[arXiv-AI] TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title><link>https://arxiv.org/abs/2510.25502</link><description>arXiv:2510.25502v4 Announce Type: replace-cross 
Abstract: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.25502v4</guid></item><item><title>[arXiv-AI] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</title><link>https://arxiv.org/abs/2511.11007</link><description>arXiv:2511.11007v2 Announce Type: replace-cross 
Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.11007v2</guid></item><item><title>[arXiv-AI] CARL: Focusing Agentic Reinforcement Learning on Critical Actions</title><link>https://arxiv.org/abs/2512.04949</link><description>arXiv:2512.04949v2 Announce Type: replace-cross 
Abstract: Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for long-horizon agentic reasoning. CARL leverages entropy as a heuristic proxy for action criticality and achieves focused training by assigning rewards to high-criticality actions while excluding low-criticality actions from model updates, avoiding noisy credit assignment and redundant computation. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency across diverse evaluation settings. The source code will be publicly available.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.04949v2</guid></item><item><title>[arXiv-AI] Sparse Attention Post-Training for Mechanistic Interpretability</title><link>https://arxiv.org/abs/2512.05865</link><description>arXiv:2512.05865v2 Announce Type: replace-cross 
Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 7B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.4 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. Additionally, using cross-layer transcoders, we show that sparse attention substantially simplifies attention attribution, enabling a unified view of feature-based and circuit-based perspectives. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.05865v2</guid></item><item><title>[arXiv-AI] Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning</title><link>https://arxiv.org/abs/2601.12535</link><description>arXiv:2601.12535v2 Announce Type: replace-cross 
Abstract: Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving. The code is available on github: https://github.com/Copticoder/thesis-nllb-bootstrap-grpo.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.12535v2</guid></item><item><title>[arXiv-AI] Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models</title><link>https://arxiv.org/abs/2601.13948</link><description>arXiv:2601.13948v3 Announce Type: replace-cross 
Abstract: Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13948v3</guid></item><item><title>[arXiv-AI] Reflexis: Supporting Reflexivity and Rigor in Collaborative Qualitative Analysis through Design for Deliberation</title><link>https://arxiv.org/abs/2601.15445</link><description>arXiv:2601.15445v2 Announce Type: replace-cross 
Abstract: Reflexive Thematic Analysis (RTA) is a critical method for generating deep interpretive insights. Yet its core tenets, including researcher reflexivity, tangible analytical evolution, and productive disagreement, are often poorly supported by software tools that prioritize speed and consensus over interpretive depth. To address this gap, we introduce Reflexis, a collaborative workspace that centers these practices. It supports reflexivity by integrating in-situ reflection prompts, makes code evolution transparent and tangible, and scaffolds collaborative interpretation by turning differences into productive, positionality-aware dialogue. Results from our paired-analyst study (N=12) indicate that Reflexis encouraged participants toward more granular reflection and reframed disagreements as productive conversations. The evaluation also surfaced key design tensions, including a desire for higher-level, networked memos and more user control over the timing of proactive alerts. Reflexis contributes a design framework for tools that prioritize rigor and transparency to support deep, collaborative interpretation in an age of automation.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15445v2</guid></item><item><title>[arXiv-AI] Learning to Discover at Test Time</title><link>https://arxiv.org/abs/2601.16175</link><description>arXiv:2601.16175v2 Announce Type: replace-cross 
Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\H{o}s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16175v2</guid></item><item><title>[arXiv-AI] STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification</title><link>https://arxiv.org/abs/2601.19903</link><description>arXiv:2601.19903v2 Announce Type: replace-cross 
Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19903v2</guid></item><item><title>[arXiv-AI] Impact of LLMs news Sentiment Analysis on Stock Price Movement Prediction</title><link>https://arxiv.org/abs/2602.00086</link><description>arXiv:2602.00086v2 Announce Type: replace-cross 
Abstract: This paper addresses stock price movement prediction by leveraging LLM-based news sentiment analysis. Earlier works have largely focused on proposing and assessing sentiment analysis models and stock movement prediction methods, however, separately. Although promising results have been achieved, a clear and in-depth understanding of the benefit of the news sentiment to this task, as well as a comprehensive assessment of different architecture types in this context, is still lacking. Herein, we conduct an evaluation study that compares 3 different LLMs, namely, DeBERTa, RoBERTa and FinBERT, for sentiment-driven stock prediction. Our results suggest that DeBERTa outperforms the other two models with an accuracy of 75% and that an ensemble model that combines the three models can increase the accuracy to about 80%. Also, we see that sentiment news features can benefit (slightly) some stock market prediction models, i.e., LSTM-, PatchTST- and tPatchGNN-based classifiers and PatchTST- and TimesNet-based regression tasks models.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00086v2</guid></item><item><title>[arXiv-AI] Sounding Highlights: Dual-Pathway Audio Encoders for Audio-Visual Video Highlight Detection</title><link>https://arxiv.org/abs/2602.03891</link><description>arXiv:2602.03891v2 Announce Type: replace-cross 
Abstract: Audio-visual video highlight detection aims to automatically identify the most salient moments in videos by leveraging both visual and auditory cues. However, existing models often underutilize the audio modality, focusing on high-level semantic features while failing to fully leverage the rich, dynamic characteristics of sound. To address this limitation, we propose a novel framework, Dual-Pathway Audio Encoders for Video Highlight Detection (DAViHD). The dual-pathway audio encoder is composed of a semantic pathway for content understanding and a dynamic pathway that captures spectro-temporal dynamics. The semantic pathway extracts high-level information by identifying the content within the audio, such as speech, music, or specific sound events. The dynamic pathway employs a frequency-adaptive mechanism as time evolves to jointly model these dynamics, enabling it to identify transient acoustic events via salient spectral bands and rapid energy changes. We integrate the novel audio encoder into a full audio-visual framework and achieve new state-of-the-art performance on the large-scale MrHiSum benchmark. Our results demonstrate that a sophisticated, dual-faceted audio representation is key to advancing the field of highlight detection.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03891v2</guid></item><item><title>[arXiv-LG] SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines</title><link>https://arxiv.org/abs/2602.05134</link><description>arXiv:2602.05134v1 Announce Type: new 
Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05134v1</guid></item><item><title>[arXiv-LG] Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification</title><link>https://arxiv.org/abs/2602.05535</link><description>arXiv:2602.05535v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05535v1</guid></item><item><title>[arXiv-LG] EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking</title><link>https://arxiv.org/abs/2602.05571</link><description>arXiv:2602.05571v1 Announce Type: new 
Abstract: Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\%, a +3.8 pp improvement over the prior state of the art (74.2\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05571v1</guid></item><item><title>[arXiv-LG] OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph</title><link>https://arxiv.org/abs/2602.05576</link><description>arXiv:2602.05576v1 Announce Type: new 
Abstract: Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05576v1</guid></item><item><title>[arXiv-LG] Empowering Time Series Analysis with Large-Scale Multimodal Pretraining</title><link>https://arxiv.org/abs/2602.05646</link><description>arXiv:2602.05646v1 Announce Type: new 
Abstract: While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05646v1</guid></item><item><title>[arXiv-LG] CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs</title><link>https://arxiv.org/abs/2602.05861</link><description>arXiv:2602.05861v1 Announce Type: new 
Abstract: Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05861v1</guid></item><item><title>[arXiv-LG] Breaking Symmetry Bottlenecks in GNN Readouts</title><link>https://arxiv.org/abs/2602.05950</link><description>arXiv:2602.05950v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05950v1</guid></item><item><title>[arXiv-LG] Layer-wise LoRA fine-tuning: a similarity metric approach</title><link>https://arxiv.org/abs/2602.05988</link><description>arXiv:2602.05988v1 Announce Type: new 
Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05988v1</guid></item><item><title>[arXiv-LG] Beyond Independent Genes: Learning Module-Inductive Representations for Gene Perturbation Prediction</title><link>https://arxiv.org/abs/2602.04901</link><description>arXiv:2602.04901v1 Announce Type: cross 
Abstract: Predicting transcriptional responses to genetic perturbations is a central problem in functional genomics. In practice, perturbation responses are rarely gene-independent but instead manifest as coordinated, program-level transcriptional changes among functionally related genes. However, most existing methods do not explicitly model such coordination, due to gene-wise modeling paradigms and reliance on static biological priors that cannot capture dynamic program reorganization. To address these limitations, we propose scBIG, a module-inductive perturbation prediction framework that explicitly models coordinated gene programs. scBIG induces coherent gene programs from data via Gene-Relation Clustering, captures inter-program interactions through a Gene-Cluster-Aware Encoder, and preserves modular coordination using structure-aware alignment objectives. These structured representations are then modeled using conditional flow matching to enable flexible and generalizable perturbation prediction. Extensive experiments on multiple single-cell perturbation benchmarks show that scBIG consistently outperforms state-of-the-art methods, particularly on unseen and combinatorial perturbation settings, achieving an average improvement of 6.7% over the strongest baselines.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04901v1</guid></item><item><title>[arXiv-LG] Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2602.04926</link><description>arXiv:2602.04926v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04926v1</guid></item><item><title>[arXiv-LG] Scaling Laws for Embedding Dimension in Information Retrieval</title><link>https://arxiv.org/abs/2602.05062</link><description>arXiv:2602.05062v1 Announce Type: cross 
Abstract: Dense retrieval, which encodes queries and documents into a single dense vector, has become the dominant neural retrieval approach due to its simplicity and compatibility with fast approximate nearest neighbor algorithms. As the tasks dense retrieval performs grow in complexity, the fundamental limitations of the underlying data structure and similarity metric -- namely vectors and inner-products -- become more apparent. Prior recent work has shown theoretical limitations inherent to single vectors and inner-products that are generally tied to the embedding dimension. Given the importance of embedding dimension for retrieval capacity, understanding how dense retrieval performance changes as embedding dimension is scaled is fundamental to building next generation retrieval models that balance effectiveness and efficiency. In this work, we conduct a comprehensive analysis of the relationship between embedding dimension and retrieval performance. Our experiments include two model families and a range of model sizes from each to construct a detailed picture of embedding scaling behavior. We find that the scaling behavior fits a power law, allowing us to derive scaling laws for performance given only embedding dimension, as well as a joint law accounting for embedding dimension and model size. Our analysis shows that for evaluation tasks aligned with the training task, performance continues to improve as embedding size increases, though with diminishing returns. For evaluation data that is less aligned with the training task, we find that performance is less predictable, with performance degrading with larger embedding dimensions for certain tasks. We hope our work provides additional insight into the limitations of embeddings and their behavior as well as offers a practical guide for selecting model and embedding dimension to achieve optimal performance with reduced storage and compute costs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05062v1</guid></item><item><title>[arXiv-LG] Price of universality in vector quantization is at most 0.11 bit</title><link>https://arxiv.org/abs/2602.05790</link><description>arXiv:2602.05790v1 Announce Type: cross 
Abstract: Fast computation of a matrix product $W^\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\widehat W$ in place of true $W$ ("weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as "waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive.
  Equivalently, our result shows existence of a net in $\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05790v1</guid></item><item><title>[arXiv-LG] Multi-Token Prediction via Self-Distillation</title><link>https://arxiv.org/abs/2602.06019</link><description>arXiv:2602.06019v1 Announce Type: cross 
Abstract: Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $&lt;5\%$ drop in accuracy relative to single token decoding performance.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06019v1</guid></item><item><title>[arXiv-LG] PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</title><link>https://arxiv.org/abs/2602.06030</link><description>arXiv:2602.06030v1 Announce Type: cross 
Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06030v1</guid></item><item><title>[arXiv-LG] Symplectic convolutional neural networks</title><link>https://arxiv.org/abs/2508.19842</link><description>arXiv:2508.19842v2 Announce Type: replace 
Abstract: We propose a new symplectic convolutional neural network (CNN) architecture by leveraging symplectic neural networks, proper symplectic decomposition, and tensor techniques. Specifically, we first introduce a mathematically equivalent form of the convolution layer and then, using symplectic neural networks, we demonstrate a way to parameterize the layers of the CNN to ensure that the convolution layer remains symplectic. To construct a complete autoencoder, we introduce a symplectic pooling layer. We demonstrate the performance of the proposed neural network on three examples: the wave equation, the nonlinear Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical results indicate that the symplectic CNN outperforms the linear symplectic autoencoder obtained via proper symplectic decomposition.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.19842v2</guid></item><item><title>[arXiv-LG] Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning</title><link>https://arxiv.org/abs/2510.00761</link><description>arXiv:2510.00761v4 Announce Type: replace 
Abstract: Large language model (LLM) unlearning aims to surgically remove the influence of undesired data or knowledge from an existing model while preserving its utility on unrelated tasks. This paradigm has shown promise in addressing privacy and safety concerns. However, recent findings reveal that unlearning effects are often fragile: post-unlearning manipulations such as weight quantization or fine-tuning can quickly neutralize the intended forgetting. Prior efforts to improve robustness primarily reformulate unlearning objectives by explicitly assuming the role of vulnerability sources. In this work, we take a different perspective by investigating the role of the optimizer, independent of unlearning objectives and formulations, in shaping unlearning robustness. We show that the 'grade' of the optimizer, defined by the level of information it exploits, ranging from zeroth-order (gradient-free) to first-order (gradient-based) to second-order (Hessian-based), is tightly linked to the resilience of unlearning. Surprisingly, we find that downgrading the optimizer, such as using zeroth-order methods or compressed-gradient variants (e.g., gradient sign-based optimizers), often leads to stronger robustness. While these optimizers produce noisier and less precise updates, they encourage convergence to harder-to-disturb basins in the loss landscape, thereby resisting post-training perturbations. By connecting zeroth-order methods with randomized smoothing, we further highlight their natural advantage for robust unlearning. Motivated by these insights, we propose a hybrid optimizer that combines first-order and zeroth-order updates, preserving unlearning efficacy while enhancing robustness. Extensive experiments on the MUSE and WMDP benchmarks, across multiple LLM unlearning algorithms, validate that our approach achieves more resilient forgetting without sacrificing unlearning quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.00761v4</guid></item><item><title>[arXiv-LG] Progressive multi-fidelity learning with neural networks for physical system predictions</title><link>https://arxiv.org/abs/2510.13762</link><description>arXiv:2510.13762v2 Announce Type: replace 
Abstract: Highly accurate datasets from numerical or physical experiments are often expensive and time-consuming to acquire, posing a significant challenge for applications that require precise evaluations, potentially across multiple scenarios and in real-time. Even building sufficiently accurate surrogate models can be extremely challenging with limited high-fidelity data. Conversely, less expensive, low-fidelity data can be computed more easily and encompass a broader range of scenarios. By leveraging multi-fidelity information, prediction capabilities of surrogates can be improved. However, in practical situations, data may be different in types, come from sources of different modalities, and not be concurrently available, further complicating the modeling process. To address these challenges, we introduce a progressive multi-fidelity surrogate model. This model can sequentially incorporate diverse data types using tailored encoders. Multi-fidelity regression from the encoded inputs to the target quantities of interest is then performed using neural networks. Input information progressively flows from lower to higher fidelity levels through two sets of connections: concatenations among all the encoded inputs, and additive connections among the final outputs. This dual connection system enables the model to exploit correlations among different datasets while ensuring that each level makes an additive correction to the previous level without altering it. This approach prevents performance degradation as new input data are integrated into the model and automatically adapts predictions based on the available inputs. We demonstrate the effectiveness of the approach on numerical benchmarks and a real-world case study, showing that it reliably integrates multi-modal data and provides accurate predictions, maintaining performance when generalizing across time and parameter variations.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.13762v2</guid></item><item><title>[arXiv-LG] The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces</title><link>https://arxiv.org/abs/2512.13821</link><description>arXiv:2512.13821v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability - adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a theoretically grounded approach to AI control for code generation tasks, though practical deployment requires addressing the high false positive rates observed in initial evaluations.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.13821v2</guid></item><item><title>[arXiv-CL] FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters</title><link>https://arxiv.org/abs/2602.05235</link><description>arXiv:2602.05235v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05235v1</guid></item><item><title>[arXiv-CL] IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models</title><link>https://arxiv.org/abs/2602.05385</link><description>arXiv:2602.05385v1 Announce Type: new 
Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05385v1</guid></item><item><title>[arXiv-CL] Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs</title><link>https://arxiv.org/abs/2602.05444</link><description>arXiv:2602.05444v1 Announce Type: new 
Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05444v1</guid></item><item><title>[arXiv-CL] Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models</title><link>https://arxiv.org/abs/2602.05897</link><description>arXiv:2602.05897v1 Announce Type: new 
Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05897v1</guid></item><item><title>[arXiv-CL] Invisible Walls in Cities: Designing LLM Agent to Predict Urban Segregation Experience with Social Media Content</title><link>https://arxiv.org/abs/2503.04773</link><description>arXiv:2503.04773v4 Announce Type: replace 
Abstract: Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose a novel Large Language Model (LLM) agent to automate online review mining for segregation prediction. Specifically, we propose a reflective LLM coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction.
  Experiments on real-world data demonstrate that our agent substantially improves prediction accuracy, with a 22.79% elevation in R$^{2}$ and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving places of interest (POIs)' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with Web technology.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.04773v4</guid></item><item><title>[arXiv-CL] Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning</title><link>https://arxiv.org/abs/2503.16252</link><description>arXiv:2503.16252v4 Announce Type: replace 
Abstract: In recent years, general-purpose large language models (LLMs) such as GPT, Gemini, Claude, and DeepSeek have advanced at an unprecedented pace. Despite these achievements, their application to finance remains challenging, due to fragmented data sources, intransparent reasoning processes, and weak transferability to business applications. In response, we introduce Fin-R1, a reasoning LLM designed for financial scenarios. With a compact size of 7 billion parameters, Fin-R1 reduces deployment costs while addressing the aforementioned challenges. Its development follows a two-stage pipeline. First, we construct Fin-R1-Data, a high-quality financial dataset consisting of 60,091 chain-of-thought (CoT) samples, distilled and filtered from multiple authoritative benchmarks to ensure consistency and reliability. Second, we train Fin-R1 using Fin-R1-Data through supervised fine-tuning (SFT), followed by reinforcement learning (RL). This stage substantially improves the model's ability to solve complex financial reasoning tasks, yielding outputs that are both accurate and interpretable. Despite its relatively small parameter scale, Fin-R1 achieves competitive empirical performance across established financial benchmarks and demonstrates practical utility in compliance checking and robo-advisory. Our code is publicly available at https://github.com/SUFE-AIFLM-Lab/Fin-R1, and has already attracted over 700 stars.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.16252v4</guid></item><item><title>[arXiv-CL] DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models</title><link>https://arxiv.org/abs/2602.00377</link><description>arXiv:2602.00377v2 Announce Type: replace 
Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: (i) cross-sequence coupling that concentrates probability mass on high-frequency prefixes, (ii) competitive decoding effects that suppress long-tail concepts, and (iii) scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse - divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 19.6-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models intended for deployment.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00377v2</guid></item><item><title>[arXiv-IR] GLASS: A Generative Recommender for Long-sequence Modeling via SID-Tier and Semantic Search</title><link>https://arxiv.org/abs/2602.05663</link><description>arXiv:2602.05663v1 Announce Type: new 
Abstract: Leveraging long-term user behavioral patterns is a key trajectory for enhancing the accuracy of modern recommender systems. While generative recommender systems have emerged as a transformative paradigm, they face hurdles in effectively modeling extensive historical sequences. To address this challenge, we propose GLASS, a novel framework that integrates long-term user interests into the generative process via SID-Tier and Semantic Search. We first introduce SID-Tier, a module that maps long-term interactions into a unified interest vector to enhance the prediction of the initial SID token. Unlike traditional retrieval models that struggle with massive item spaces, SID-Tier leverages the compact nature of the semantic codebook to incorporate cross features between the user's long-term history and candidate semantic codes. Furthermore, we present semantic hard search, which utilizes generated coarse-grained semantic ID as dynamic keys to extract relevant historical behaviors, which are then fused via an adaptive gated fusion module to recalibrate the trajectory of subsequent fine-grained tokens. To address the inherent data sparsity in semantic hard search, we propose two strategies: semantic neighbor augmentation and codebook resizing. Extensive experiments on two large-scale real-world datasets, TAOBAO-MM and KuaiRec, demonstrate that GLASS outperforms state-of-the-art baselines, achieving significant gains in recommendation quality. Our codes are made publicly available to facilitate further research in generative recommendation.</description><author>cs.IR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05663v1</guid></item><item><title>[arXiv-CR 2026] PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM</title><link>https://arxiv.org/abs/2602.04927</link><description>arXiv:2602.04927v1 Announce Type: new 
Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04927v1</guid></item><item><title>[arXiv-CR 2026] Attack Selection Reduces Safety in Concentrated AI Control Settings against Trusted Monitoring</title><link>https://arxiv.org/abs/2602.04930</link><description>arXiv:2602.04930v1 Announce Type: new 
Abstract: Future AI deployments will likely be monitored for malicious behaviour. The ability of these AIs to subvert monitors by adversarially selecting against them - attack selection - is particularly concerning. To study this, we let a red team create attack policies that attempt to insert attacks into code without being caught by a monitor in the concentrated BigCodeBench backdooring setting. We decompose attack selection into two problems: mapping attacks to a quality score and mapping quality scores to submission probabilities. We frame attack selection as a classification problem and show that safety is significantly more sensitive to FPR than TPR. We find that prompting the attacker model to reason about the monitor while being cautious with attack selection reduces safety from a baseline of 99% to 59% at 0.5% auditing budget, emphasizing that eliciting attack selection capabilities of models is vital to avoid overly optimistic safety scores in control evaluations.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04930v1</guid></item><item><title>[arXiv-CR 2026] Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?</title><link>https://arxiv.org/abs/2602.05023</link><description>arXiv:2602.05023v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05023v1</guid></item><item><title>[arXiv-CR 2026] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening</title><link>https://arxiv.org/abs/2602.05386</link><description>arXiv:2602.05386v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05386v1</guid></item><item><title>[arXiv-CR 2026] BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models</title><link>https://arxiv.org/abs/2602.05401</link><description>arXiv:2602.05401v1 Announce Type: new 
Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05401v1</guid></item><item><title>[arXiv-CR 2026] CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions</title><link>https://arxiv.org/abs/2602.04991</link><description>arXiv:2602.04991v1 Announce Type: cross 
Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04991v1</guid></item><item><title>[arXiv-CR 2026] Hallucination-Resistant Security Planning with a Large Language Model</title><link>https://arxiv.org/abs/2602.05279</link><description>arXiv:2602.05279v1 Announce Type: cross 
Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05279v1</guid></item><item><title>[arXiv-CR 2026] Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration</title><link>https://arxiv.org/abs/2602.05759</link><description>arXiv:2602.05759v1 Announce Type: cross 
Abstract: The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05759v1</guid></item><item><title>[arXiv-CR 2026] From Sands to Mansions: Towards Automated Cyberattack Emulation with Classical Planning and Large Language Models</title><link>https://arxiv.org/abs/2407.16928</link><description>arXiv:2407.16928v4 Announce Type: replace 
Abstract: Evolving attacker capabilities demand realistic and continuously updated cyberattack emulation for threat-informed defense and security benchmarking. Towards automated attack emulation, this paper defines modular attack actions and a linking model to organize and chain heterogeneous attack tools into causality-preserving cyberattacks. Building on this foundation, we introduce Aurora: an automated cyberattack emulation system powered by symbolic planning and large language models (LLMs). Aurora crafts actionable, causality-preserving attack chains tailored to Cyber Threat Intelligence (CTI) reports and target environments, and automatically executes these emulations. Using Aurora, we generated an extensive cyberattack emulation dataset from 250 attack reports, 15 times larger than the leading expert-crafted dataset. Our evaluation shows that Aurora significantly outperforms existing methods in creating actionable, diverse, and realistic attack chains. We release the dataset and use it to evaluate three state-of-the-art intrusion detection systems, whose performance differed notably from results on older datasets, highlighting the need for up-to-date, automated attack emulation.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2407.16928v4</guid></item><item><title>[arXiv-CR 2026] Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates</title><link>https://arxiv.org/abs/2602.04653</link><description>arXiv:2602.04653v2 Announce Type: replace 
Abstract: Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04653v2</guid></item><item><title>[arXiv-CR 2026] How Catastrophic is Your LLM? Certifying Risk in Conversation</title><link>https://arxiv.org/abs/2510.03969</link><description>arXiv:2510.03969v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose C$^3$LLM, a novel, principled statistical Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions--random node, graph path, and adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.03969v3</guid></item><item><title>[arXiv-SE 2026] ASA: Activation Steering for Tool-Calling Domain Adaptation</title><link>https://arxiv.org/abs/2602.04935</link><description>arXiv:2602.04935v1 Announce Type: new 
Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04935v1</guid></item><item><title>[arXiv-SE 2026] Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations</title><link>https://arxiv.org/abs/2602.05523</link><description>arXiv:2602.05523v1 Announce Type: new 
Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05523v1</guid></item><item><title>[arXiv-SE 2026] ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval</title><link>https://arxiv.org/abs/2602.05550</link><description>arXiv:2602.05550v1 Announce Type: new 
Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05550v1</guid></item><item><title>[arXiv-SE 2026] SEAL: Symbolic Execution with Separation Logic (Competition Contribution)</title><link>https://arxiv.org/abs/2602.05703</link><description>arXiv:2602.05703v1 Announce Type: new 
Abstract: SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05703v1</guid></item><item><title>[arXiv-SE 2026] When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models</title><link>https://arxiv.org/abs/2602.05891</link><description>arXiv:2602.05891v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05891v1</guid></item><item><title>[arXiv-SE 2026] GAMMS: Graph based Adversarial Multiagent Modeling Simulator</title><link>https://arxiv.org/abs/2602.05105</link><description>arXiv:2602.05105v1 Announce Type: cross 
Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05105v1</guid></item><item><title>[arXiv-SE 2026] RocqSmith: Can Automatic Optimization Forge Better Proof Agents?</title><link>https://arxiv.org/abs/2602.05762</link><description>arXiv:2602.05762v1 Announce Type: cross 
Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05762v1</guid></item><item><title>[arXiv-PL 2026] Strong Normalisation for Asynchronous Effects</title><link>https://arxiv.org/abs/2602.05528</link><description>arXiv:2602.05528v1 Announce Type: new 
Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.</description><author>cs.PL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05528v1</guid></item><item><title>[arXiv-AI 2026] DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search</title><link>https://arxiv.org/abs/2602.05014</link><description>arXiv:2602.05014v1 Announce Type: new 
Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05014v1</guid></item><item><title>[arXiv-AI 2026] MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation</title><link>https://arxiv.org/abs/2602.05048</link><description>arXiv:2602.05048v1 Announce Type: new 
Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05048v1</guid></item><item><title>[arXiv-AI 2026] Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education</title><link>https://arxiv.org/abs/2602.05059</link><description>arXiv:2602.05059v1 Announce Type: new 
Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05059v1</guid></item><item><title>[arXiv-AI 2026] Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents</title><link>https://arxiv.org/abs/2602.05073</link><description>arXiv:2602.05073v1 Announce Type: new 
Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05073v1</guid></item><item><title>[arXiv-AI 2026] SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers</title><link>https://arxiv.org/abs/2602.05115</link><description>arXiv:2602.05115v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05115v1</guid></item><item><title>[arXiv-AI 2026] HugRAG: Hierarchical Causal Knowledge Graph Design for RAG</title><link>https://arxiv.org/abs/2602.05143</link><description>arXiv:2602.05143v1 Announce Type: new 
Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05143v1</guid></item><item><title>[arXiv-AI 2026] PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences</title><link>https://arxiv.org/abs/2602.05302</link><description>arXiv:2602.05302v1 Announce Type: new 
Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05302v1</guid></item><item><title>[arXiv-AI 2026] RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs</title><link>https://arxiv.org/abs/2602.05367</link><description>arXiv:2602.05367v1 Announce Type: new 
Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05367v1</guid></item><item><title>[arXiv-AI 2026] Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation</title><link>https://arxiv.org/abs/2602.05381</link><description>arXiv:2602.05381v1 Announce Type: new 
Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05381v1</guid></item><item><title>[arXiv-AI 2026] Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation</title><link>https://arxiv.org/abs/2602.05403</link><description>arXiv:2602.05403v1 Announce Type: new 
Abstract: Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05403v1</guid></item><item><title>[arXiv-AI 2026] H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration</title><link>https://arxiv.org/abs/2602.05407</link><description>arXiv:2602.05407v1 Announce Type: new 
Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05407v1</guid></item><item><title>[arXiv-AI 2026] THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs</title><link>https://arxiv.org/abs/2602.05424</link><description>arXiv:2602.05424v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05424v1</guid></item><item><title>[arXiv-AI 2026] M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining</title><link>https://arxiv.org/abs/2602.05429</link><description>arXiv:2602.05429v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05429v1</guid></item><item><title>[arXiv-AI 2026] ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</title><link>https://arxiv.org/abs/2602.05472</link><description>arXiv:2602.05472v1 Announce Type: new 
Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05472v1</guid></item><item><title>[arXiv-AI 2026] A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma</title><link>https://arxiv.org/abs/2602.05515</link><description>arXiv:2602.05515v1 Announce Type: new 
Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05515v1</guid></item><item><title>[arXiv-AI 2026] Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities</title><link>https://arxiv.org/abs/2602.05532</link><description>arXiv:2602.05532v1 Announce Type: new 
Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05532v1</guid></item><item><title>[arXiv-AI 2026] Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach</title><link>https://arxiv.org/abs/2602.05533</link><description>arXiv:2602.05533v1 Announce Type: new 
Abstract: We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05533v1</guid></item><item><title>[arXiv-AI 2026] Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation</title><link>https://arxiv.org/abs/2602.05544</link><description>arXiv:2602.05544v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\% in Sports and 4.59\% in Toys, along with ROUGE-L by 8.02\% and 3.49\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\% in cold-start and 11.9\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\% in Beauty and 23.16\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05544v1</guid></item><item><title>[arXiv-AI 2026] Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents</title><link>https://arxiv.org/abs/2602.05597</link><description>arXiv:2602.05597v1 Announce Type: new 
Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05597v1</guid></item><item><title>[arXiv-AI 2026] BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages</title><link>https://arxiv.org/abs/2602.05599</link><description>arXiv:2602.05599v1 Announce Type: new 
Abstract: Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05599v1</guid></item><item><title>[arXiv-AI 2026] Reactive Knowledge Representation and Asynchronous Reasoning</title><link>https://arxiv.org/abs/2602.05625</link><description>arXiv:2602.05625v1 Announce Type: new 
Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05625v1</guid></item><item><title>[arXiv-AI 2026] Graph-based Agent Memory: Taxonomy, Techniques, and Applications</title><link>https://arxiv.org/abs/2602.05665</link><description>arXiv:2602.05665v1 Announce Type: new 
Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05665v1</guid></item><item><title>[arXiv-AI 2026] NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking</title><link>https://arxiv.org/abs/2602.05805</link><description>arXiv:2602.05805v1 Announce Type: new 
Abstract: Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via "Effective-vs-Redundant" neuron transfer.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05805v1</guid></item><item><title>[arXiv-AI 2026] STProtein: predicting spatial protein expression from multi-omics data</title><link>https://arxiv.org/abs/2602.05811</link><description>arXiv:2602.05811v1 Announce Type: new 
Abstract: The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological "Dark Matter".</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05811v1</guid></item><item><title>[arXiv-AI 2026] TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05818</link><description>arXiv:2602.05818v1 Announce Type: new 
Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05818v1</guid></item><item><title>[arXiv-AI 2026] Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy</title><link>https://arxiv.org/abs/2602.05877</link><description>arXiv:2602.05877v1 Announce Type: new 
Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05877v1</guid></item><item><title>[arXiv-AI 2026] A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges</title><link>https://arxiv.org/abs/2602.05883</link><description>arXiv:2602.05883v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling &amp; Simulation (M&amp;amp;S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&amp;amp;S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&amp;amp;S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&amp;amp;S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05883v1</guid></item><item><title>[arXiv-AI 2026] Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability</title><link>https://arxiv.org/abs/2602.04902</link><description>arXiv:2602.04902v1 Announce Type: cross 
Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + \gamma p_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $\gamma^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04902v1</guid></item><item><title>[arXiv-AI 2026] Temporal Pair Consistency for Variance-Reduced Flow Matching</title><link>https://arxiv.org/abs/2602.04908</link><description>arXiv:2602.04908v1 Announce Type: cross 
Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04908v1</guid></item><item><title>[arXiv-AI 2026] SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel</title><link>https://arxiv.org/abs/2602.04915</link><description>arXiv:2602.04915v1 Announce Type: cross 
Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04915v1</guid></item><item><title>[arXiv-AI 2026] Smart Diagnosis and Early Intervention in PCOS: A Deep Learning Approach to Women's Reproductive Health</title><link>https://arxiv.org/abs/2602.04944</link><description>arXiv:2602.04944v1 Announce Type: cross 
Abstract: Polycystic Ovary Syndrome (PCOS) is a widespread disorder in women of reproductive age, characterized by a hormonal imbalance, irregular periods, and multiple ovarian cysts. Infertility, metabolic syndrome, and cardiovascular risks are long-term complications that make early detection essential. In this paper, we design a powerful framework based on transfer learning utilizing DenseNet201 and ResNet50 for classifying ovarian ultrasound images. The model was trained on an online dataset containing 3856 ultrasound images of cyst-infected and non-infected patients. Each ultrasound frame was resized to 224x224 pixels and encoded with precise pathological indicators. The MixUp and CutMix augmentation strategies were used to improve generalization, yielding a peak validation accuracy of 99.80% by Densenet201 and a validation loss of 0.617 with alpha values of 0.25 and 0.4, respectively. We evaluated the model's interpretability using leading Explainable AI (XAI) approaches such as SHAP, Grad-CAM, and LIME, reasoning with and presenting explicit visual reasons for the model's behaviors, therefore increasing the model's transparency. This study proposes an automated system for medical picture diagnosis that may be used effectively and confidently in clinical practice.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04944v1</guid></item><item><title>[arXiv-AI 2026] AI-Based Detection of In-Treatment Changes from Prostate MR-Linac Images</title><link>https://arxiv.org/abs/2602.04983</link><description>arXiv:2602.04983v1 Announce Type: cross 
Abstract: Purpose: To investigate whether routinely acquired longitudinal MR-Linac images can be leveraged to characterize treatment-induced changes during radiotherapy, particularly subtle inter-fraction changes over short intervals (average of 2 days). Materials and Methods: This retrospective study included a series of 0.35T MR-Linac images from 761 patients. An artificial intelligence (deep learning) model was used to characterize treatment-induced changes by predicting the temporal order of paired images. The model was first trained with the images from the first and the last fractions (F1-FL), then with all pairs (All-pairs). Model performance was assessed using quantitative metrics (accuracy and AUC), compared to a radiologist's performance, and qualitative analyses - the saliency map evaluation to investigate affected anatomical regions. Input ablation experiments were performed to identify the anatomical regions altered by radiotherapy. The radiologist conducted an additional task on partial images reconstructed by saliency map regions, reporting observations as well. Quantitative image analysis was conducted to investigate the results from the model and the radiologist. Results: The F1-FL model yielded near-perfect performance (AUC of 0.99), significantly outperforming the radiologist. The All-pairs model yielded an AUC of 0.97. This performance reflects therapy-induced changes, supported by the performance correlation to fraction intervals, ablation tests and expert's interpretation. Primary regions driving the predictions were prostate, bladder, and pubic symphysis. Conclusion: The model accurately predicts temporal order of MR-Linac fractions and detects radiation-induced changes over one or a few days, including prostate and adjacent organ alterations confirmed by experts. This underscores MR-Linac's potential for advanced image analysis beyond image guidance.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04983v1</guid></item><item><title>[arXiv-AI 2026] CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System</title><link>https://arxiv.org/abs/2602.05004</link><description>arXiv:2602.05004v1 Announce Type: cross 
Abstract: Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05004v1</guid></item><item><title>[arXiv-AI 2026] From Fragmentation to Integration: Exploring the Design Space of AI Agents for Human-as-the-Unit Privacy Management</title><link>https://arxiv.org/abs/2602.05016</link><description>arXiv:2602.05016v1 Announce Type: cross 
Abstract: Managing one's digital footprint is overwhelming, as it spans multiple platforms and involves countless context-dependent decisions. Recent advances in agentic AI offer ways forward by enabling holistic, contextual privacy-enhancing solutions. Building on this potential, we adopted a ''human-as-the-unit'' perspective and investigated users' cross-context privacy challenges through 12 semi-structured interviews. Results reveal that people rely on ad hoc manual strategies while lacking comprehensive privacy controls, highlighting nine privacy-management challenges across applications, temporal contexts, and relationships. To explore solutions, we generated nine AI agent concepts and evaluated them via a speed-dating survey with 116 US participants. The three highest-ranked concepts were all post-sharing management tools with half or full agent autonomy, with users expressing greater trust in AI accuracy than in their own efforts. Our findings highlight a promising design space where users see AI agents bridging the fragments in privacy management, particularly through automated, comprehensive post-sharing remediation of users' digital footprints.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05016v1</guid></item><item><title>[arXiv-AI 2026] Laws of Learning Dynamics and the Core of Learners</title><link>https://arxiv.org/abs/2602.05026</link><description>arXiv:2602.05026v1 Announce Type: cross 
Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05026v1</guid></item><item><title>[arXiv-AI 2026] E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching</title><link>https://arxiv.org/abs/2602.05068</link><description>arXiv:2602.05068v1 Announce Type: cross 
Abstract: Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $\epsilon-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05068v1</guid></item><item><title>[arXiv-AI 2026] CoSA: Compressed Sensing-Based Adaptation of Large Language Models</title><link>https://arxiv.org/abs/2602.05148</link><description>arXiv:2602.05148v1 Announce Type: cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05148v1</guid></item><item><title>[arXiv-AI 2026] Total Variation Rates for Riemannian Flow Matching</title><link>https://arxiv.org/abs/2602.05174</link><description>arXiv:2602.05174v1 Announce Type: cross 
Abstract: Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\mathrm{TV}\le C_{\mathrm{Lip}}\,h + C_{\varepsilon}\,\varepsilon$ (with an additional higher-order $\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\varepsilon$ is the target accuracy. Instantiations yield \emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05174v1</guid></item><item><title>[arXiv-AI 2026] Balanced Anomaly-guided Ego-graph Diffusion Model for Inductive Graph Anomaly Detection</title><link>https://arxiv.org/abs/2602.05232</link><description>arXiv:2602.05232v1 Announce Type: cross 
Abstract: Graph anomaly detection (GAD) is crucial in applications like fraud detection and cybersecurity. Despite recent advancements using graph neural networks (GNNs), two major challenges persist. At the model level, most methods adopt a transductive learning paradigm, which assumes static graph structures, making them unsuitable for dynamic, evolving networks. At the data level, the extreme class imbalance, where anomalous nodes are rare, leads to biased models that fail to generalize to unseen anomalies. These challenges are interdependent: static transductive frameworks limit effective data augmentation, while imbalance exacerbates model distortion in inductive learning settings. To address these challenges, we propose a novel data-centric framework that integrates dynamic graph modeling with balanced anomaly synthesis. Our framework features: (1) a discrete ego-graph diffusion model, which captures the local topology of anomalies to generate ego-graphs aligned with anomalous structural distribution, and (2) a curriculum anomaly augmentation mechanism, which dynamically adjusts synthetic data generation during training, focusing on underrepresented anomaly patterns to improve detection and generalization. Experiments on five datasets demonstrate that the effectiveness of our framework.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05232v1</guid></item><item><title>[arXiv-AI 2026] CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs</title><link>https://arxiv.org/abs/2602.05258</link><description>arXiv:2602.05258v1 Announce Type: cross 
Abstract: Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05258v1</guid></item><item><title>[arXiv-AI 2026] Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science</title><link>https://arxiv.org/abs/2602.05289</link><description>arXiv:2602.05289v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05289v1</guid></item><item><title>[arXiv-AI 2026] Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances</title><link>https://arxiv.org/abs/2602.05392</link><description>arXiv:2602.05392v1 Announce Type: cross 
Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05392v1</guid></item><item><title>[arXiv-AI 2026] Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale</title><link>https://arxiv.org/abs/2602.05447</link><description>arXiv:2602.05447v1 Announce Type: cross 
Abstract: Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.
  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p&lt;0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.
  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05447v1</guid></item><item><title>[arXiv-AI 2026] Towards Segmenting the Invisible: An End-to-End Registration and Segmentation Framework for Weakly Supervised Tumour Analysis</title><link>https://arxiv.org/abs/2602.05453</link><description>arXiv:2602.05453v1 Announce Type: cross 
Abstract: Liver tumour ablation presents a significant clinical challenge: whilst tumours are clearly visible on pre-operative MRI, they are often effectively invisible on intra-operative CT due to minimal contrast between pathological and healthy tissue. This work investigates the feasibility of cross-modality weak supervision for scenarios where pathology is visible in one modality (MRI) but absent in another (CT). We present a hybrid registration-segmentation framework that combines MSCGUNet for inter-modal image registration with a UNet-based segmentation module, enabling registration-assisted pseudo-label generation for CT images. Our evaluation on the CHAOS dataset demonstrates that the pipeline can successfully register and segment healthy liver anatomy, achieving a Dice score of 0.72. However, when applied to clinical data containing tumours, performance degrades substantially (Dice score of 0.16), revealing the fundamental limitations of current registration methods when the target pathology lacks corresponding visual features in the target modality. We analyse the "domain gap" and "feature absence" problems, demonstrating that whilst spatial propagation of labels via registration is feasible for visible structures, segmenting truly invisible pathology remains an open challenge. Our findings highlight that registration-based label transfer cannot compensate for the absence of discriminative features in the target modality, providing important insights for future research in cross-modality medical image analysis. Code an weights are available at: https://github.com/BudhaTronix/Weakly-Supervised-Tumour-Detection</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05453v1</guid></item><item><title>[arXiv-AI 2026] XEmoGPT: An Explainable Multimodal Emotion Recognition Framework with Cue-Level Perception and Reasoning</title><link>https://arxiv.org/abs/2602.05496</link><description>arXiv:2602.05496v1 Announce Type: cross 
Abstract: Explainable Multimodal Emotion Recognition plays a crucial role in applications such as human-computer interaction and social media analytics. However, current approaches struggle with cue-level perception and reasoning due to two main challenges: 1) general-purpose modality encoders are pretrained to capture global structures and general semantics rather than fine-grained emotional cues, resulting in limited sensitivity to emotional signals; and 2) available datasets usually involve a trade-off between annotation quality and scale, which leads to insufficient supervision for emotional cues and ultimately limits cue-level reasoning. Moreover, existing evaluation metrics are inadequate for assessing cue-level reasoning performance. To address these challenges, we propose eXplainable Emotion GPT (XEmoGPT), a novel EMER framework capable of both perceiving and reasoning over emotional cues. It incorporates two specialized modules: the Video Emotional Cue Bridge (VECB) and the Audio Emotional Cue Bridge (AECB), which enhance the video and audio encoders through carefully designed tasks for fine-grained emotional cue perception. To further support cue-level reasoning, we construct a large-scale dataset, EmoCue, designed to teach XEmoGPT how to reason over multimodal emotional cues. In addition, we introduce EmoCue-360, an automated metric that extracts and matches emotional cues using semantic similarity, and release EmoCue-Eval, a benchmark of 400 expert-annotated samples covering diverse emotional scenarios. Experimental results show that XEmoGPT achieves strong performance in both emotional cue perception and reasoning.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05496v1</guid></item><item><title>[arXiv-AI 2026] Steering Large Reasoning Models towards Concise Reasoning via Flow Matching</title><link>https://arxiv.org/abs/2602.05539</link><description>arXiv:2602.05539v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05539v1</guid></item><item><title>[arXiv-AI 2026] Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances</title><link>https://arxiv.org/abs/2602.05650</link><description>arXiv:2602.05650v1 Announce Type: cross 
Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05650v1</guid></item><item><title>[arXiv-AI 2026] Learning to Inject: Automated Prompt Injection via Reinforcement Learning</title><link>https://arxiv.org/abs/2602.05746</link><description>arXiv:2602.05746v1 Announce Type: cross 
Abstract: Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05746v1</guid></item><item><title>[arXiv-AI 2026] DARWIN: Dynamic Agentically Rewriting Self-Improving Network</title><link>https://arxiv.org/abs/2602.05848</link><description>arXiv:2602.05848v1 Announce Type: cross 
Abstract: DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05848v1</guid></item><item><title>[arXiv-AI 2026] DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title><link>https://arxiv.org/abs/2602.05859</link><description>arXiv:2602.05859v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05859v1</guid></item><item><title>[arXiv-AI 2026] EuroLLM-22B: Technical Report</title><link>https://arxiv.org/abs/2602.05879</link><description>arXiv:2602.05879v1 Announce Type: cross 
Abstract: This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05879v1</guid></item><item><title>[arXiv-AI 2026] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</title><link>https://arxiv.org/abs/2602.05885</link><description>arXiv:2602.05885v1 Announce Type: cross 
Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05885v1</guid></item><item><title>[arXiv-AI 2026] Shared LoRA Subspaces for almost Strict Continual Learning</title><link>https://arxiv.org/abs/2602.06043</link><description>arXiv:2602.06043v1 Announce Type: cross 
Abstract: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06043v1</guid></item><item><title>[arXiv-AI 2026] Explanations are a Means to an End: Decision Theoretic Explanation Evaluation</title><link>https://arxiv.org/abs/2506.22740</link><description>arXiv:2506.22740v2 Announce Type: replace 
Abstract: Explanations of model behavior are commonly evaluated via proxy properties weakly tied to the purposes explanations serve in practice. We contribute a decision theoretic framework that treats explanations as information signals valued by the expected improvement they enable on a specified decision task. This approach yields three distinct estimands: 1) a theoretical benchmark that upperbounds achievable performance by any agent with the explanation, 2) a human-complementary value that quantifies the theoretically attainable value that is not already captured by a baseline human decision policy, and 3) a behavioral value representing the causal effect of providing the explanation to human decision-makers. We instantiate these definitions in a practical validation workflow, and apply them to assess explanation potential and interpret behavioral effects in human-AI decision support and mechanistic interpretability.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.22740v2</guid></item><item><title>[arXiv-AI 2026] Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics</title><link>https://arxiv.org/abs/2508.12840</link><description>arXiv:2508.12840v3 Announce Type: replace 
Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for reasoning about both the physical world and the beliefs of agents, with applications in domains where information flow and awareness among agents are critical. The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability. To address this, we exploit Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process. GNNs, which naturally capture the graph-like nature of Kripke models, allow us to derive meaningful estimates of state quality -- e.g., the distance from the nearest goal -- by generalizing knowledge obtained from previously solved planning instances. We integrate these predictive heuristics into an epistemic planning pipeline and evaluate them against standard baselines, showing improvements in the scalability of multi-agent epistemic planning.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.12840v3</guid></item><item><title>[arXiv-AI 2026] Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2601.08641</link><description>arXiv:2601.08641v3 Announce Type: replace 
Abstract: Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market's extremely illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from na\"ive copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal large language model (LLM) and chain-of-thought (CoT) reasoning. Our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average copier return of 3% per meme coin investment under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08641v3</guid></item><item><title>[arXiv-AI 2026] KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning</title><link>https://arxiv.org/abs/2602.03034</link><description>arXiv:2602.03034v2 Announce Type: replace 
Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03034v2</guid></item><item><title>[arXiv-AI 2026] Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration</title><link>https://arxiv.org/abs/2602.04575</link><description>arXiv:2602.04575v2 Announce Type: replace 
Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04575v2</guid></item><item><title>[arXiv-AI 2026] CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables</title><link>https://arxiv.org/abs/2403.01673</link><description>arXiv:2403.01673v2 Announce Type: replace-cross 
Abstract: For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the difficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS - continuity, sparsity, and variability - are identified and implemented through different modules. Even with a basic 2-layer MLP as core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it an efficient and transferable MTSF solution.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2403.01673v2</guid></item><item><title>[arXiv-AI 2026] Relational Graph Transformer</title><link>https://arxiv.org/abs/2505.10960</link><description>arXiv:2505.10960v2 Announce Type: replace-cross 
Abstract: Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.10960v2</guid></item><item><title>[arXiv-AI 2026] SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?</title><link>https://arxiv.org/abs/2505.20295</link><description>arXiv:2505.20295v4 Announce Type: replace-cross 
Abstract: The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables. To support the development of this universal form of LLM uncertainties, we publish the code that implements our metric for arbitrary LLMs under https://github.com/apple/ml-selfreflect .</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.20295v4</guid></item><item><title>[arXiv-AI 2026] STACK: Adversarial Attacks on LLM Safeguard Pipelines</title><link>https://arxiv.org/abs/2506.24068</link><description>arXiv:2506.24068v3 Announce Type: replace-cross 
Abstract: Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic and OpenAI guard their latest Opus 4 model and GPT-5 models using such defense pipelines, and other frontier developers including Google DeepMind pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.24068v3</guid></item><item><title>[arXiv-AI 2026] Amortized Sampling with Transferable Normalizing Flows</title><link>https://arxiv.org/abs/2508.18175</link><description>arXiv:2508.18175v4 Announce Type: replace-cross 
Abstract: Efficient equilibrium sampling of molecular conformations remains a core challenge in computational chemistry and statistical inference. Classical approaches such as molecular dynamics or Markov chain Monte Carlo inherently lack amortization; the computational cost of sampling must be paid in full for each system of interest. The widespread success of generative models has inspired interest towards overcoming this limitation through learning sampling algorithms. Despite performing competitively with conventional methods when trained on a single system, learned samplers have so far demonstrated limited ability to transfer across systems. We demonstrate that deep learning enables the design of scalable and transferable samplers by introducing Prose, a 285 million parameter all-atom transferable normalizing flow trained on a corpus of peptide molecular dynamics trajectories up to 8 residues in length. Prose draws zero-shot uncorrelated proposal samples for arbitrary peptide systems, achieving the previously intractable transferability across sequence length, whilst retaining the efficient likelihood evaluation of normalizing flows. Through extensive empirical evaluation we demonstrate the efficacy of Prose as a proposal for a variety of sampling algorithms, finding a simple importance sampling-based fine-tuning procedure to achieve competitive performance to established methods such as sequential Monte Carlo. We open-source the Prose codebase, model weights, and training dataset, to further stimulate research into amortized sampling methods and objectives.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.18175v4</guid></item><item><title>[arXiv-AI 2026] Language Models and Logic Programs for Trustworthy Tax Reasoning</title><link>https://arxiv.org/abs/2508.21051</link><description>arXiv:2508.21051v3 Announce Type: replace-cross 
Abstract: According to the United States Internal Revenue Service, ``the average American spends $\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the effectiveness of applying semantic parsing methods to statutory reasoning, and show promising economic feasibility of neuro-symbolic architectures for increasing access to reliable tax assistance.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.21051v3</guid></item><item><title>[arXiv-AI 2026] Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>arXiv:2509.03531v2 Announce Type: replace-cross 
Abstract: Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets entity-level hallucinations-e.g., fabricated names, dates, citations-rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Despite being trained only to detect hallucinated entities, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.03531v2</guid></item><item><title>[arXiv-AI 2026] BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring</title><link>https://arxiv.org/abs/2509.06690</link><description>arXiv:2509.06690v2 Announce Type: replace-cross 
Abstract: Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.06690v2</guid></item><item><title>[arXiv-AI 2026] Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>arXiv:2510.09712v3 Announce Type: replace-cross 
Abstract: Online fake news profoundly distorts public judgment and erodes trust in social platforms. While existing detectors achieve competitive performance on benchmark datasets, they remain notably vulnerable to malicious comments designed specifically to induce misclassification. This evolving threat landscape necessitates detection systems that simultaneously prioritize predictive accuracy and structural robustness. However, current detectors often fail to generalize across diverse and novel comment attack patterns. To bridge this gap, we propose AdComment, an adaptive adversarial training framework for robustness enhancement against diverse malicious comments. Based on cognitive psychology, we categorize adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and leverage LLMs to synthesize diverse, category-specific perturbations. Central to our framework is an InfoDirichlet Resampling (IDR) mechanism that dynamically adjusts malicious comment proportions during training, thereby steering optimization toward the model's most susceptible regions. Experimental results demonstrate that our approach achieves state-of-the-art performance on three benchmark datasets, improving the F1 scores by 17.9%, 14.5% and 9.0%, respectively.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.09712v3</guid></item><item><title>[arXiv-AI 2026] Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models</title><link>https://arxiv.org/abs/2510.17196</link><description>arXiv:2510.17196v2 Announce Type: replace-cross 
Abstract: Effectively processing long contexts is a critical challenge for language models. While standard Transformers are limited by quadratic complexity and poor length extrapolation, alternative architectures like sliding window attention and state space models sacrifice the ability to effectively utilize the full context due to their fixed-size memory. Chunk-based sparse attention has emerged as a promising paradigm for extreme length generalization, yet the key architectural principles underpinning its success are not yet fully understood. In this work, we present a systematic dissection of these models to identify the core components driving their performance. Through a unified framework and comprehensive ablation studies, we demonstrate that a combination of three design principles is critical: (1) an expressive, non-linear Chunk Encoder with a dedicated CLS token to produce representations for retrieval; (2) a Bypassing Residual Path to stably integrate retrieved global information without it being overridden by the local residual stream; and (3) enforced selection sparsity during pre-training to bridge the train-test distribution gap. We provide a theoretical motivation for intra-chunk information processing and landmark generation. By combining these principles, we establish a new state-of-the-art for training-free length extrapolation, successfully generalizing models trained on a 4K context to 32 million tokens on RULER and BABILong. Our findings provide a clear and empirically-grounded set of design principles for developing future, highly-capable long-context language models.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.17196v2</guid></item><item><title>[arXiv-AI 2026] Auto-Rubric: Learning From Implicit Weights to Explicit Rubrics for Reward Modeling</title><link>https://arxiv.org/abs/2510.17314</link><description>arXiv:2510.17314v2 Announce Type: replace-cross 
Abstract: Conventional reward modeling relies on gradient descent over neural weights, creating opaque, data-hungry "black boxes." We propose a paradigm shift from implicit to explicit reward parameterization, recasting optimization from continuous weight spaces to the discrete space of natural language rubrics. We introduce a training-free framework based on iterative rubric learning: it locally induces discriminative criteria via verification-driven refinement, and globally compresses the candidate criteria pool into a compact core set by maximizing an information-theoretic coding rate objective. We organize the compressed core set into a hierarchical rubric structure -- high-level evaluation dimensions supported by concrete verification checks -- serving as an interpretable, portable reward function. Empirically, our approach challenges prevailing data scaling assumptions: using only 70 preference pairs, our rubric-guided judges outperform fully trained reward models on diverse benchmarks. For instance, Qwen3-8B equipped with our learned rubrics achieves 80.91% on RewardBench2, surpassing the specialized Skywork-Reward-V2-Qwen3-8B (78.20%). These results demonstrate that alignment signals are highly compressible and can be effectively captured through explicit symbolic search.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.17314v2</guid></item><item><title>[arXiv-AI 2026] Differentiable Constraint-Based Causal Discovery</title><link>https://arxiv.org/abs/2510.22031</link><description>arXiv:2510.22031v2 Announce Type: replace-cross 
Abstract: Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code and data of the proposed method are publicly available at https://github$.$com/PurdueMINDS/DAGPA.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.22031v2</guid></item><item><title>[arXiv-AI 2026] Your Latent Reasoning is Secretly Policy Improvement Operator</title><link>https://arxiv.org/abs/2511.16886</link><description>arXiv:2511.16886v4 Announce Type: replace-cross 
Abstract: Recently, small models with latent recursion have obtained promising results on complex reasoning tasks. These results are typically explained by the theory that such recursion increases a networks depth, allowing it to compactly emulate the capacity of larger models. However, the performance of recursively added layers remains behind the capabilities of one pass models with the same feed forward depth. This means that in the looped version, not every recursive step effectively contributes to depth. This raises the question: when and why does latent reasoning improve performance, and when does it result in dead compute? In our work, we analyze the algorithms that latent reasoning provides answer to this question. We show that latent reasoning can be formalized as a classifier free guidance and policy improvement algorithm. Building on these insights, we propose to use a training schemes from reinforcement learning and diffusion methods for latent reasoning models. Using the Tiny Recursive Model as our testbed, we show that with our modifications we can avoid dead compute steps and reduce the total number of forward passes by 18x while maintaining performance. Broadly speaking, we show how a policy improvement perspective on recursive steps can explain model behavior and provide insights for further improvements.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.16886v4</guid></item><item><title>[arXiv-AI 2026] WebSTAR: Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering</title><link>https://arxiv.org/abs/2512.10962</link><description>arXiv:2512.10962v3 Announce Type: replace-cross 
Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 267K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal process reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight process reward model (StepRM) as practical tools to advance robust and efficient CUAs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.10962v3</guid></item><item><title>[arXiv-AI 2026] GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR</title><link>https://arxiv.org/abs/2601.09361</link><description>arXiv:2601.09361v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09361v2</guid></item><item><title>[arXiv-AI 2026] Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency</title><link>https://arxiv.org/abs/2602.00151</link><description>arXiv:2602.00151v2 Announce Type: replace-cross 
Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00151v2</guid></item><item><title>[arXiv-AI 2026] Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints</title><link>https://arxiv.org/abs/2602.00166</link><description>arXiv:2602.00166v2 Announce Type: replace-cross 
Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00166v2</guid></item><item><title>[arXiv-AI 2026] Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</title><link>https://arxiv.org/abs/2602.00906</link><description>arXiv:2602.00906v4 Announce Type: replace-cross 
Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00906v4</guid></item><item><title>[arXiv-AI 2026] Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment</title><link>https://arxiv.org/abs/2602.01023</link><description>arXiv:2602.01023v3 Announce Type: replace-cross 
Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\% reduction in keystrokes and 3.46\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01023v3</guid></item><item><title>[arXiv-AI 2026] Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community</title><link>https://arxiv.org/abs/2602.02613</link><description>arXiv:2602.02613v2 Announce Type: replace-cross 
Abstract: The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02613v2</guid></item><item><title>[arXiv-AI 2026] Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</title><link>https://arxiv.org/abs/2602.03190</link><description>arXiv:2602.03190v2 Announce Type: replace-cross 
Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 45.2 per-benchmark accuracy and 51.8 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03190v2</guid></item><item><title>[arXiv-LG 2026] Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering</title><link>https://arxiv.org/abs/2602.04903</link><description>arXiv:2602.04903v1 Announce Type: new 
Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04903v1</guid></item><item><title>[arXiv-LG 2026] LISA: Laplacian In-context Spectral Analysis</title><link>https://arxiv.org/abs/2602.04906</link><description>arXiv:2602.04906v1 Announce Type: new 
Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04906v1</guid></item><item><title>[arXiv-LG 2026] Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution</title><link>https://arxiv.org/abs/2602.04918</link><description>arXiv:2602.04918v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04918v1</guid></item><item><title>[arXiv-LG 2026] Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog</title><link>https://arxiv.org/abs/2602.04919</link><description>arXiv:2602.04919v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04919v1</guid></item><item><title>[arXiv-LG 2026] Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data</title><link>https://arxiv.org/abs/2602.05053</link><description>arXiv:2602.05053v1 Announce Type: new 
Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05053v1</guid></item><item><title>[arXiv-LG 2026] Adaptive Exploration for Latent-State Bandits</title><link>https://arxiv.org/abs/2602.05139</link><description>arXiv:2602.05139v1 Announce Type: new 
Abstract: The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05139v1</guid></item><item><title>[arXiv-LG 2026] Extreme Weather Nowcasting via Local Precipitation Pattern Prediction</title><link>https://arxiv.org/abs/2602.05204</link><description>arXiv:2602.05204v1 Announce Type: new 
Abstract: Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05204v1</guid></item><item><title>[arXiv-LG 2026] Disentangled Representation Learning via Flow Matching</title><link>https://arxiv.org/abs/2602.05214</link><description>arXiv:2602.05214v1 Announce Type: new 
Abstract: Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05214v1</guid></item><item><title>[arXiv-LG 2026] Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions</title><link>https://arxiv.org/abs/2602.05234</link><description>arXiv:2602.05234v1 Announce Type: new 
Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05234v1</guid></item><item><title>[arXiv-LG 2026] Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization</title><link>https://arxiv.org/abs/2602.05285</link><description>arXiv:2602.05285v1 Announce Type: new 
Abstract: In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05285v1</guid></item><item><title>[arXiv-LG 2026] Smoothness Errors in Dynamics Models and How to Avoid Them</title><link>https://arxiv.org/abs/2602.05352</link><description>arXiv:2602.05352v1 Announce Type: new 
Abstract: Modern neural networks have shown promise for solving partial differential equations over surfaces, often by discretizing the surface as a mesh and learning with a mesh-aware graph neural network. However, graph neural networks suffer from oversmoothing, where a node's features become increasingly similar to those of its neighbors. Unitary graph convolutions, which are mathematically constrained to preserve smoothness, have been proposed to address this issue. Despite this, in many physical systems, such as diffusion processes, smoothness naturally increases and unitarity may be overconstraining. In this paper, we systematically study the smoothing effects of different GNNs for dynamics modeling and prove that unitary convolutions hurt performance for such tasks. We propose relaxed unitary convolutions that balance smoothness preservation with the natural smoothing required for physical systems. We also generalize unitary and relaxed unitary convolutions from graphs to meshes. In experiments on PDEs such as the heat and wave equations over complex meshes and on weather forecasting, we find that our method outperforms several strong baselines, including mesh-aware transformers and equivariant neural networks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05352v1</guid></item><item><title>[arXiv-LG 2026] Bayesian Neighborhood Adaptation for Graph Neural Networks</title><link>https://arxiv.org/abs/2602.05358</link><description>arXiv:2602.05358v1 Announce Type: new 
Abstract: The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05358v1</guid></item><item><title>[arXiv-LG 2026] Erase at the Core: Representation Unlearning for Machine Unlearning</title><link>https://arxiv.org/abs/2602.05375</link><description>arXiv:2602.05375v1 Announce Type: new 
Abstract: Many approximate machine unlearning methods demonstrate strong logit-level forgetting -- such as near-zero accuracy on the forget set -- yet continue to preserve substantial information within their internal feature representations. We refer to this discrepancy as superficial forgetting. Recent studies indicate that most existing unlearning approaches primarily alter the final classifier, leaving intermediate representations largely unchanged and highly similar to those of the original model. To address this limitation, we introduce the Erase at the Core (EC), a framework designed to enforce forgetting throughout the entire network hierarchy. EC integrates multi-layer contrastive unlearning on the forget set with retain set preservation through deeply supervised learning. Concretely, EC attaches auxiliary modules to intermediate layers and applies both contrastive unlearning and cross-entropy losses at each supervision point, with layer-wise weighted losses. Experimental results show that EC not only achieves effective logit-level forgetting, but also substantially reduces representational similarity to the original model across intermediate layers. Furthermore, EC is model-agnostic and can be incorporated as a plug-in module into existing unlearning methods, improving representation-level forgetting while maintaining performance on the retain set.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05375v1</guid></item><item><title>[arXiv-LG 2026] BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs</title><link>https://arxiv.org/abs/2602.05448</link><description>arXiv:2602.05448v1 Announce Type: new 
Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05448v1</guid></item><item><title>[arXiv-LG 2026] Logical Guidance for the Exact Composition of Diffusion Models</title><link>https://arxiv.org/abs/2602.05549</link><description>arXiv:2602.05549v1 Announce Type: new 
Abstract: We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.
  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.
  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.
  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.
  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.
  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.
  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.
  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05549v1</guid></item><item><title>[arXiv-LG 2026] MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks</title><link>https://arxiv.org/abs/2602.05567</link><description>arXiv:2602.05567v1 Announce Type: new 
Abstract: Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05567v1</guid></item><item><title>[arXiv-LG 2026] Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection</title><link>https://arxiv.org/abs/2602.05667</link><description>arXiv:2602.05667v1 Announce Type: new 
Abstract: Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05667v1</guid></item><item><title>[arXiv-LG 2026] Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents</title><link>https://arxiv.org/abs/2602.05810</link><description>arXiv:2602.05810v1 Announce Type: new 
Abstract: Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05810v1</guid></item><item><title>[arXiv-LG 2026] Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks</title><link>https://arxiv.org/abs/2602.05873</link><description>arXiv:2602.05873v1 Announce Type: new 
Abstract: Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05873v1</guid></item><item><title>[arXiv-LG 2026] ContextBench: A Benchmark for Context Retrieval in Coding Agents</title><link>https://arxiv.org/abs/2602.05892</link><description>arXiv:2602.05892v1 Announce Type: new 
Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05892v1</guid></item><item><title>[arXiv-LG 2026] Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training</title><link>https://arxiv.org/abs/2602.05933</link><description>arXiv:2602.05933v1 Announce Type: new 
Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$\chi^2$ regularizer. This additional $\chi^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05933v1</guid></item><item><title>[arXiv-LG 2026] Tuning Out-of-Distribution (OOD) Detectors Without Given OOD Data</title><link>https://arxiv.org/abs/2602.05935</link><description>arXiv:2602.05935v1 Announce Type: new 
Abstract: Existing out-of-distribution (OOD) detectors are often tuned by a separate dataset deemed OOD with respect to the training distribution of a neural network (NN). OOD detectors process the activations of NN layers and score the output, where parameters of the detectors are determined by fitting to an in-distribution (training) set and the aforementioned dataset chosen adhocly. At detector training time, this adhoc dataset may not be available or difficult to obtain, and even when it's available, it may not be representative of actual OOD data, which is often ''unknown unknowns." Current benchmarks may specify some left-out set from test OOD sets. We show that there can be significant variance in performance of detectors based on the adhoc dataset chosen in current literature, and thus even if such a dataset can be collected, the performance of the detector may be highly dependent on the choice. In this paper, we introduce and formalize the often neglected problem of tuning OOD detectors without a given ``OOD'' dataset. To this end, we present strong baselines as an attempt to approach this problem. Furthermore, we propose a new generic approach to OOD detector tuning that does not require any extra data other than those used to train the NN. We show that our approach improves over baseline methods consistently across higher-parameter OOD detector families, while being comparable across lower-parameter families.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05935v1</guid></item><item><title>[arXiv-LG 2026] Dimensionality Reduction on Riemannian Manifolds in Data Analysis</title><link>https://arxiv.org/abs/2602.05936</link><description>arXiv:2602.05936v1 Announce Type: new 
Abstract: In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05936v1</guid></item><item><title>[arXiv-LG 2026] AP-OOD: Attention Pooling for Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2602.06031</link><description>arXiv:2602.06031v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06031v1</guid></item><item><title>[arXiv-LG 2026] Atomic Information Flow: A Network Flow Model for Tool Attributions in RAG Systems</title><link>https://arxiv.org/abs/2602.04912</link><description>arXiv:2602.04912v1 Announce Type: cross 
Abstract: Many tool-based Retrieval Augmented Generation (RAG) systems lack precise mechanisms for tracing final responses back to specific tool components -- a critical gap as systems scale to complex multi-agent architectures. We present \textbf{Atomic Information Flow (AIF)}, a graph-based network flow model that decomposes tool outputs and LLM calls into atoms: indivisible, self-contained units of information. By modeling LLM orchestration as a directed flow of atoms from tool and LLM nodes to a response super-sink, AIF enables granular attribution metrics for AI explainability.
  Motivated by the max-flow min-cut theorem in network flow theory, we train a lightweight Gemma3 (4B parameter) language model as a context compressor to approximate the minimum cut of tool atoms using flow signals computed offline by AIF. We note that the base Gemma3-4B model struggles to identify critical information with \textbf{54.7\%} accuracy on HotpotQA, barely outperforming lexical baselines (BM25). However, post-training on AIF signals boosts accuracy to \textbf{82.71\%} (+28.01 points) while achieving \textbf{87.52\%} (+1.85\%) context token compression -- bridging the gap with the Gemma3-27B variant, a model nearly $7\times$ larger.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04912v1</guid></item><item><title>[arXiv-LG 2026] Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking</title><link>https://arxiv.org/abs/2602.05079</link><description>arXiv:2602.05079v1 Announce Type: cross 
Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05079v1</guid></item><item><title>[arXiv-LG 2026] Path Sampling for Rare Events Boosted by Machine Learning</title><link>https://arxiv.org/abs/2602.05167</link><description>arXiv:2602.05167v1 Announce Type: cross 
Abstract: The study by Jung et al. (Jung H, Covino R, Arjun A, et al., Nat Comput Sci. 3:334-345 (2023)) introduced Artificial Intelligence for Molecular Mechanism Discovery (AIMMD), a novel sampling algorithm that integrates machine learning to enhance the efficiency of transition path sampling (TPS). By enabling on-the-fly estimation of the committor probability and simultaneously deriving a human-interpretable reaction coordinate, AIMMD offers a robust framework for elucidating the mechanistic pathways of complex molecular processes. This commentary provides a discussion and critical analysis of the core AIMMD framework, explores its recent extensions, and offers an assessment of the method's potential impact and limitations.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05167v1</guid></item><item><title>[arXiv-LG 2026] Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky</title><link>https://arxiv.org/abs/2602.05189</link><description>arXiv:2602.05189v1 Announce Type: cross 
Abstract: As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05189v1</guid></item><item><title>[arXiv-LG 2026] Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks</title><link>https://arxiv.org/abs/2602.05374</link><description>arXiv:2602.05374v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05374v1</guid></item><item><title>[arXiv-LG 2026] Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better</title><link>https://arxiv.org/abs/2602.05393</link><description>arXiv:2602.05393v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\times$ speedup with nearly 5\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\times$ fewer parameters than the target model.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05393v1</guid></item><item><title>[arXiv-LG 2026] Broken neural scaling laws in materials science</title><link>https://arxiv.org/abs/2602.05702</link><description>arXiv:2602.05702v1 Announce Type: cross 
Abstract: In materials science, data are scarce and expensive to generate, whether computationally or experimentally. Therefore, it is crucial to identify how model performance scales with dataset size and model capacity to distinguish between data- and model-limited regimes. Neural scaling laws provide a framework for quantifying this behavior and guide the design of materials datasets and machine learning architectures. Here, we investigate neural scaling laws for a paradigmatic materials science task: predicting the dielectric function of metals, a high-dimensional response that governs how solids interact with light. Using over 200,000 dielectric functions from high-throughput ab initio calculations, we study two multi-objective graph neural networks trained to predict the frequency-dependent complex interband dielectric function and the Drude frequency. We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law that rapidly saturates.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05702v1</guid></item><item><title>[arXiv-LG 2026] Ethology of Latent Spaces</title><link>https://arxiv.org/abs/2602.05710</link><description>arXiv:2602.05710v1 Announce Type: cross 
Abstract: This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.
  Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.
  We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05710v1</guid></item><item><title>[arXiv-LG 2026] A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion</title><link>https://arxiv.org/abs/2602.05855</link><description>arXiv:2602.05855v1 Announce Type: cross 
Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05855v1</guid></item><item><title>[arXiv-LG 2026] Energy Guided smoothness to improve Robustness in Graph Classification</title><link>https://arxiv.org/abs/2412.08419</link><description>arXiv:2412.08419v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are powerful at solving graph classification tasks, yet applied problems often contain noisy labels. In this work, we study GNN robustness to label noise, demonstrate GNN failure modes when models struggle to generalise on low-order graphs, low label coverage, or when a model is over-parameterized. We establish both empirical and theoretical links between GNN robustness and the reduction of the total Dirichlet Energy of learned node representations, which encapsulates the hypothesized GNN smoothness inductive bias. Finally, we introduce two training strategies to enhance GNN robustness: (1) by incorporating a novel inductive bias in the weight matrices through the removal of negative eigenvalues, connected to Dirichlet Energy minimization; (2) by extending to GNNs a loss penalty that promotes learned smoothness. Importantly, neither approach negatively impacts performance in noise-free settings, supporting our hypothesis that the source of GNNs robustness is their smoothness inductive bias.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2412.08419v2</guid></item><item><title>[arXiv-LG 2026] VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model</title><link>https://arxiv.org/abs/2502.01989</link><description>arXiv:2502.01989v4 Announce Type: replace 
Abstract: Inspired by human SYSTEM 2 thinking, LLMs excel at complex reasoning tasks via extended Chain-of-Thought. However, similar test-time scaling for diffusion models to tackle complex reasoning remains largely unexplored. From existing work, two primary challenges emerge in this setting: (i) the dependence on an external verifier indicating a notable gap from intrinsic reasoning of human intelligence without any external feedback, and (ii) the lack of an efficient search algorithm. In this paper, we introduce the Verifier-free Test-time Scalable Diffusion Model (VFScale) to achieve scalable intrinsic reasoning, which equips number-of-sample test-time scaling with the intrinsic energy function of diffusion models as the verifier. Concretely, VFScale comprises two key innovations to address the aforementioned challenges. On the training side, VFScale consists of a novel MRNCL loss and a KL regularization to improve the energy landscape, ensuring that the learned energy function itself serves as a reliable verifier. On the inference side, VFScale integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS) to improve search efficiency. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of VFScale's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our VFScale solves 88% of Maze problems with much larger sizes of $15\times15$, while standard diffusion models completely fail. The code can be found at https://github.com/AI4Science-WestlakeU/VFScale.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2502.01989v4</guid></item><item><title>[arXiv-LG 2026] Are Your Generated Instances Truly Useful? GenBench-MILP: A Benchmark Suite for MILP Instance Generation</title><link>https://arxiv.org/abs/2505.24779</link><description>arXiv:2505.24779v4 Announce Type: replace 
Abstract: The proliferation of machine learning-based methods for Mixed-Integer Linear Programming (MILP) instance generation has surged, driven by the need for diverse training datasets. However, a critical question remains: Are these generated instances truly useful and realistic? Current evaluation protocols often rely on superficial structural metrics or simple solvability checks, which frequently fail to capture the true computational complexity of real-world problems. To bridge this gap, we introduce GenBench-MILP, a comprehensive benchmark suite designed for the standardized and objective evaluation of MILP generators. Our framework assesses instance quality across four key dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream tasks. A distinctive innovation of GenBench-MILP is the analysis of solver-internal features -- including root node gaps, heuristic success rates, and cut plane usage. By treating the solver's dynamic behavior as an expert assessment, we reveal nuanced computational discrepancies that static graph features miss. Our experiments on instance generative models demonstrate that instances with high structural similarity scores can still exhibit drastically divergent solver interactions and difficulty levels. By providing this multifaceted evaluation toolkit, GenBench-MILP aims to facilitate rigorous comparisons and guide the development of high-fidelity instance generators.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.24779v4</guid></item><item><title>[arXiv-LG 2026] "Faithful to What?" On the Limits of Fidelity-Based Explanations</title><link>https://arxiv.org/abs/2506.12176</link><description>arXiv:2506.12176v4 Announce Type: replace 
Abstract: In explainable AI, surrogate models are commonly evaluated by their fidelity to a neural network's predictions. Fidelity, however, measures alignment to a learned model rather than alignment to the data-generating signal underlying the task. This work introduces the linearity score $\lambda(f)$, a diagnostic that quantifies the extent to which a regression network's input--output behavior is linearly decodable. $\lambda(f)$ is defined as an $R^2$ measure of surrogate fit to the network. Across synthetic and real-world regression datasets, we find that surrogates can achieve high fidelity to a neural network while failing to recover the predictive gains that distinguish the network from simpler models. In several cases, high-fidelity surrogates underperform even linear baselines trained directly on the data. These results demonstrate that explaining a model's behavior is not equivalent to explaining the task-relevant structure of the data, highlighting a limitation of fidelity-based explanations when used to reason about predictive performance.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.12176v4</guid></item><item><title>[arXiv-LG 2026] Multi-Agent Inverted Transformer for Flight Trajectory Prediction</title><link>https://arxiv.org/abs/2509.21004</link><description>arXiv:2509.21004v2 Announce Type: replace 
Abstract: Flight trajectory prediction for multiple aircraft is essential and provides critical insights into how aircraft navigate within current air traffic flows. However, predicting multi-agent flight trajectories is inherently challenging. One of the major difficulties is modeling both the individual aircraft behaviors over time and the complex interactions between flights. Generating explainable prediction outcomes is also a challenge. Therefore, we propose a Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture that predicts multi-agent flight trajectories. The proposed framework features two key attention modules: (i) masked multivariate attention, which captures spatio-temporal patterns of individual aircraft, and (ii) agent attention, which models the social patterns among multiple agents in complex air traffic scenes. We evaluated MAIFormer using a real-world automatic dependent surveillance-broadcast flight trajectory dataset from the terminal airspace of Incheon International Airport in South Korea. The experimental results show that MAIFormer achieves the best performance across multiple metrics and outperforms other methods. In addition, MAIFormer produces prediction outcomes that are interpretable from a human perspective, which improves both the transparency of the model and its practical utility in air traffic control.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21004v2</guid></item><item><title>[arXiv-LG 2026] Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction</title><link>https://arxiv.org/abs/2512.24139</link><description>arXiv:2512.24139v4 Announce Type: replace 
Abstract: While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \(1-\alpha \pm \delta\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.24139v4</guid></item><item><title>[arXiv-LG 2026] Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks</title><link>https://arxiv.org/abs/2601.22579</link><description>arXiv:2601.22579v4 Announce Type: replace 
Abstract: Malicious bots pose a growing threat to e-commerce platforms by scraping data, hoarding inventory, and perpetrating fraud. Traditional bot mitigation techniques, including IP blacklists and CAPTCHA-based challenges, are increasingly ineffective or intrusive, as modern bots leverage proxies, botnets, and AI-assisted evasion strategies. This work proposes a non-intrusive graph-based bot detection framework for e-commerce that models user session behavior through a graph representation and applies an inductive graph neural network for classification. The approach captures both relational structure and behavioral semantics, enabling accurate identification of subtle automated activity that evades feature-based methods. Experiments on real-world e-commerce traffic demonstrate that the proposed inductive graph model outperforms a strong session-level multilayer perceptron baseline in terms of AUC and F1 score. Additional adversarial perturbation and cold-start simulations show that the model remains robust under moderate graph modifications and generalizes effectively to previously unseen sessions and URLs. The proposed framework is deployment-friendly, integrates with existing systems without client-side instrumentation, and supports real-time inference and incremental updates, making it suitable for practical e-commerce security deployments.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22579v4</guid></item><item><title>[arXiv-LG 2026] Training Data Efficiency in Multimodal Process Reward Models</title><link>https://arxiv.org/abs/2602.04145</link><description>arXiv:2602.04145v2 Announce Type: replace 
Abstract: Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training. Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora. To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04145v2</guid></item><item><title>[arXiv-LG 2026] Root Cause Analysis of Outliers with Missing Structural Knowledge</title><link>https://arxiv.org/abs/2406.05014</link><description>arXiv:2406.05014v4 Announce Type: replace-cross 
Abstract: The goal of Root Cause Analysis (RCA) is to explain why an anomaly occurred by identifying where the fault originated. Several recent works model the anomalous event as resulting from a change in the causal mechanism at the root cause, i.e., as a soft intervention. RCA is then the task of identifying which causal mechanism changed. In real-world applications, one often has either few or only a single sample from the post-intervention distribution: a severe limitation for most methods, which assume one knows or can estimate the distribution. However, even those that do not are statistically ill-posed due to the need to probe regression models in regions of low probability density. In this paper, we propose simple, efficient methods to overcome both difficulties in the case where there is a single root cause and the causal graph is a polytree. When one knows the causal graph, we give guarantees for a traversal algorithm that requires only marginal anomaly scores and does not depend on specifying an arbitrary anomaly score cut-off. When one does not know the causal graph, we show that the heuristic of identifying root causes as the variables with the highest marginal anomaly scores is causally justified. To this end, we prove that anomalies with small scores are unlikely to cause those with larger scores in polytrees and give upper bounds for the likelihood of causal pathways with non-monotonic anomaly scores.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2406.05014v4</guid></item><item><title>[arXiv-LG 2026] A physics-based data-driven model for CO$_2$ gas diffusion electrodes to drive automated laboratories</title><link>https://arxiv.org/abs/2502.06323</link><description>arXiv:2502.06323v2 Announce Type: replace-cross 
Abstract: The electrochemical reduction of atmospheric CO$_2$ into high-energy molecules with renewable energy is a promising avenue for energy storage that can take advantage of existing infrastructure especially in areas where sustainable alternatives to fossil fuels do not exist. Automated laboratories are currently being developed and used to optimize the composition and operating conditions of gas diffusion electrodes (GDEs), the device in which this reaction takes place. Improving the efficiency of GDEs is crucial for this technology to become viable. Here we present a modeling framework to efficiently explore the high-dimensional parameter space of GDE designs in an active learning context. At the core of the framework is an uncertainty-aware physics model calibrated with experimental data. The model has the flexibility to capture various input parameter spaces and any carbon products which can be modeled with Tafel kinetics. It is interpretable, and a Gaussian process layer can capture deviations of real data from the function space of the physical model itself. We deploy the model in a simulated active learning setup with real electrochemical data gathered by the AdaCarbon automated laboratory and show that it can be used to efficiently traverse the multi-dimensional parameter space.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2502.06323v2</guid></item><item><title>[arXiv-LG 2026] PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective</title><link>https://arxiv.org/abs/2505.21799</link><description>arXiv:2505.21799v4 Announce Type: replace-cross 
Abstract: The ever-growing scale of deep learning models and training data underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing "matrix-aware" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.21799v4</guid></item><item><title>[arXiv-LG 2026] BACHI: Boundary-Aware Symbolic Chord Recognition Through Masked Iterative Decoding on Pop and Classical Music</title><link>https://arxiv.org/abs/2510.06528</link><description>arXiv:2510.06528v2 Announce Type: replace-cross 
Abstract: Automatic chord recognition (ACR) via deep learning models has gradually achieved promising recognition accuracy, yet two key challenges remain. First, prior work has primarily focused on audio-domain ACR, while symbolic music (e.g., score) ACR has received limited attention due to data scarcity. Second, existing methods still overlook strategies that are aligned with human music analytical practices. To address these challenges, we make two contributions: (1) we introduce POP909-CL, an enhanced version of POP909 dataset with tempo-aligned content and human-corrected labels of chords, beats, keys, and time signatures; and (2) We propose BACHI, a symbolic chord recognition model that decomposes the task into different decision steps, namely boundary detection and iterative ranking of chord root, quality, and bass (inversion). This mechanism mirrors the human ear-training practices. Experiments demonstrate that BACHI achieves state-of-the-art chord recognition performance on both classical and pop music benchmarks, with ablation studies validating the effectiveness of each module.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.06528v2</guid></item><item><title>[arXiv-LG 2026] Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space</title><link>https://arxiv.org/abs/2601.20339</link><description>arXiv:2601.20339v2 Announce Type: replace-cross 
Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20339v2</guid></item><item><title>[arXiv-LG 2026] Provably Reliable Classifier Guidance via Cross-Entropy Control</title><link>https://arxiv.org/abs/2601.21200</link><description>arXiv:2601.21200v2 Announce Type: replace-cross 
Abstract: Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of the log-probability predicted by a probabilistic classifier. In practice, this classifier is usually obtained by minimizing an empirical loss function. While existing statistical theory guarantees good generalization performance when the sample size is sufficiently large, it remains unclear whether such training yields an effective guidance mechanism.
  We study this question in the context of cross-entropy loss, which is widely used for classifier training. Under mild smoothness assumptions on the classifier, we show that controlling the cross-entropy at each diffusion model step is sufficient to control the corresponding guidance error. In particular, probabilistic classifiers achieving conditional KL divergence $\varepsilon^2$ induce guidance vectors with mean squared error $\widetilde O(d \varepsilon )$, up to constant and logarithmic factors. Our result yields an upper bound on the sampling error of classifier-guided diffusion models and bears resemblance to a reverse log-Sobolev--type inequality. To the best of our knowledge, this is the first result that quantitatively links classifier training to guidance alignment in diffusion models, providing both a theoretical explanation for the empirical success of classifier guidance, and principled guidelines for selecting classifiers that induce effective guidance.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.21200v2</guid></item><item><title>[arXiv-CL 2026] Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems</title><link>https://arxiv.org/abs/2602.05176</link><description>arXiv:2602.05176v1 Announce Type: new 
Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05176v1</guid></item><item><title>[arXiv-CL 2026] Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions</title><link>https://arxiv.org/abs/2602.05220</link><description>arXiv:2602.05220v1 Announce Type: new 
Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05220v1</guid></item><item><title>[arXiv-CL 2026] MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning</title><link>https://arxiv.org/abs/2602.05307</link><description>arXiv:2602.05307v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05307v1</guid></item><item><title>[arXiv-CL 2026] OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration</title><link>https://arxiv.org/abs/2602.05400</link><description>arXiv:2602.05400v1 Announce Type: new 
Abstract: As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05400v1</guid></item><item><title>[arXiv-CL 2026] Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation</title><link>https://arxiv.org/abs/2602.05419</link><description>arXiv:2602.05419v1 Announce Type: new 
Abstract: Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05419v1</guid></item><item><title>[arXiv-CL 2026] Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision</title><link>https://arxiv.org/abs/2602.05471</link><description>arXiv:2602.05471v1 Announce Type: new 
Abstract: Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05471v1</guid></item><item><title>[arXiv-CL 2026] A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering</title><link>https://arxiv.org/abs/2602.05512</link><description>arXiv:2602.05512v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05512v1</guid></item><item><title>[arXiv-CL 2026] CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models</title><link>https://arxiv.org/abs/2602.05633</link><description>arXiv:2602.05633v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05633v1</guid></item><item><title>[arXiv-CL 2026] OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</title><link>https://arxiv.org/abs/2602.05843</link><description>arXiv:2602.05843v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., &gt; 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05843v1</guid></item><item><title>[arXiv-CL 2026] KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs</title><link>https://arxiv.org/abs/2602.05929</link><description>arXiv:2602.05929v1 Announce Type: new 
Abstract: Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05929v1</guid></item><item><title>[arXiv-CL 2026] Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions</title><link>https://arxiv.org/abs/2602.05932</link><description>arXiv:2602.05932v1 Announce Type: new 
Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05932v1</guid></item><item><title>[arXiv-CL 2026] Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training</title><link>https://arxiv.org/abs/2602.05940</link><description>arXiv:2602.05940v1 Announce Type: new 
Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05940v1</guid></item><item><title>[arXiv-CL 2026] A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies</title><link>https://arxiv.org/abs/2602.06015</link><description>arXiv:2602.06015v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06015v1</guid></item><item><title>[arXiv-CL 2026] Position: The Real Barrier to LLM Agent Usability is Agentic ROI</title><link>https://arxiv.org/abs/2505.17767</link><description>arXiv:2505.17767v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. While LLM agents are technically capable of performing a broad range of tasks, not all of these capabilities translate into meaningful usability. This position paper argues that the central question for LLM agent usability is no longer whether a task can be automated, but whether it delivers sufficient Agentic Return on Investment (Agentic ROI). Agentic ROI reframes evaluation from raw performance to a holistic, utility-driven perspective, guiding when, where, and for whom LLM agents should be deployed. Despite widespread application in high-ROI tasks like coding and scientific research, we identify a critical usability gap in mass-market, everyday applications. To address this, we propose a zigzag developmental trajectory: first scaling up to improve information gain and time savings, then scaling down to reduce cost. We present a strategic roadmap across these phases to make LLM agents truly usable, accessible, and scalable in real-world applications.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17767v2</guid></item><item><title>[arXiv-CL 2026] Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization</title><link>https://arxiv.org/abs/2509.20900</link><description>arXiv:2509.20900v3 Announce Type: replace 
Abstract: Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.20900v3</guid></item><item><title>[arXiv-CL 2026] LH-Deception: Simulating and Understanding LLM Deceptive Behaviors in Long-Horizon Interactions</title><link>https://arxiv.org/abs/2510.03999</link><description>arXiv:2510.03999v3 Announce Type: replace 
Abstract: Deception is a pervasive feature of human communication and an emerging concern in large language models (LLMs). While recent studies document instances of LLM deception, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce a new simulation framework, LH-Deception, for a systematic, empirical quantification of deception in LLMs under extended sequences of interdependent tasks and dynamic contextual pressures. LH-Deception is designed as a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed-source and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal emergent, long-horizon phenomena, such as ``chains of deception", which are invisible to static, single-turn evaluations. Our findings provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.03999v3</guid></item><item><title>[arXiv-CL 2026] DEBATE: A Large-Scale Benchmark for Evaluating Opinion Dynamics in Role-Playing LLM Agents</title><link>https://arxiv.org/abs/2510.25110</link><description>arXiv:2510.25110v4 Announce Type: replace 
Abstract: Accurately modeling opinion change through social interactions is crucial for understanding and mitigating polarization, misinformation, and societal conflict. Recent work simulates opinion dynamics with role-playing LLM agents (RPLAs), but multi-agent simulations often display unnatural group behavior (e.g., premature convergence) and lack empirical benchmarks for assessing alignment with real human group interactions. We introduce DEBATE, a large-scale benchmark for evaluating the authenticity of opinion dynamics in multi-agent RPLA simulations. DEBATE contains 36,383 messages from 2,832 U.S.-based participants across 708 groups and 107 topics, with both public messages and private Likert-scale beliefs, enabling evaluation at the utterance and group levels (and supporting future individual-level analyses). We instantiate "digital twin" RPLAs with seven LLMs and evaluate across two settings: next-message prediction and full conversation rollout, using stance-alignment and opinion-convergence metrics. In zero-shot settings, RPLA groups exhibit strong opinion convergence relative to human groups. Post-training via supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) improves stance alignment and brings group-level convergence closer to human behavior, though discrepancies in opinion change and belief updating remain. DEBATE enables rigorous benchmarking of simulated opinion dynamics and supports future research on aligning multi-agent RPLAs with realistic human interactions.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.25110v4</guid></item><item><title>[arXiv-CL 2026] Remembering Unequally: Global and Disciplinary Bias in LLM Reconstruction of Scholarly Coauthor Lists</title><link>https://arxiv.org/abs/2511.00476</link><description>arXiv:2511.00476v2 Announce Type: replace 
Abstract: Ongoing breakthroughs in large language models (LLMs) are reshaping scholarly search and discovery interfaces. While these systems offer new possibilities for navigating scientific knowledge, they also raise concerns about fairness and representational bias rooted in the models' memorized training data. As LLMs are increasingly used to answer queries about researchers and research communities, their ability to accurately reconstruct scholarly coauthor lists becomes an important but underexamined issue. In this study, we investigate how memorization in LLMs affects the reconstruction of coauthor lists and whether this process reflects existing inequalities across academic disciplines and world regions. We evaluate three prominent models, DeepSeek R1, Llama 4 Scout, and Mixtral 8x7B, by comparing their generated coauthor lists against bibliographic reference data. Our analysis reveals a systematic advantage for highly cited researchers, indicating that LLM memorization disproportionately favors already visible scholars. However, this pattern is not uniform: certain disciplines, such as Clinical Medicine, and some regions, including parts of Africa, exhibit more balanced reconstruction outcomes. These findings highlight both the risks and limitations of relying on LLM-generated relational knowledge in scholarly discovery contexts and emphasize the need for careful auditing of memorization-driven biases in LLM-based systems.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.00476v2</guid></item><item><title>[arXiv-CL 2026] Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models</title><link>https://arxiv.org/abs/2601.21826</link><description>arXiv:2601.21826v3 Announce Type: replace 
Abstract: As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.21826v3</guid></item><item><title>[arXiv-CL 2026] Short Chains, Deep Thoughts: Balancing Reasoning Efficiency and Intra-Segment Capability via Split-Merge Optimization</title><link>https://arxiv.org/abs/2602.03141</link><description>arXiv:2602.03141v2 Announce Type: replace 
Abstract: While Large Reasoning Models (LRMs) have demonstrated impressive capabilities in solving complex tasks through the generation of long reasoning chains, this reliance on verbose generation results in significant latency and computational overhead. To address these challenges, we propose \textbf{CoSMo} (\textbf{Co}nsistency-Guided \textbf{S}plit-\textbf{M}erge \textbf{O}ptimization), a framework designed to eliminate structural redundancy rather than indiscriminately restricting token volume. Specifically, CoSMo utilizes a split-merge algorithm that dynamically refines reasoning chains by merging redundant segments and splitting logical gaps to ensure coherence. We then employ structure-aligned reinforcement learning with a novel segment-level budget to supervise the model in maintaining efficient reasoning structures throughout training. Extensive experiments across multiple benchmarks and backbones demonstrate that CoSMo achieves superior performance, improving accuracy by \textbf{3.3} points while reducing segment usage by \textbf{28.7\%} on average compared to reasoning efficiency baselines.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03141v2</guid></item><item><title>[arXiv-CL 2026] CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation</title><link>https://arxiv.org/abs/2602.04856</link><description>arXiv:2602.04856v2 Announce Type: replace 
Abstract: From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04856v2</guid></item><item><title>[arXiv-CL 2026] Horizon-LM: A RAM-Centric Architecture for LLM Training</title><link>https://arxiv.org/abs/2602.04816</link><description>arXiv:2602.04816v2 Announce Type: replace-cross 
Abstract: The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04816v2</guid></item><item><title>[arXiv-IR 2026] AgenticTagger: Structured Item Representation for Recommendation with LLM Agents</title><link>https://arxiv.org/abs/2602.05945</link><description>arXiv:2602.05945v1 Announce Type: new 
Abstract: High-quality representations are a core requirement for effective recommendation. In this work, we study the problem of LLM-based descriptor generation, i.e., keyphrase-like natural language item representation generation frameworks with minimal constraints on downstream applications. We propose AgenticTagger, a framework that queries LLMs for representing items with sequences of text descriptors. However, open-ended generation provides little control over the generation space, leading to high cardinality, low-performance descriptors that renders downstream modeling challenging. To this end, AgenticTagger features two core stages: (1) a vocabulary building stage where a set of hierarchical, low-cardinality, and high-quality descriptors is identified, and (2) a vocabulary assignment stage where LLMs assign in-vocabulary descriptors to items. To effectively and efficiently ground vocabulary in the item corpus of interest, we design a multi-agent reflection mechanism where an architect LLM iteratively refines the vocabulary guided by parallelized feedback from annotator LLMs that validates the vocabulary against item data. Experiments on public and private data show AgenticTagger brings consistent improvements across diverse recommendation scenarios, including generative and term-based retrieval, ranking, and controllability-oriented, critique-based recommendation.</description><author>cs.IR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05945v1</guid></item><item><title>[arXiv-IR 2026] Efficient Long-Document Reranking via Block-Level Embeddings and Top-k Interaction Refinement</title><link>https://arxiv.org/abs/2501.17039</link><description>arXiv:2501.17039v2 Announce Type: replace 
Abstract: Dense encoders and LLM-based rerankers struggle with long documents: single-vector representations dilute fine-grained relevance, while cross-encoders are often too expensive for practical reranking. We present an efficient long-document reranking framework based on block-level embeddings. Each document is segmented into short blocks and encoded into block embeddings that can be precomputed offline. Given a query, we encode it once and score each candidate document by aggregating top-k query-block similarities with a simple weighted sum, yielding a strong and interpretable block-level relevance signal. To capture dependencies among the selected blocks and suppress redundancy, we introduce Top-k Interaction Refinement (TIR), a lightweight setwise module that applies query-conditioned attention over the top-k blocks and produces a bounded residual correction to block scores. TIR introduces only a small number of parameters and operates on top-k blocks, keeping query-time overhead low. Experiments on long-document reranking benchmarks (TREC DL and MLDR-zh) show that block representations substantially improve over single-vector encoders, and TIR provides consistent additional gains over strong long-document reranking baselines while maintaining practical reranking latency. For example, on TREC DL 2023, NDCG at 10 improves from 0.395 to 0.451 with the same block budget k = 65, using at most 4095 tokens. The resulting model supports interpretability by exposing which blocks drive each document's score and how refinement redistributes their contributions.</description><author>cs.IR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2501.17039v2</guid></item><item><title>[arXiv-IR 2026] SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval</title><link>https://arxiv.org/abs/2602.04451</link><description>arXiv:2602.04451v2 Announce Type: replace 
Abstract: Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.</description><author>cs.IR updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04451v2</guid></item><item><title>[arXiv-statML 2026] AdapDISCOM: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors</title><link>https://arxiv.org/abs/2508.00120</link><description>arXiv:2508.00120v2 Announce Type: replace-cross 
Abstract: Multimodal high-dimensional data are increasingly prevalent in biomedical research, yet they are often compromised by block-wise missingness and measurement errors, posing significant challenges for statistical inference and prediction. We propose AdapDISCOM, a novel adaptive direct sparse regression method that simultaneously addresses these two pervasive issues. Building on the DISCOM framework, AdapDISCOM introduces modality-specific weighting schemes to account for heterogeneity in data structures and error magnitudes across modalities. We establish the theoretical properties of AdapDISCOM, including model selection consistency and convergence rates under sub-Gaussian and heavy-tailed settings, and develop robust and computationally efficient variants (AdapDISCOM-Huber and Fast-AdapDISCOM). Extensive simulations demonstrate that AdapDISCOM consistently outperforms existing methods such as DISCOM, SCOM, and CoCoLasso, particularly under heterogeneous contamination and heavy-tailed distributions. Finally, we apply AdapDISCOM to Alzheimers Disease Neuroimaging Initiative (ADNI) data, demonstrating improved prediction of cognitive scores and reliable selection of established biomarkers, even with substantial missingness and measurement errors. AdapDISCOM provides a flexible, robust, and scalable framework for high-dimensional multimodal data analysis under realistic data imperfections.</description><author>stat.ML updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.00120v2</guid></item><item><title>[arXiv-statML 2026] Quantum Circuit Generation via test-time learning with large language models</title><link>https://arxiv.org/abs/2602.03466</link><description>arXiv:2602.03466v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can generate structured artifacts, but using them as dependable optimizers for scientific design requires a mechanism for iterative improvement under black-box evaluation. Here, we cast quantum circuit synthesis as a closed-loop, test-time optimization problem: an LLM proposes edits to a fixed-length gate list, and an external simulator evaluates the resulting state with the Meyer-Wallach (MW) global entanglement measure. We introduce a lightweight test-time learning recipe that can reuse prior high-performing candidates as an explicit memory trace, augments prompts with a score-difference feedback, and applies restart-from-the-best sampling to escape potential plateaus. Across fixed 20-qubit settings, the loop without feedback and restart-from-the-best improves random initial circuits over a range of gate budgets. To lift up this performance and success rate, we use the full learning strategy. For the 25-qubit, it mitigates a pronounced performance plateau when naive querying is used. Beyond raw scores, we analyze the structure of synthesized states and find that high MW solutions can correspond to stabilizer or graph-state-like constructions, but full connectivity is not guaranteed due to the metric property and prompt design. These results illustrate both the promise and the pitfalls of memory evaluator-guided LLM optimization for circuit synthesis, highlighting the critical role of prior human-made theoretical theorems to optimally design a custom tool in support of research.</description><author>stat.ML updates on arXiv.org</author><pubDate>Fri, 06 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03466v2</guid></item><item><title>[CSUR 2026] Bridging the Black Box: A Survey on Mechanistic Interpretability in AI</title><link>https://dl.acm.org/doi/abs/10.1145/3787104?af=R</link><description>ACM Computing Surveys, Volume 58, Issue 8, Page 1-35, June 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Computing Surveys: Table of Contents</author><pubDate>Wed, 04 Feb 2026 11:19:08 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3787104?af=R</guid></item><item><title>[ICML 2026] Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives.</title><link>https://proceedings.mlr.press/v267/meyerson25a.html</link><description></description><author>dblp: new volumes for streams/conf/icml</author><pubDate>Mon, 02 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/icml/Meyerson025</guid></item><item><title>[ICML 2026] Position: Spectral GNNs Rely Less on Graph Fourier Basis than Conceived.</title><link>https://proceedings.mlr.press/v267/guo25w.html</link><description></description><author>dblp: new volumes for streams/conf/icml</author><pubDate>Mon, 02 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/icml/GuoTMXW25</guid></item><item><title>[TSE] The Power of Small LLMs: A Multi-Agent for Code Generation via Dynamic Precaution Tuning.</title><link>https://doi.org/10.1109/TSE.2025.3632508</link><description></description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/ZhangLTZ26</guid></item><item><title>[TSE] Shield Broken: Black-Box Adversarial Attacks on LLM-Based Vulnerability Detectors.</title><link>https://doi.org/10.1109/TSE.2025.3638998</link><description></description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/JiangHTSW26</guid></item><item><title>[TSE] MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing.</title><link>https://doi.org/10.1109/TSE.2025.3619966</link><description></description><author>dblp: new issues for streams/journals/tse</author><pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tse/OuLYWWCCTP26</guid></item><item><title>[TOSEM] PonziHunter: Hunting Ethereum Ponzi Contract via Static Analysis and Contrastive Learning on the Bytecode Level</title><link>https://dl.acm.org/doi/abs/10.1145/3735971?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-21, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Wed, 21 Jan 2026 04:18:39 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3735971?af=R</guid></item><item><title>[TOSEM 2026] Abundant Modalities Offer More Nutrients: Multi-Modal-Based Function-Level Vulnerability Detection</title><link>https://dl.acm.org/doi/abs/10.1145/3731557?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-31, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Tue, 20 Jan 2026 02:02:29 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3731557?af=R</guid></item><item><title>[TSC 2026] Diffusion-Based Multi-Agent Reinforcement Learning for Semantic Vehicular Edge Computing.</title><link>https://doi.org/10.1109/TSC.2025.3618082</link><description></description><author>dblp: new issues for streams/journals/tsc</author><pubDate>Fri, 16 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tsc/YangMSHFYZ25</guid></item><item><title>[TSC 2026] Knowledge Graph-Enhanced Masked Auto-Encoders for Recommendation Systems.</title><link>https://doi.org/10.1109/TSC.2025.3632319</link><description></description><author>dblp: new issues for streams/journals/tsc</author><pubDate>Fri, 16 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tsc/LiuQWS25</guid></item><item><title>[TSC 2026] MAO: A Framework for Process Model Generation With Multi-Agent Orchestration.</title><link>https://doi.org/10.1109/TSC.2025.3611816</link><description></description><author>dblp: new issues for streams/journals/tsc</author><pubDate>Fri, 16 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tsc/LinJZCQ25</guid></item><item><title>[TSC 2026] TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent With Multi-Modality Observation Data in Cloud-Native Systems.</title><link>https://doi.org/10.1109/TSC.2025.3629066</link><description></description><author>dblp: new issues for streams/journals/tsc</author><pubDate>Fri, 16 Jan 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tsc/ZhangWLYXZY25</guid></item><item><title>[IEEE S&amp;P 2025] Agentic AI's OODA Loop Problem.</title><link>https://doi.org/10.1109/MSEC.2025.3604105</link><description></description><author>dblp: new issues for streams/journals/ieeesp</author><pubDate>Wed, 31 Dec 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ieeesp/RaghavanS25</guid></item><item><title>[TDSC 2025] Attention-Based Membership Inference Attacks on Graph Neural Network Through Topological Features.</title><link>https://doi.org/10.1109/TDSC.2025.3586251</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/GuanZTZ25</guid></item><item><title>[TDSC 2025] HgtJIT: Just-in-Time Vulnerability Detection Based on Heterogeneous Graph Transformer.</title><link>https://doi.org/10.1109/TDSC.2025.3586669</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/SunZCWBWLX25</guid></item><item><title>[TDSC 2025] Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks.</title><link>https://doi.org/10.1109/TDSC.2025.3591543</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/GuanZCRZ25</guid></item><item><title>[TDSC 2025] Tacco: A Framework for Ensuring the Security of Real-World TEEs via Formal Verification.</title><link>https://doi.org/10.1109/TDSC.2025.3594594</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/HuZPDR25</guid></item><item><title>[TDSC 2025] RMAAC: Joint Markov Games and Robust Multiagent Actor-Critic for Explainable Malware Defense in Social IoT.</title><link>https://doi.org/10.1109/TDSC.2025.3595097</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/ShenHSWDW25</guid></item><item><title>[TDSC 2025] LLMGraph: Label-Free Detection Against APTs in Edge Networks via LLM and GCN.</title><link>https://doi.org/10.1109/TDSC.2025.3596092</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/YuLWY25</guid></item><item><title>[TDSC 2025] Real-World Code Vulnerability Detection Framework: From Data Preprocessing to Multi-Feature Fusion Detection.</title><link>https://doi.org/10.1109/TDSC.2025.3601228</link><description></description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/ZhangDLLHL25</guid></item><item><title>[CCS] Demo: Reverse Engineering Android Apps with Code Coverage.</title><link>https://doi.org/10.1145/3719027.3762169</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Pilgun25</guid></item><item><title>[CCS] Poster: Insecure Coding Habits Die Hard. Can PEFT Really Turn LLMs into Secure Coders?</title><link>https://doi.org/10.1145/3719027.3760716</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/ChaeCKJOK25</guid></item><item><title>[CCS] Poster: Leveraging Large Language Models to Effectively and Efficiently Identify Vulnerability Patches for WordPress Plugins.</title><link>https://doi.org/10.1145/3719027.3760720</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/LengZZS25</guid></item><item><title>[CCS] AI-Augmented Static Analysis: Bridging Heuristics and Completeness for Practical Reverse Engineering.</title><link>https://doi.org/10.1145/3719027.3765565</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Santra25</guid></item><item><title>[CCS 2025] Autonomous Vulnerability Analysis, Triaging, and Repair: A Historical Perspective.</title><link>https://doi.org/10.1145/3719027.3748270</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Vigna25</guid></item><item><title>[CCS 2025] OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs.</title><link>https://doi.org/10.1145/3719027.3765219</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Aly0Y25</guid></item><item><title>[CCS 2025] CITesting: Systematic Testing of Context Integrity Violations in LTE Core Networks.</title><link>https://doi.org/10.1145/3719027.3765230</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/SonKOPK25</guid></item><item><title>[CCS 2025] CryptGNN: Enabling Secure Inference for Graph Neural Networks.</title><link>https://doi.org/10.1145/3719027.3765232</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/SenMB25</guid></item><item><title>[CCS 2025] SyzSpec: Specification Generation for Linux Kernel Fuzzing via Under-Constrained Symbolic Execution.</title><link>https://doi.org/10.1145/3719027.3744811</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/0006PLQS25</guid></item><item><title>[CCS 2025] Protocols to Code: Formal Verification of a Secure Next-Generation Internet Router.</title><link>https://doi.org/10.1145/3719027.3765104</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/PereiraKGLSWE0B25</guid></item><item><title>[CCS 2025] Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems.</title><link>https://doi.org/10.1145/3719027.3765210</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/KoscinskiNOFM25</guid></item><item><title>[CCS 2025] Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator.</title><link>https://doi.org/10.1145/3719027.3765193</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/HallyburtonP25</guid></item><item><title>[CCS 2025] Safeguarding Graph Neural Networks against Topology Inference Attacks.</title><link>https://doi.org/10.1145/3719027.3765173</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/00030CW25</guid></item><item><title>[CCS 2025] ZVDetector: State-Guided Vulnerability Detection System for Zigbee Devices.</title><link>https://doi.org/10.1145/3719027.3765035</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lin00WB25</guid></item><item><title>[CCS 2025] Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection.</title><link>https://doi.org/10.1145/3719027.3765027</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/BarsBSH25</guid></item><item><title>[CCS 2025] Security Analysis of Privately Verifiable Privacy Pass.</title><link>https://doi.org/10.1145/3719027.3765172</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/HanffLO25</guid></item><item><title>[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.</title><link>https://doi.org/10.1145/3719027.3765049</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/LinWQCM25</guid></item><item><title>[CCS 2025] Fuzzy Extractors are Practical: Cryptographic Strength Key Derivation from the Iris.</title><link>https://doi.org/10.1145/3719027.3765098</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/ShuklaD0AMRC25</guid></item><item><title>[CCS 2025] Augmenting Search-based Program Synthesis with Local Inference Rules to Improve Black-box Deobfuscation.</title><link>https://doi.org/10.1145/3719027.3765134</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Attias0MBM25</guid></item><item><title>[CCS 2025] Poster: LogCraft: Crafting CVE-Aware Synthetic Worlds (Logs).</title><link>https://doi.org/10.1145/3719027.3760736</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/WongTHGJWC25</guid></item><item><title>[CCS 2025] Scalable Cryptography for Trustworthy Machine Learning in the LLM Era.</title><link>https://doi.org/10.1145/3719027.3765571</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Tan25</guid></item><item><title>[CCS 2025] SCORED '25: Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses.</title><link>https://doi.org/10.1145/3719027.3767662</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/YelgundhalliHRD25</guid></item><item><title>[CCS 2025] LAMPS '25: ACM CCS Workshop on Large AI Systems and Models with Privacy and Security Analysis.</title><link>https://doi.org/10.1145/3719027.3767670</link><description></description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lam0W0XC0YBW25</guid></item><item><title>[USENIXSec] LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lekssays</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LekssaysMT0K25</guid></item><item><title>[USENIXSec] We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/spracklen</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/SpracklenWSMV25</guid></item><item><title>[USENIXSec] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM Analysis.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kim-youngjoon</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Kim0KY25</guid></item><item><title>[USENIXSec] APPATCH: Automated Adaptive Prompting Large Language Models for Real-World Software Vulnerability Patching.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/nong</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/NongY0HC25</guid></item><item><title>[USENIXSec] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for Static Analysis Result Verification.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/bao-andrew</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/BaoZWCMY25</guid></item><item><title>[USENIXSec 2025] Game of Arrows: On the (In-)Security of Weight Obfuscation for On-Device TEE-Shielded LLM Partition Algorithms.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-pengli</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangDCZLXWZZ25</guid></item><item><title>[USENIXSec 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/krauss</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KraussDD25</guid></item><item><title>[USENIXSec 2025] Confusing Value with Enumeration: Studying the Use of CVEs in Academia.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/schloegel</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/SchloegelK000WT25</guid></item><item><title>[USENIXSec 2025] A Thorough Security Analysis of BLE Proximity Tracking Protocols.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/liu-xiaofeng</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0013ZHR0ZG25</guid></item><item><title>[USENIXSec 2025] CoreCrisis: Threat-Guided and Context-Aware Iterative Learning and Fuzzing of 5G Core Networks.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/dong-yilu</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/DongYIRRTWMH25</guid></item><item><title>[USENIXSec 2025] SCASE: Automated Secret Recovery via Side-Channel-Assisted Symbolic Execution.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/weber</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00070TLB025</guid></item><item><title>[USENIXSec 2025] Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kwesi</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/KwesiCMN25</guid></item><item><title>[USENIXSec 2025] Hybrid Language Processor Fuzzing via LLM-Based Constraint Solving.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/yang-yupeng</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/YangYCL25</guid></item><item><title>[USENIXSec 2025] A Comprehensive Formal Security Analysis of OPC UA.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/diemunsch</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/DiemunschHK25</guid></item><item><title>[USENIXSec 2025] AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/li-jiate</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LiW25</guid></item><item><title>[USENIXSec 2025] Distributed Private Aggregation in Graph Neural Networks.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/jia-huanhuan</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/JiaZ000LF25</guid></item><item><title>[USENIXSec 2025] Cloak, Honey, Trap: Proactive Defenses Against LLM Agents.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/ayzenshteyn</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/AyzenshteynWM25</guid></item><item><title>[USENIXSec 2025] ZIPPER: Static Taint Analysis for PHP Applications with Precision and Efficiency.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-xinyi</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangLLCSMZ00H25</guid></item><item><title>[USENIXSec 2025] Effective Directed Fuzzing with Hierarchical Scheduling for Web Vulnerability Detection.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lin-zihan</link><description></description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Lin0DHX0Y0C025</guid></item><item><title>[ASE] GPTVD: vulnerability detection and analysis method based on LLM's chain of thoughts.</title><link>https://doi.org/10.1007/s10515-025-00550-4</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/ChenHCSY26</guid></item><item><title>[ASE] BRMDS: an LLM-based multi-dimensional summary generation approach for bug reports.</title><link>https://doi.org/10.1007/s10515-025-00553-1</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/ZhangLFYD26</guid></item><item><title>[ASE] ByteEye: A smart contract vulnerability detection framework at bytecode level with graph neural networks.</title><link>https://doi.org/10.1007/s10515-025-00559-9</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YangLDFXL26</guid></item><item><title>[ASE] SPVR: syntax-to-prompt vulnerability repair based on large language models.</title><link>https://doi.org/10.1007/s10515-025-00579-5</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/WangLGWXW26</guid></item><item><title>[ASE 2025] HMF: Enhancing reentrancy vulnerability detection and repair with a hybrid model framework.</title><link>https://doi.org/10.1007/s10515-025-00546-0</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/LiSRFLS26</guid></item><item><title>[ASE 2025] Toward efficient testing of graph neural networks via test input prioritization.</title><link>https://doi.org/10.1007/s10515-025-00554-0</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YangWYHL26</guid></item><item><title>[ASE 2025] Graph neural networks for precise bug localization through structural program analysis.</title><link>https://doi.org/10.1007/s10515-025-00556-y</link><description></description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YousofvandSRN26</guid></item><item><title>[SOSP] KNighter: Transforming Static Analysis with LLM-Synthesized Checkers.</title><link>https://doi.org/10.1145/3731569.3764827</link><description></description><author>dblp: new volumes for streams/conf/sosp</author><pubDate>Wed, 01 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sosp/YangZXLZ25</guid></item><item><title>[ACL 2025] Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics.</title><link>https://doi.org/10.18653/v1/2025.acl-short.15</link><description></description><author>dblp: new volumes for streams/conf/acl</author><pubDate>Wed, 24 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acl/FloresEC25</guid></item><item><title>[ACL 2025] CoRet: Improved Retriever for Code Editing.</title><link>https://doi.org/10.18653/v1/2025.acl-short.62</link><description></description><author>dblp: new volumes for streams/conf/acl</author><pubDate>Wed, 24 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acl/FehrS0Z25</guid></item><item><title>[ACL 2025] Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation.</title><link>https://doi.org/10.18653/v1/2025.acl-short.93</link><description></description><author>dblp: new volumes for streams/conf/acl</author><pubDate>Wed, 24 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acl/QinZSWXRHTTWJF025</guid></item><item><title>[IJCAI] CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models.</title><link>https://doi.org/10.24963/ijcai.2025/34</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhanLGZ0SLH25</guid></item><item><title>[IJCAI] POLO: An LLM-Powered Project-Level Code Performance Optimization Framework.</title><link>https://doi.org/10.24963/ijcai.2025/814</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/BaiXWY0025</guid></item><item><title>[IJCAI] Can We Translate Code Better with LLMs and Call Graph Analysis?</title><link>https://doi.org/10.24963/ijcai.2025/848</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Luo25</guid></item><item><title>[IJCAI] SecV: LLM-based Secure Verilog Generation with Clue-Guided Exploration on Hardware-CWE Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/895</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/FanXK25</guid></item><item><title>[IJCAI] How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging (Extended Abstract).</title><link>https://doi.org/10.24963/ijcai.2025/1217</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Ma0KW25</guid></item><item><title>[IJCAI 2025] AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.</title><link>https://doi.org/10.24963/ijcai.2025/2</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/AnokhinSSEK0B25</guid></item><item><title>[IJCAI 2025] Requirement Patterns for Engineering Multiagent Interaction Protocols.</title><link>https://doi.org/10.24963/ijcai.2025/5</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ChopraVS25</guid></item><item><title>[IJCAI 2025] Multi-Agent Communication with Information Preserving Graph Contrastive Learning.</title><link>https://doi.org/10.24963/ijcai.2025/8</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0010D0SYC25</guid></item><item><title>[IJCAI 2025] L2M2: A Hierarchical Framework Integrating Large Language Model and Multi-agent Reinforcement Learning.</title><link>https://doi.org/10.24963/ijcai.2025/12</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/GengPSLZT25</guid></item><item><title>[IJCAI 2025] MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning.</title><link>https://doi.org/10.24963/ijcai.2025/14</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/GuoXW025</guid></item><item><title>[IJCAI 2025] Steady-State Strategy Synthesis for Swarms of Autonomous Agents.</title><link>https://doi.org/10.24963/ijcai.2025/16</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Jonas0KM25</guid></item><item><title>[IJCAI 2025] RoLocMe: A Robust Multi-agent Source Localization System with Learning-based Map Estimation.</title><link>https://doi.org/10.24963/ijcai.2025/18</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LeYH25</guid></item><item><title>[IJCAI 2025] Asynchronous Credit Assignment for Multi-Agent Reinforcement Learning.</title><link>https://doi.org/10.24963/ijcai.2025/20</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LiangWWC25</guid></item><item><title>[IJCAI 2025] Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity.</title><link>https://doi.org/10.24963/ijcai.2025/24</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/MaiYNXH25</guid></item><item><title>[IJCAI 2025] Multi-Agent Corridor Generating Algorithm.</title><link>https://doi.org/10.24963/ijcai.2025/28</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/PertzovskiySZF25</guid></item><item><title>[IJCAI 2025] Synthesis of Communication Policies for Multi-Agent Systems Robust to Communication Restrictions.</title><link>https://doi.org/10.24963/ijcai.2025/30</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/SoudijaniD25</guid></item><item><title>[IJCAI 2025] Streaming Multi-agent Pathfinding.</title><link>https://doi.org/10.24963/ijcai.2025/31</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/00020Z25</guid></item><item><title>[IJCAI 2025] FADE: Towards Fairness-aware Data Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models.</title><link>https://doi.org/10.24963/ijcai.2025/50</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Lin00W025</guid></item><item><title>[IJCAI 2025] fairGNN-WOD: Fair Graph Learning Without Complete Demographics.</title><link>https://doi.org/10.24963/ijcai.2025/63</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WangLPLSQZ25</guid></item><item><title>[IJCAI 2025] Explainable Graph Neural Networks via Structural Externalities.</title><link>https://doi.org/10.24963/ijcai.2025/66</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WuHF25</guid></item><item><title>[IJCAI 2025] Causality-Inspired Disentanglement for Fair Graph Neural Networks.</title><link>https://doi.org/10.24963/ijcai.2025/72</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhangCY0Z25</guid></item><item><title>[IJCAI 2025] An Efficient Core-Guided Solver for Weighted Partial MaxSAT.</title><link>https://doi.org/10.24963/ijcai.2025/295</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/PanW025</guid></item><item><title>[IJCAI 2025] Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming.</title><link>https://doi.org/10.24963/ijcai.2025/314</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Gong0Y0DCF25</guid></item><item><title>[IJCAI 2025] NeuBM: Mitigating Model Bias in Graph Neural Networks Through Neutral Input Calibration.</title><link>https://doi.org/10.24963/ijcai.2025/315</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/GuQ025</guid></item><item><title>[IJCAI 2025] NAAST-GNN: Neighborhood Adaptive Aggregation and Spectral Tuning for Graph Anomaly Detection.</title><link>https://doi.org/10.24963/ijcai.2025/317</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/GuoZYZZ025</guid></item><item><title>[IJCAI 2025] MaskDGNN: Self-Supervised Dynamic Graph Neural Networks with Activeness-aware Temporal Masking.</title><link>https://doi.org/10.24963/ijcai.2025/322</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/He000HY25</guid></item><item><title>[IJCAI 2025] Heterogeneous Temporal Hypergraph Neural Network.</title><link>https://doi.org/10.24963/ijcai.2025/347</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LiuJ0C025</guid></item><item><title>[IJCAI 2025] DGExplainer: Explaining Dynamic Graph Neural Networks via Relevance Back-propagation.</title><link>https://doi.org/10.24963/ijcai.2025/349</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LiuXS25</guid></item><item><title>[IJCAI 2025] Dynamic Seed-GrowthCM: A Dynamic Benefit-Oriented Algorithm for Core Maximization on Large Graphs.</title><link>https://doi.org/10.24963/ijcai.2025/352</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/MaH025</guid></item><item><title>[IJCAI 2025] Exploring the Over-smoothing Problem of Graph Neural Networks for Graph Classification: An Entropy-based Viewpoint.</title><link>https://doi.org/10.24963/ijcai.2025/360</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Qian0CLD0H25</guid></item><item><title>[IJCAI 2025] EVICheck: Evidence-Driven Independent Reasoning and Combined Verification Method for Fact-Checking.</title><link>https://doi.org/10.24963/ijcai.2025/376</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WangSKZMZXL25</guid></item><item><title>[IJCAI 2025] The Core of Approval-Based Committee Elections with Few Seats.</title><link>https://doi.org/10.24963/ijcai.2025/447</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Peters25</guid></item><item><title>[IJCAI 2025] Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions.</title><link>https://doi.org/10.24963/ijcai.2025/457</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Aroca-Ouellette25</guid></item><item><title>[IJCAI 2025] Relational Decomposition for Program Synthesis.</title><link>https://doi.org/10.24963/ijcai.2025/504</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HocquetteC25</guid></item><item><title>[IJCAI 2025] Verifying Quantized Graph Neural Networks is PSPACE-complete.</title><link>https://doi.org/10.24963/ijcai.2025/519</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/SalzerST25</guid></item><item><title>[IJCAI 2025] LLM4VKG: Leveraging Large Language Models for Virtual Knowledge Graph Construction.</title><link>https://doi.org/10.24963/ijcai.2025/525</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0003RQXPL25</guid></item><item><title>[IJCAI 2025] FGeo-HyperGNet: Geometric Problem Solving Integrating FormalGeo Symbolic System and Hypergraph Neural Network.</title><link>https://doi.org/10.24963/ijcai.2025/527</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhangLZQZL25</guid></item><item><title>[IJCAI 2025] Grounding Methods for Neural-Symbolic AI.</title><link>https://doi.org/10.24963/ijcai.2025/535</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/OntiverosGGMD25</guid></item><item><title>[IJCAI 2025] Variational Offline Multi-agent Skill Discovery.</title><link>https://doi.org/10.24963/ijcai.2025/538</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Chen0A25</guid></item><item><title>[IJCAI 2025] ADPFedGNN: Adaptive Decoupling Personalized Federated Graph Neural Network.</title><link>https://doi.org/10.24963/ijcai.2025/585</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Guan00TM25</guid></item><item><title>[IJCAI 2025] Going Beyond Consistency: Target-oriented Multi-view Graph Neural Network.</title><link>https://doi.org/10.24963/ijcai.2025/604</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HuangFZQH0025</guid></item><item><title>[IJCAI 2025] CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting.</title><link>https://doi.org/10.24963/ijcai.2025/619</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LeeYK25</guid></item><item><title>[IJCAI 2025] Leveraging Peer-Informed Label Consistency for Robust Graph Neural Networks with Noisy Labels.</title><link>https://doi.org/10.24963/ijcai.2025/623</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0002SLFZWXZ025</guid></item><item><title>[IJCAI 2025] A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge.</title><link>https://doi.org/10.24963/ijcai.2025/649</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Lorello0M25</guid></item><item><title>[IJCAI 2025] Mitigating Over-Smoothing in Graph Neural Networks via Separation Coefficient-Guided Adaptive Graph Structure Adjustment.</title><link>https://doi.org/10.24963/ijcai.2025/663</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/MengYP25</guid></item><item><title>[IJCAI 2025] SALE-MLP: Structure Aware Latent Embeddings for GNN to Graph-free MLP Distillation.</title><link>https://doi.org/10.24963/ijcai.2025/668</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/PalMPM25</guid></item><item><title>[IJCAI 2025] Breaking the Self-Evaluation Barrier: Reinforced Neuro-Symbolic Planning with Large Language Models.</title><link>https://doi.org/10.24963/ijcai.2025/682</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ShaoYCDDG25</guid></item><item><title>[IJCAI 2025] Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks.</title><link>https://doi.org/10.24963/ijcai.2025/724</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WangW0F025</guid></item><item><title>[IJCAI 2025] Wave-driven Graph Neural Networks with Energy Dynamics for Over-smoothing Mitigation.</title><link>https://doi.org/10.24963/ijcai.2025/732</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WuQHALZ25</guid></item><item><title>[IJCAI 2025] From General Relation Patterns to Task-Specific Decision-Making in Continual Multi-Agent Coordination.</title><link>https://doi.org/10.24963/ijcai.2025/759</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/YaoLS0M0025</guid></item><item><title>[IJCAI 2025] A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting.</title><link>https://doi.org/10.24963/ijcai.2025/796</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Zheng025</guid></item><item><title>[IJCAI 2025] Underground Diagnosis in 3D GPR Data by Learning in CuCoRes Model Space.</title><link>https://doi.org/10.24963/ijcai.2025/801</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhouLYWC25</guid></item><item><title>[IJCAI 2025] CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation.</title><link>https://doi.org/10.24963/ijcai.2025/825</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HuLCY25</guid></item><item><title>[IJCAI 2025] APIMig: A Project-Level Cross-Multi-Version API Migration Framework Based on Evolution Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/829</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/KuangXYYWKX25</guid></item><item><title>[IJCAI 2025] Backdoor Attack on Vertical Federated Graph Neural Network Learning.</title><link>https://doi.org/10.24963/ijcai.2025/877</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/YangCLZDDD25</guid></item><item><title>[IJCAI 2025] Sanitizing Backdoored Graph Neural Networks: A Multidimensional Approach.</title><link>https://doi.org/10.24963/ijcai.2025/884</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhaoZWZ025</guid></item><item><title>[IJCAI 2025] Learn to Think: Bootstrapping LLM Logic Through Graph Representation Learning.</title><link>https://doi.org/10.24963/ijcai.2025/896</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0004ZWZWZ025</guid></item><item><title>[IJCAI 2025] How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback.</title><link>https://doi.org/10.24963/ijcai.2025/901</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HuangB0025</guid></item><item><title>[IJCAI 2025] Improving Consistency Identification in Task-oriented Dialogue Through Multi-Agent Collaboration.</title><link>https://doi.org/10.24963/ijcai.2025/918</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WangLZC00LC025</guid></item><item><title>[IJCAI 2025] EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness.</title><link>https://doi.org/10.24963/ijcai.2025/932</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhangXLZ25</guid></item><item><title>[IJCAI 2025] SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation.</title><link>https://doi.org/10.24963/ijcai.2025/965</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/XuLLG25</guid></item><item><title>[IJCAI 2025] LLM-enhanced Score Function Evolution for Causal Structure Learning.</title><link>https://doi.org/10.24963/ijcai.2025/1010</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WangLF0025</guid></item><item><title>[IJCAI 2025] The Graph's Apprentice: Teaching an LLM Low-Level Knowledge for Circuit Quality Estimation.</title><link>https://doi.org/10.24963/ijcai.2025/1033</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/MoravejBZCT0ZHY25</guid></item><item><title>[IJCAI 2025] Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks.</title><link>https://doi.org/10.24963/ijcai.2025/1051</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/ZhangWLDWL25</guid></item><item><title>[IJCAI 2025] An Interactive Game-based Multi-Agent AI System for Children's Social and Emotional Development.</title><link>https://doi.org/10.24963/ijcai.2025/1062</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/BanerjeeSPBASSH25</guid></item><item><title>[IJCAI 2025] Exploring Equity of Climate Policies Using Multi-Agent Multi-Objective Reinforcement Learning.</title><link>https://doi.org/10.24963/ijcai.2025/1064</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/BiswasOTWSKOM25</guid></item><item><title>[IJCAI 2025] Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs.</title><link>https://doi.org/10.24963/ijcai.2025/1073</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/GeWCL025</guid></item><item><title>[IJCAI 2025] Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration.</title><link>https://doi.org/10.24963/ijcai.2025/1080</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/0007YXT025</guid></item><item><title>[IJCAI 2025] ContextAware: A Multi-Agent Framework for Detecting Harmful Image-Based Comments on Social Media.</title><link>https://doi.org/10.24963/ijcai.2025/1103</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WeiLZLQ025</guid></item><item><title>[IJCAI 2025] LLM-based Collaborative Agents with Pedagogy-guided Interaction Modeling for Timely Instructive Feedback Generation in Task-oriented Group Discussions.</title><link>https://doi.org/10.24963/ijcai.2025/1108</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/Yang0AHX25</guid></item><item><title>[IJCAI 2025] Explainable Automatic Fact-Checking for Journalists Augmentation in the Wild.</title><link>https://doi.org/10.24963/ijcai.2025/1140</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/AltoePP25</guid></item><item><title>[IJCAI 2025] Neuro-Symbolic Artificial Intelligence: A Task-Directed Survey in the Black-Box Models Era.</title><link>https://doi.org/10.24963/ijcai.2025/1157</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/DelvecchioMM25</guid></item><item><title>[IJCAI 2025] Graph Neural Networks for Databases: A Survey.</title><link>https://doi.org/10.24963/ijcai.2025/1172</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/LiLLLZ25</guid></item><item><title>[IJCAI 2025] Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review.</title><link>https://doi.org/10.24963/ijcai.2025/1190</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WuW00025</guid></item><item><title>[IJCAI 2025] Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models.</title><link>https://doi.org/10.24963/ijcai.2025/1195</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/YangSGZZJDL25</guid></item><item><title>[IJCAI 2025] Efficient Rectification of Neuro-Symbolic Reasoning Inconsistencies by Abductive Reflection (Extended Abstract).</title><link>https://doi.org/10.24963/ijcai.2025/1212</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/HuD0Z25</guid></item><item><title>[IJCAI 2025] Scalable Primal Heuristics Using Graph Neural Networks for Combinatorial Optimization (Abstract Reprint).</title><link>https://doi.org/10.24963/ijcai.2025/1223</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/CanturkVAO25</guid></item><item><title>[IJCAI 2025] Veracity: An Open-Source AI Fact-Checking System.</title><link>https://doi.org/10.24963/ijcai.2025/1254</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/CurtisTGGPWKCGR25</guid></item><item><title>[IJCAI 2025] GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs.</title><link>https://doi.org/10.24963/ijcai.2025/1256</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/DaSLZ025</guid></item><item><title>[IJCAI 2025] Search Swarm: Multiagent Large Language Models Framework for E-commerce Product Search.</title><link>https://doi.org/10.24963/ijcai.2025/1263</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/IsyanbaevM25</guid></item><item><title>[IJCAI 2025] SAFE: Structured Argumentation for Fact-checking with Explanations.</title><link>https://doi.org/10.24963/ijcai.2025/1274</link><description></description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/WangCV25</guid></item><item><title>[EuroS&amp;P] Mitigating Information Leakage in Large Language Models: Evaluating the Impact of Code Obfuscation on Vulnerability Detection.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00007</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/GulayY25</guid></item><item><title>[EuroS&amp;P 2025] Admin/1423: On the Insecurity of Open-Source 5G Core Network Deployments in the Wild.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00025</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/Kohls25</guid></item><item><title>[EuroS&amp;P 2025] Hunting Review Bombs with BRIDGE - A Bipartite Graph Neural Network Approach to Abuse Detection.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00022</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/AbufonVBC25</guid></item><item><title>[EuroS&amp;P 2025] PPT-GNN: A Practical Pretrained Spatio-Temporal Graph Neural Network for Network Security.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00026</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/LangendonckCB25</guid></item><item><title>[EuroS&amp;P 2025] One Does Not Simply Score a Website: Evaluating Website Security Scoring Algorithms.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00028</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/VansteenhuysePFD25</guid></item><item><title>[EuroS&amp;P 2025] CFA-Bench: Cybersecurity Forensic Llm Agent Benchmark and Testing.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00031</link><description></description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/SantisHVGMBR25</guid></item><item><title>[S&amp;P] Code Vulnerability Repair with Large Language Model Using Context-Aware Prompt Tuning.</title><link>https://doi.org/10.1109/SPW67851.2025.00040</link><description></description><author>dblp: new volumes for streams/conf/sp</author><pubDate>Sun, 20 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sp/KhanLG25</guid></item><item><title>[S&amp;P 2025] Ignoring Directionality Leads to Compromised Graph Neural Network Explanations.</title><link>https://doi.org/10.1109/SPW67851.2025.00032</link><description></description><author>dblp: new volumes for streams/conf/sp</author><pubDate>Sun, 20 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sp/SunLD25</guid></item><item><title>[OSDI] Stripeless Data Placement for Erasure-Coded In-Memory Storage.</title><link>https://www.usenix.org/conference/osdi25/presentation/gao</link><description></description><author>dblp: new volumes for streams/conf/osdi</author><pubDate>Wed, 16 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/osdi/GaoSY0H25</guid></item><item><title>[OSDI] Paralegal: Practical Static Analysis for Privacy Bugs.</title><link>https://www.usenix.org/conference/osdi25/presentation/adam</link><description></description><author>dblp: new volumes for streams/conf/osdi</author><pubDate>Wed, 16 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/osdi/AdamZZRHJCKS25</guid></item><item><title>[ISSTA] Revisiting the Combination of Static Analysis Error Traces and Dynamic Symbolic Execution: A Potential Approach for True Positive Confirmation (Registered Report).</title><link>https://doi.org/10.1145/3713081.3731720</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/Xu0P25</guid></item><item><title>[ISSTA] A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection.</title><link>https://doi.org/10.1145/3713081.3731746</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/YuSF0TK025</guid></item><item><title>[ISSTA] Patch the Leak: Strengthening CodeLLMs Against Privacy Extraction Threats.</title><link>https://doi.org/10.1145/3713081.3732931</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/GuoMXWD025</guid></item><item><title>[ISSTA 2025] TBFV4J: An Automated Testing-Based Formal Verification Tool for Java.</title><link>https://doi.org/10.1145/3713081.3731740</link><description></description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/LiuLL25</guid></item><item><title>[ACSAC] Software Vulnerability Detection Using LLM: Does Additional Information Help?</title><link>https://doi.org/10.1109/ACSACW65225.2024.00031</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/ShimmiSSOR24</guid></item><item><title>[ACSAC 2025] Learning to Unfix: Towards ML Robustness in Vulnerability Detection via Structure-Aware Code Generation.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00014</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/Rozi024</guid></item><item><title>[ACSAC 2025] AdVul: Adversarial Attack against ML-based Vulnerability Detection.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00018</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/KatohPX24</guid></item><item><title>[ACSAC 2025] Automated Vulnerability Detection in Smart Contracts using Control Flow Graphs and Machine Learning.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00037</link><description></description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/LohestBL24</guid></item><item><title>[NDSS] A Comprehensive Memory Safety Analysis of Bootloaders.</title><link>https://www.ndss-symposium.org/ndss-paper/a-comprehensive-memory-safety-analysis-of-bootloaders/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/WangWWLS0H25</guid></item><item><title>[NDSS] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/from-large-to-mammoth-a-comparative-evaluation-of-large-language-models-in-vulnerability-detection/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LinM25</guid></item><item><title>[NDSS] PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation.</title><link>https://www.ndss-symposium.org/ndss-paper/propertygpt-llm-driven-formal-verification-of-smart-contracts-through-retrieval-augmented-property-generation/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/0012XW00S025</guid></item><item><title>[NDSS 2025] Generating API Parameter Security Rules with LLM for API Misuse Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/generating-api-parameter-security-rules-with-llm-for-api-misuse-detection/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LiuY0L25</guid></item><item><title>[NDSS 2025] DShield: Defending against Backdoor Attacks on Graph Neural Networks via Discrepancy Learning.</title><link>https://www.ndss-symposium.org/ndss-paper/dshield-defending-against-backdoor-attacks-on-graph-neural-networks-via-discrepancy-learning/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/Yu0W00S025</guid></item><item><title>[NDSS 2025] EMIRIS: Eavesdropping on Iris Information via Electromagnetic Side Channel.</title><link>https://www.ndss-symposium.org/ndss-paper/emiris-eavesdropping-on-iris-information-via-electromagnetic-side-channel/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LiWZYS0025</guid></item><item><title>[NDSS 2025] Iris: Dynamic Privacy Preserving Search in Authenticated Chord Peer-to-Peer Networks.</title><link>https://www.ndss-symposium.org/ndss-paper/iris-dynamic-privacy-preserving-search-in-authenticated-chord-peer-to-peer-networks/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/AktypiR25</guid></item><item><title>[NDSS 2025] You Can Rand but You Can't Hide: A Holistic Security Analysis of Google Fuchsia's (and gVisor's) Network Stack.</title><link>https://www.ndss-symposium.org/ndss-paper/you-can-rand-but-you-cant-hide-a-holistic-security-analysis-of-google-fuchsias-and-gvisors-network-stack/</link><description></description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/KaplanE025</guid></item><item><title>[NeurIPS 2025] Fair Wasserstein Coresets.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/005413e90d003d13886019607b037f52-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/XiongDSLMPBV24</guid></item><item><title>[NeurIPS 2025] AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0142921fad7ef9192bd87229cdafa9d4-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0001LYYL024</guid></item><item><title>[NeurIPS 2025] GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/023d2c1a17cf35b11a0cbb43a0677c91-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhangPGLZWXLHWZ24</guid></item><item><title>[NeurIPS 2025] Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0520537ba799d375b8ff5523295c337a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0001XJZYSZHS24</guid></item><item><title>[NeurIPS 2025] Autonomous Agents for Collaborative Task under Information Asymmetry.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0534abc9e6db91683d82186ef0d68202-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiuWWXQDDCYQ24</guid></item><item><title>[NeurIPS 2025] MADiff: Offline Multi-agent Learning with Diffusion Models.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/07e278a120830b10aae20cc600a8c07b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhuLMKX0E024</guid></item><item><title>[NeurIPS 2025] Revisiting Score Propagation in Graph Out-of-Distribution Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MaSDL024</guid></item><item><title>[NeurIPS 2025] Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0852b88e96d973bd4e21b673f51621d0-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhouTQYJX0024</guid></item><item><title>[NeurIPS 2025] AGILE: A Novel Reinforcement Learning Framework of LLM Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/097c514162ea7126d40671d23e12f51b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/FengHHLZZL24</guid></item><item><title>[NeurIPS 2025] SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/YinXWZL24</guid></item><item><title>[NeurIPS 2025] Can Graph Learning Improve Planning in LLM-based Agents?</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/098d1bd3eb6156a4c2f834563cdcf617-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WuSSSWZFCCXL24</guid></item><item><title>[NeurIPS 2025] Graph Neural Networks and Arithmetic Circuits.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/09bf6a87e80d099cf17c6347301c6120-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BarlagHSVV24</guid></item><item><title>[NeurIPS 2025] Schur Nets: exploiting local structure for equivariance in higher order graph neural networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0a0e2c6a487314f821346bdc04869e36-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhangXK24</guid></item><item><title>[NeurIPS 2025] LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0b77d3a82b59e9d9899370b378087faf-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangZL024</guid></item><item><title>[NeurIPS 2025] Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0f94c552e5fe82bc152494985e34bd48-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SubramonianKS24</guid></item><item><title>[NeurIPS 2025] Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/0fe6a18be9491139fb759e2f645374b1-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Feng024</guid></item><item><title>[NeurIPS 2025] Online Relational Inference for Evolving Multi-agent Interacting Systems.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/12d7ba753894ed348904df1bf0ce02ec-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/KangSSCM24</guid></item><item><title>[NeurIPS 2025] Score-based 3D molecule generation with neural fields.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1486bbd863ba396398ff4227c5b3dccd-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/KirchmeyerPS24</guid></item><item><title>[NeurIPS 2025] What do Graph Neural Networks learn? Insights from Tropical Geometry.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/14d2d59c5e2302bfa6ce6cae59156e33-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Pham024</guid></item><item><title>[NeurIPS 2025] Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1588dc2b2ef339d6e4c47d212e36f991-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LeeK24</guid></item><item><title>[NeurIPS 2025] Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/16986b69068fbe6acf64eb6566519c74-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiLSYLL24</guid></item><item><title>[NeurIPS 2025] Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1c2b1c8f7d317719a9ce32dd7386ba35-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MaHPLA0C24</guid></item><item><title>[NeurIPS 2025] HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1c9c85bae6161d52182d0fe2f3640512-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BarkeGKBP24</guid></item><item><title>[NeurIPS 2025] Cryptographic Hardness of Score Estimation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1d774c112926348c3e25ea47d87c835b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Song24</guid></item><item><title>[NeurIPS 2025] The Map Equation Goes Neural: Mapping Network Flows with Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/1f59562caae05e6aae0ffd1145bea5da-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BlockerTS24</guid></item><item><title>[NeurIPS 2025] On the Scalability of GNNs for Molecular Graphs.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2345275663a15ee92a06bc957be54a2c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SypetkowskiWPDS24</guid></item><item><title>[NeurIPS 2025] Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/274d0146144643ee2459a602123c60ff-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiPZ24</guid></item><item><title>[NeurIPS 2025] Learning the Expected Core of Strictly Convex Stochastic Cooperative Games.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2844b1bec82dd4be201f68715c03ed1b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/TranTSM0T24</guid></item><item><title>[NeurIPS 2025] Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/29219efdb96e8164c589b4a0124451b7-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Miani0H24</guid></item><item><title>[NeurIPS 2025] Neuro-Symbolic Data Generation for Math Reasoning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/29d319f7c1513c9ecd81d3a6e9632a6e-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiZYZLCYM24</guid></item><item><title>[NeurIPS 2025] Is Score Matching Suitable for Estimating Point Processes?</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2a7157c84dcf263f77b37d6c11d7d149-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/CaoMKZ24</guid></item><item><title>[NeurIPS 2025] AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WuZHHYCISLZ24</guid></item><item><title>[NeurIPS 2025] Score Distillation via Reparametrized DDIM.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2ded44d59f5094eed0d02132fe75b60d-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LukoianovBGGBSS24</guid></item><item><title>[NeurIPS 2025] Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2e4531a45b99f61947b23ccdd608303b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/CenLLRW024</guid></item><item><title>[NeurIPS 2025] Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/2fbeed1dd7162f91804e7b9246e0c1a8-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BuiMN24</guid></item><item><title>[NeurIPS 2025] Multi-Agent Imitation Learning: Value is Easy, Regret is Hard.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3103b25853719847502559bf67eb4037-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/TangS0W24</guid></item><item><title>[NeurIPS 2025] Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/31888563b194f9bb33ce1aebc7e1551c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/HuLLPH24</guid></item><item><title>[NeurIPS 2025] Using Time-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3514dbacaebf0f38b25adfe59ed81a8a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/HeegS24</guid></item><item><title>[NeurIPS 2025] Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3838bf9070b80e888d571ec126d844c2-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Lyu0XZ24</guid></item><item><title>[NeurIPS 2025] Non-convolutional graph neural networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/397271e11322fae8ba7f827c50ca8d9b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangC24</guid></item><item><title>[NeurIPS 2025] Rethinking Score Distillation as a Bridge Between Image Distributions.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3b62bca132cf5c8973b09a2fc6dc8ca6-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/McAllisterG00EH24</guid></item><item><title>[NeurIPS 2025] Spiking Graph Neural Network on Riemannian Manifolds.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3ba7560b4c3e66d760fbdd472cf4a5a9-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SunHW0Y24</guid></item><item><title>[NeurIPS 2025] Energy-based Epistemic Uncertainty for Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3cd50f2922b7adaaa9e5113e35bae095-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/FuchsgruberWG24</guid></item><item><title>[NeurIPS 2025] AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3e4d8407cb468850f2f8f4a949e64bf0-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/HuangWLKQ0WZBQF24</guid></item><item><title>[NeurIPS 2025] The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3eec5006051d9544e717067de3220198-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/QinMDLMLWS24</guid></item><item><title>[NeurIPS 2025] Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/3fa2d2b637122007845a2fbb7c21453b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0001C24</guid></item><item><title>[NeurIPS 2025] NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/436ffa18e7e17be336fd884f8ebb5748-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangSZWFHB24</guid></item><item><title>[NeurIPS 2025] On the Power of Small-size Graph Neural Networks for Linear Programming.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/442553041ab2d860c013b870d5e8f167-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiDYOS024</guid></item><item><title>[NeurIPS 2025] Maximizing utility in multi-agent environments by anticipating the behavior of other learners.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/4476dd7320e0eba63961990d73525064-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/AssosDD24</guid></item><item><title>[NeurIPS 2025] GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/45326c2df19fee16fc1ebc44941fea8e-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiCH24</guid></item><item><title>[NeurIPS 2025] Graphcode: Learning from multiparameter persistent homology using graph neural networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/4822d9adc9cec7a39e254d007aa78276-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/RussoldK24</guid></item><item><title>[NeurIPS 2025] Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/4830a9b95a2f63fc4b3fe09abc18f045-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/McClellanHWHT24</guid></item><item><title>[NeurIPS 2025] Verified Code Transpilation with LLMs.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/48bb60a0c0aebb4142bf314bd1a5c6a0-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BhatiaQHSC24</guid></item><item><title>[NeurIPS 2025] Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/492592890311679d7f71559148358973-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/OskarssonLDL24</guid></item><item><title>[NeurIPS 2025] Aligning Individual and Collective Objectives in Multi-Agent Cooperation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/4ee3ac2cd119023c79b0d21c4a464dc7-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiZWZ00P24</guid></item><item><title>[NeurIPS 2025] Grounded Answers for Multi-agent Decision-making Problem through Generative World Model.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/52c21a32429a7d6050430b606a286a75-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiuYSQ0CL24</guid></item><item><title>[NeurIPS 2025] ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/54a7139c548c88e288aa0fcd2bcbeceb-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangZZDC0024</guid></item><item><title>[NeurIPS 2025] Graph Neural Networks Do Not Always Oversmooth.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/5623c35f3ab5e2c72aeb3abce27dc28f-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/EppingRHS24</guid></item><item><title>[NeurIPS 2025] Similarity-Navigated Conformal Prediction for Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/571c7e164fb1ffbcf2f84a63784451ec-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SongHJZLW24</guid></item><item><title>[NeurIPS 2025] Spatio-Spectral Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/580c4ec4738ff61d5862a122cdf139b6-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/GeislerKHG24</guid></item><item><title>[NeurIPS 2025] JaxMARL: Multi-Agent RL Environments and Algorithms in JAX.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/5aee125f052c90e326dcf6f380df94f6-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/RutherfordEG0LI24</guid></item><item><title>[NeurIPS 2025] MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/5d1f02132ef51602adf07000ca5b6138-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0003ZWZ0C24</guid></item><item><title>[NeurIPS 2025] MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/5fcedec09977357f32e8e0ec8957073b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LeiZZPD24</guid></item><item><title>[NeurIPS 2025] FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/62c6d7893b13a13c659cb815852dd00d-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LinCHWLLSL24</guid></item><item><title>[NeurIPS 2025] NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/69d97a6493fbf016fff0a751f253ad18-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ShaoJUDxM0YGKKK24</guid></item><item><title>[NeurIPS 2025] No-Regret Learning for Fair Multi-Agent Social Welfare Optimization.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/6a183ab792c56627d4c9dc45c1f5a616-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhangVL24</guid></item><item><title>[NeurIPS 2025] Local Curvature Smoothing with Stein's Identity for Efficient Score Matching.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/6a412f0037b0df295a39a198666ea6a6-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/OsadaSN24</guid></item><item><title>[NeurIPS 2025] DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/6d0942e288ce41db8d4ebd041e7d1100-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Zhao0L0LL24</guid></item><item><title>[NeurIPS 2025] SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/6efcc7fd8efeee29a050a79c843c90e0-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DingPMKYR24</guid></item><item><title>[NeurIPS 2025] Gradient Rewiring for Editable Graph Neural Network Training.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/716ddcbb5aa8802f56a7dfd94c4df3db-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Jiang0HFJTZ0H24</guid></item><item><title>[NeurIPS 2025] HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/722968fb52c75947371c6b02a58c959a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/CotnareanuZZ0C24</guid></item><item><title>[NeurIPS 2025] Score-based generative models are provably robust: an uncertainty quantification perspective.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7371b5f5fab9fd401c4ebd352f29dc48-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Mimikos-Stamatopoulos24</guid></item><item><title>[NeurIPS 2025] SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/754612bde73a8b65ad8743f1f6d8ddf6-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LuCYZ24</guid></item><item><title>[NeurIPS 2025] On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/79353864175b6b8c3d073cde84d7014a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhuLY0CK24</guid></item><item><title>[NeurIPS 2025] GraphVis: Boosting LLMs with Visual Knowledge Graph Integration.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7cb04f510593c9ba30da398f5e0a7e7b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Deng00MK024</guid></item><item><title>[NeurIPS 2025] Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7d6e85e88495104442af94c98e899659-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/NayakOHZTCKR0HM24</guid></item><item><title>[NeurIPS 2025] Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptative Residual Module.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7e0dc9ccba0f1333be13a3f9dc2b3138-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhouDZ0YZ00ZL24</guid></item><item><title>[NeurIPS 2025] What Is Missing For Graph Homophily? Disentangling Graph Homophily For Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/7e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhengL024</guid></item><item><title>[NeurIPS 2025] Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/806288e682d8a38c0bf21e37ab38af0a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/YuanXHS0024</guid></item><item><title>[NeurIPS 2025] Multi-Agent Domain Calibration with a Handful of Offline Data.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/80f628f21b040cfd281b10628ba7e6c0-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Jiang0LGZ024</guid></item><item><title>[NeurIPS 2025] LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/8196be81e68289d7a9ece21ed7f5750a-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiLDLWXSWSRGSS24</guid></item><item><title>[NeurIPS 2025] WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/820c61a0cd419163ccbd2c33b268816e-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0008KE24</guid></item><item><title>[NeurIPS 2025] Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/83b7da3ed13f06c13ce82235c8eedf35-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/00010ZZC024</guid></item><item><title>[NeurIPS 2025] Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/83ce241ce40aef32225eb2833ca2363c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/VepaYCJSS24</guid></item><item><title>[NeurIPS 2025] Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/84f44b36ceb4fbc9bb269959f4796eed-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/UwamichiSKS24</guid></item><item><title>[NeurIPS 2025] Are Graph Neural Networks Optimal Approximation Algorithms?</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/85db52cc08c5e00cfb1d216b1c85ba35-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/YauKLXJ24</guid></item><item><title>[NeurIPS 2025] Secret Collusion among AI Agents: Multi-Agent Deception via Steganography.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/861f7dad098aec1c3560fb7add468d41-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MotwaniBSB0HW24</guid></item><item><title>[NeurIPS 2025] Efficient Leverage Score Sampling for Tensor Train Decomposition.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/86c1fd74fa25bd6be0072937803e0bd1-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BharadwajRMR24</guid></item><item><title>[NeurIPS 2025] AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/877b40688e330a0e2a3fc24084208dfa-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MaZZYYJLKH24</guid></item><item><title>[NeurIPS 2025] Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/8839556afa4efe2a7dc5aae5e0a22fd4-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/HsuWP024</guid></item><item><title>[NeurIPS 2025] Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/8bb7d93ee3ce2c75da68ebeb51508111-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DouDLC24</guid></item><item><title>[NeurIPS 2025] RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/909f526db5127f8bd8158af32d9e313a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0002CMDL024</guid></item><item><title>[NeurIPS 2025] Compact Proofs of Model Performance via Mechanistic Interpretability.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/90e73f3cf1a6c84c723a2e8b7fb2b2c1-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/GrossAKOYGNC24</guid></item><item><title>[NeurIPS 2025] Graph Neural Networks Need Cluster-Normalize-Activate Modules.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/91b1966914203338495a76d13b23599c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SkryaginDADK24</guid></item><item><title>[NeurIPS 2025] Automated Label Unification for Multi-Dataset Semantic Segmentation with GNNs.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/92af0c8c2664429de2bb44c2692d84ae-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MaC0P24</guid></item><item><title>[NeurIPS 2025] AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/97091a5177d8dc64b1da8bf3e1f6fb54-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DebenedettiZBB024</guid></item><item><title>[NeurIPS 2025] Small coresets via negative dependence: DPPs, linear statistics, and concentration.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/997089469acbeb410405e43f0011be1f-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BardenetGST24</guid></item><item><title>[NeurIPS 2025] Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/99d7a578d72ed133203d1f88c9d39044-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BuiMN24a</guid></item><item><title>[NeurIPS 2025] Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2).</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/9b9cfd5428153ccfbd4ba34b7e007305-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SaxonJKLSW24</guid></item><item><title>[NeurIPS 2025] Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a06e129e01e0d2ef853e9ff67b911360-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiMCTLMLS24</guid></item><item><title>[NeurIPS 2025] DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/SongHLFS24</guid></item><item><title>[NeurIPS 2025] Continuous Product Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a4352e2c9d93582898a2a20e1f514e8f-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/EinizadeMG24</guid></item><item><title>[NeurIPS 2025] GRANOLA: Adaptive Normalization for Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a4c17d9b88eaefc9bdf7c656ffc8ce55-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/EliasofBSM24</guid></item><item><title>[NeurIPS 2025] Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a862f5788fd09bb6843c694d8120d50c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiuCQLL24</guid></item><item><title>[NeurIPS 2025] InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a8f7d43ae092d9a5295775eb17f3f4f7-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/GuptaMKG24</guid></item><item><title>[NeurIPS 2025] Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/a9db2b121c3517fd559ecbe5038701ee-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DuanGWYQSTY24</guid></item><item><title>[NeurIPS 2025] Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/aaa0ac4253da75faf9b0dc0dda062612-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/TaradayDB24</guid></item><item><title>[NeurIPS 2025] Graph neural networks and non-commuting operators.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ad9964b731bc7b5621a83c7869fc653b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/VelascoORV24</guid></item><item><title>[NeurIPS 2025] Real-time Core-Periphery Guided ViT with Smart Data Layout Selection on Mobile Devices.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/adb77ecc8ba1c2d3135c86a46b8f2496-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ShuY00SY0ZN24</guid></item><item><title>[NeurIPS 2025] ID-to-3D: Expressive ID-guided 3D Heads via Score Distillation Sampling.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/b0171a5965e0b58afcd4ce1adb0baa56-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BabiloniLDZ24</guid></item><item><title>[NeurIPS 2025] Evaluating language models as risk scores.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/b0a4b3e384b4554e65a47ad1f6b0310a-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/CruzHM24</guid></item><item><title>[NeurIPS 2025] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/b10ed15ff1aa864f1be3a75f1ffc021b-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Luo0024</guid></item><item><title>[NeurIPS 2025] A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/b739d7ae14c0dd4c7619476f3f80ec98-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Bar-ShalomEFM24</guid></item><item><title>[NeurIPS 2025] Integrating Suboptimal Human Knowledge with Hierarchical Reinforcement Learning for Large-Scale Multiagent Systems.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ba29e3f830d039c3f1fa0b4dfcf19c54-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiuKGR0S24</guid></item><item><title>[NeurIPS 2025] HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ba92705991cfbbcedc26e27e833ebbae-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DuX024</guid></item><item><title>[NeurIPS 2025] APDDv2: Aesthetics of Paintings and Drawings Dataset with Artist Labeled Scores and Comments.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/baa66b34c11716bdff4854bbc8e667e4-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/JinQLWHGLL24</guid></item><item><title>[NeurIPS 2025] Reinforcing LLM Agents via Policy Optimization with Action Decomposition.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/bc09efb501c801ed92e181e26a885c2d-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WenWWZ024</guid></item><item><title>[NeurIPS 2025] Logical characterizations of recurrent graph neural networks with reals and floats.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/bca7a9a0dd85e2a68420e5cae27eccfb-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/AhvonenHKL24</guid></item><item><title>[NeurIPS 2025] SCOREQ: Speech Quality Assessment with Contrastive Regression.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/bece7e02455a628b770e49fcfa791147-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/RaganoSH24</guid></item><item><title>[NeurIPS 2025] Compositional PAC-Bayes: Generalization of GNNs with persistence and beyond.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/c3266c14d7eb9715d9fad4306133aa4e-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BrilliantovS024</guid></item><item><title>[NeurIPS 2025] Score-Optimal Diffusion Schedules.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/c34057f0cfb685eadf7d8d7cc99f734a-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WilliamsCDS24</guid></item><item><title>[NeurIPS 2025] Robust Graph Neural Networks via Unbiased Aggregation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/c6e31f86c1eb8dfc05190cf15ed52064-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/HouFDL24</guid></item><item><title>[NeurIPS 2025] Hamiltonian Score Matching and Generative Flows.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/c735853cfb9003002db68cdacb1b6e1b-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/HolderriethXJ24</guid></item><item><title>[NeurIPS 2025] PowerGraph: A power grid benchmark dataset for graph neural networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/c7caf017cbbca1f4b368ffdc7bb8f319-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/VarbellaAGES24</guid></item><item><title>[NeurIPS 2025] Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ca9567d8ef6b2ea2da0d7eed57b933ee-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Piatti0KSSM24</guid></item><item><title>[NeurIPS 2025] SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/cef5c8dec67597b854f0162ad76d92d2-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0021FGK24</guid></item><item><title>[NeurIPS 2025] One-Step Diffusion Distillation through Score Implicit Matching.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d107ca794d83c8242e357e6a43a068f4-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LuoHGKQ24</guid></item><item><title>[NeurIPS 2025] A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d1d11bf8299334d354949ba8738e8301-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/BortolottiMCMKV24</guid></item><item><title>[NeurIPS 2025] DropEdge not Foolproof: Effective Augmentation Method for Signed Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d450dceeacd6083d1d550247377f2320-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0004LWWWLHL24</guid></item><item><title>[NeurIPS 2025] Multi-Agent Coordination via Multi-Level Communication.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d6be51e667e0b263e89a23294b57f8cf-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/DingLFS0L24</guid></item><item><title>[NeurIPS 2025] Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Wang0LZH024</guid></item><item><title>[NeurIPS 2025] On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/d978cd64d598bbaf6a2fff1d6864d601-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/PellizzoniSCB24</guid></item><item><title>[NeurIPS 2025] Suitable is the Best: Task-Oriented Knowledge Fusion in Vulnerability Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/db7a81128155d6f14970c12d0b5e7a4c-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangHN0DKDK24</guid></item><item><title>[NeurIPS 2025] GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/df2d51e1d3e899241c5c4c779c1d509f-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ArmgaanDMR24</guid></item><item><title>[NeurIPS 2025] Rethinking the Capacity of Graph Neural Networks for Branching Strategy.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/dff528ce3e1390c88f10bbf5e722a241-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Chen0CWY24</guid></item><item><title>[NeurIPS 2025] Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/e142fd2b70f10db2543c64bca1417de8-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangJYWOSK024</guid></item><item><title>[NeurIPS 2025] Almost Surely Asymptotically Constant Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/e1cd1db6fcd4a6df8618259f5b9cca0f-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Adam-DayBCF24</guid></item><item><title>[NeurIPS 2025] Dynamic Rescaling for Training GNNs.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/e2d89f6d3d7707709afa2e01f5952f95-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MustafaB24</guid></item><item><title>[NeurIPS 2025] Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/e46c303cfd3b93bd32ecf6070318efc3-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/0005Y24</guid></item><item><title>[NeurIPS 2025] General bounds on the quality of Bayesian coresets.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/e589022774244df75b97eae18bb3628d-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Campbell24</guid></item><item><title>[NeurIPS 2025] AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/eb113910e9c3f6242541c1652e30dfd6-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ChenXXSL24</guid></item><item><title>[NeurIPS 2025] LLMDFA: Analyzing Dataflow in Code with Large Language Models.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ed9dcde1eb9c597f68c1d375bbecf3fc-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangZSXX024</guid></item><item><title>[NeurIPS 2025] EigenVI: score-based variational inference with orthogonal function expansions.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ef72fa6579401ffff9da246a5014f055-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Cai0MGBS24</guid></item><item><title>[NeurIPS 2025] WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/f0a4a0ecdc29a0087c0848948e2fce81-Abstract-Datasets_and_Benchmarks_Track.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/MonrocBDZ24</guid></item><item><title>[NeurIPS 2025] Exploring Consistency in Graph Representations: from Graph Kernels to Graph Neural Networks.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/f631e778fd3c1b871e9e3a94369335e9-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/LiuCYY24</guid></item><item><title>[NeurIPS 2025] Aligning LLM Agents by Learning Latent Preference from User Edits.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/f75744612447126da06767daecce1a84-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/GaoTSMM24</guid></item><item><title>[NeurIPS 2025] FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/f7ae4fe91d96f50abc2211f09b6a7e49-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/YuYLDJCCSCLXZSX24</guid></item><item><title>[NeurIPS 2025] Reflective Multi-Agent Collaboration based on Large Language Models.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/fa54b0edce5eef0bb07654e8ee800cb4-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/Bo0DFW00W24</guid></item><item><title>[NeurIPS 2025] Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/fa76985f05e0a25c66528308dda33de0-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/ZhangLWSYL24</guid></item><item><title>[NeurIPS 2025] Propensity Score Alignment of Unpaired Multimodal Data.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/ff56c8dc4c61017eab9771481b7fda0d-Abstract-Conference.html</link><description></description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/XiOXH24</guid></item><item><title>[ASE] Language-Agnostic Static Analysis of Probabilistic Programs.</title><link>https://doi.org/10.1145/3691620.3695031</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Bock0C24</guid></item><item><title>[ASE] Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency.</title><link>https://doi.org/10.1145/3691620.3695010</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangL0X24</guid></item><item><title>[ASE] Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network.</title><link>https://doi.org/10.1145/3691620.3695068</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/CuiWWJQZHWL24</guid></item><item><title>[ASE] AdvSCanner: Generating Adversarial Smart Contracts to Exploit Reentrancy Vulnerabilities Using LLM and Static Analysis.</title><link>https://doi.org/10.1145/3691620.3695482</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuXPLW00W24</guid></item><item><title>[ASE] STASE: Static Analysis Guided Symbolic Execution for UEFI Vulnerability Signature Generation.</title><link>https://doi.org/10.1145/3691620.3695543</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ShafiuzzamanDSB24</guid></item><item><title>[ASE] Efficient Incremental Code Coverage Analysis for Regression Test Suites.</title><link>https://doi.org/10.1145/3691620.3695551</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangW024</guid></item><item><title>[ASE] Enhancing Compositional Static Analysis with Dynamic Analysis.</title><link>https://doi.org/10.1145/3691620.3695599</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/DistefanoMACSGH24</guid></item><item><title>[ASE] Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation.</title><link>https://doi.org/10.1145/3691620.3695291</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZZLCZ024</guid></item><item><title>[ASE] Oracle-Guided Vulnerability Diversity and Exploit Synthesis of Smart Contracts Using LLMs.</title><link>https://doi.org/10.1145/3691620.3695292</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/EshghieA24</guid></item><item><title>[ASE] ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts.</title><link>https://doi.org/10.1145/3691620.3695349</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZGXG024</guid></item><item><title>[ASE] Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers.</title><link>https://doi.org/10.1145/3691620.3695322</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LuoYZLX24</guid></item><item><title>[ASE 2025] Can Cooperative Multi-Agent Reinforcement Learning Boost Automatic Web Testing? An Exploratory Study.</title><link>https://doi.org/10.1145/3691620.3694983</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/FanWFQL024</guid></item><item><title>[ASE 2025] GPP: A Graph-Powered Prioritizer for Code Review Requests.</title><link>https://doi.org/10.1145/3691620.3694990</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/YangX0WLLB24</guid></item><item><title>[ASE 2025] Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models.</title><link>https://doi.org/10.1145/3691620.3695013</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Wu0YG024</guid></item><item><title>[ASE 2025] LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference.</title><link>https://doi.org/10.1145/3691620.3695014</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuC0W0M24</guid></item><item><title>[ASE 2025] Program Synthesis Meets Visual What-Comes-Next Puzzles.</title><link>https://doi.org/10.1145/3691620.3695015</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LahiriKCV024</guid></item><item><title>[ASE 2025] GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph.</title><link>https://doi.org/10.1145/3691620.3695054</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiuYZS00JW24</guid></item><item><title>[ASE 2025] Snopy: Bridging Sample Denoising with Causal Graph Learning for Effective Vulnerability Detection.</title><link>https://doi.org/10.1145/3691620.3695057</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Cao000B00L024</guid></item><item><title>[ASE 2025] Compositional Security Analysis of Dynamic Component-based Systems.</title><link>https://doi.org/10.1145/3691620.3695499</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/KhakpourS24</guid></item><item><title>[ASE 2025] HITS: High-coverage LLM-based Unit Test Generation via Method Slicing.</title><link>https://doi.org/10.1145/3691620.3695501</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangL0J24</guid></item><item><title>[ASE 2025] Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?</title><link>https://doi.org/10.1145/3691620.3695539</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoGHWW024</guid></item><item><title>[ASE 2025] Understanding Developer-Analyzer Interactions in Code Reviews.</title><link>https://doi.org/10.1145/3691620.3695257</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/SchafCLMTSZZ24</guid></item><item><title>[ASE 2025] DeployFix: Dynamic Repair of Software Deployment Failures via Constraint Solving.</title><link>https://doi.org/10.1145/3691620.3695268</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiaoG0HY0DXYZ24</guid></item><item><title>[ASE 2025] Experience Report on Applying Program Analysis Techniques for Mainframe Application Understanding.</title><link>https://doi.org/10.1145/3691620.3695270</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/AgarwalNK24</guid></item><item><title>[ASE 2025] RepoGenix: Dual Context-Aided Repository-Level Code Completion with Language Models.</title><link>https://doi.org/10.1145/3691620.3695331</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiangXZZD0CWF24</guid></item><item><title>[ASE 2025] Towards LLM-augmented multiagent systems for agile software engineering.</title><link>https://doi.org/10.1145/3691620.3695336</link><description></description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/CinkuszC24</guid></item><item><title>[RAID 2024] Large-Scale Security Analysis of Real-World Backend Deployments Speaking IoT-Focused Protocols.</title><link>https://doi.org/10.1145/3678890.3678899</link><description></description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Mon, 30 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/TagliaroKCBL24</guid></item><item><title>[RAID 2024] A Comprehensive, Automated Security Analysis of the Uptane Automotive Over-the-Air Update Framework.</title><link>https://doi.org/10.1145/3678890.3678927</link><description></description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Mon, 30 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/LorchLTC24</guid></item><item><title>[FM 2024] Learning Branching-Time Properties in CTL and ATL via Constraint Solving.</title><link>https://doi.org/10.1007/978-3-031-71162-6_16</link><description></description><author>dblp: new volumes for streams/conf/fm</author><pubDate>Mon, 16 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/fm/BordaisNR24</guid></item><item><title>[PLDI 2022] A graph neural network-based performance model for deep learning applications.</title><link>https://doi.org/10.1145/3520312.3534863</link><description></description><author>dblp: new volumes for streams/conf/pldi</author><pubDate>Tue, 28 Jun 2022 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/pldi/SinghHLS22</guid></item><item><title>[POPL] Hypercollecting semantics and its application to static analysis of information flow.</title><link>https://doi.org/10.1145/3009837.3009889</link><description></description><author>dblp: new volumes for streams/conf/popl</author><pubDate>Tue, 27 Dec 2016 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/popl/AssafNSTT17</guid></item></channel></rss>