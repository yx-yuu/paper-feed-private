<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Sun, 01 Mar 2026 06:56:00 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[SCP 2026] DATVD: A novel vulnerability detection method based on dynamic attention and hybrid convolutional pooling</title><link>https://www.sciencedirect.com/science/article/pii/S0167642326000080?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Science of Computer Programming, Volume 251&lt;/p&gt;&lt;p&gt;Author(s): Jinfu Chen, Jinyu Mu, Saihua Cai, Jiapeng Zhou, Ziyan Liu, Xinping Shi&lt;/p&gt;</description><author>ScienceDirect Publication: Science of Computer Programming</author><pubDate>Sun, 01 Mar 2026 06:56:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167642326000080</guid></item><item><title>[JSS 2026] Clash: Enhancing context-sensitivity in data-flow analysis for mitigating the impact of indirect calls</title><link>https://www.sciencedirect.com/science/article/pii/S0164121225004224?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 235&lt;/p&gt;&lt;p&gt;Author(s): Jinyan Xie, Yingzhou Zhang, Mingzhe Hu, Liping Han, Le Yu, Qiuran Ding&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Sun, 01 Mar 2026 06:56:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0164121225004224</guid></item><item><title>[JSS 2026] RLV: LLM-based vulnerability detection by retrieving and refining contextual information</title><link>https://www.sciencedirect.com/science/article/pii/S016412122500425X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Journal of Systems and Software, Volume 235&lt;/p&gt;&lt;p&gt;Author(s): Fangcheng Qiu, Zhongxin Liu, Bingde Hu, Zhengong Cai, Lingfeng Bao, Xinyu Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Journal of Systems and Software</author><pubDate>Sun, 01 Mar 2026 06:56:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S016412122500425X</guid></item><item><title>[IST 2026] COTVD: A function-level vulnerability detection framework using chain-of-thought reasoning with large language models</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000327?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 193&lt;/p&gt;&lt;p&gt;Author(s): Yinan Chen, Xiangping Chen, Yuan Huang, Changlin Yang, Lei Yun&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sun, 01 Mar 2026 06:55:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000327</guid></item><item><title>[arXiv-CR 2026] Accelerating Incident Response: A Hybrid Approach for Data Breach Reporting</title><link>https://arxiv.org/abs/2602.22244</link><description>arXiv:2602.22244v1 Announce Type: new 
Abstract: The General Data Protection Regulation (GDPR) requires organisations to notify supervisory authorities of personal data breaches within 72 hours of discovery. Meeting this strict deadline is challenging because incident responders must manually translate low-level forensic artefacts such as malware traces, system-call logs, and network captures into the structured, legally framed information required by data-protection authorities. This gap between technical evidence and regulatory reporting often results in delays, incomplete notifications, and a high cognitive burden on analysts. We propose a hybrid malware analysis pipeline that automates the extraction and organisation of breach-relevant information, with a particular focus on exfiltration-oriented Linux/ARM malware, which is rapidly increasing in prevalence due to the widespread adoption of IoT and embedded devices. The system combines static analysis to identify potential exfiltrators with dynamic analysis to reconstruct their behaviour. It employs a Large Language Model (LLM) constrained by a formal JSON schema aligned with the official Italian Garante Privacy notification form. The LLM transforms heterogeneous forensic artefacts into a structured, compliance-ready report that a human operator can rapidly validate.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22244v1</guid></item><item><title>[arXiv-CR 2026] Predicting Known Vulnerabilities from Attack Descriptions Using Sentence Transformers</title><link>https://arxiv.org/abs/2602.22433</link><description>arXiv:2602.22433v1 Announce Type: new 
Abstract: Modern infrastructures rely on software systems that remain vulnerable to cyberattacks. These attacks frequently exploit vulnerabilities documented in repositories such as MITRE's Common Vulnerabilities and Exposures (CVE). However, Cyber Threat Intelligence resources, including MITRE ATT&amp;amp;CK and CVE, provide only partial coverage of attack-vulnerability relationships. Attack information often appears before vulnerabilities are formally linked, creating the need for automated methods that infer likely vulnerabilities directly from attack descriptions.
  This thesis addresses the problem of predicting known vulnerabilities from natural-language descriptions of cyberattacks. We develop transformer-based sentence embedding methods that encode attack and vulnerability descriptions into semantic vector representations, enabling similarity-based ranking and recommendation.
  Fourteen state-of-the-art transformer models were evaluated across four attack description types (Tactic, Technique, Procedure, and Attack Pattern). Results show that Technique descriptions in MITRE ATT&amp;amp;CK provide the strongest predictive signal. The multi-qa-mpnet-base-dot-v1 (MMPNet) model achieved the best performance due to its hybrid pre-training and optimization for semantic similarity.
  The approach was implemented in the VULDAT tool, which automatically links attacks to vulnerabilities. Manual validation revealed previously undocumented relationships in MITRE repositories. Evaluation on unseen cyberattack reports demonstrates that the models generalize beyond curated datasets and support proactive vulnerability awareness.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22433v1</guid></item><item><title>[arXiv-CR 2026] Automated Vulnerability Detection in Source Code Using Deep Representation Learning</title><link>https://arxiv.org/abs/2602.23121</link><description>arXiv:2602.23121v1 Announce Type: new 
Abstract: Each year, software vulnerabilities are discovered, which pose significant risks of exploitation and system compromise. We present a convolutional neural network model that can successfully identify bugs in C code. We trained our model using two complementary datasets: a machine-labeled dataset created by Draper Labs using three static analyzers and the NIST SATE Juliet human-labeled dataset designed for testing static analyzers. In contrast with the work of Russell et al. on these datasets, we focus on C programs, enabling us to specialize and optimize our detection techniques for this language. After removing duplicates from the dataset, we tokenize the input into 91 token categories. The category values are converted to a binary vector to save memory. Our first convolution layer is chosen so that the entire encoding of the token is presented to the filter. We use two convolution and pooling layers followed by two fully connected layers to classify programs into either a common weakness enumeration category or as ``clean.'' We obtain higher recall than prior work by Russell et al. on this dataset when requiring high precision. We also demonstrate on a custom Linux kernel dataset that we are able to find real vulnerabilities in complex code with a low false-positive rate.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23121v1</guid></item><item><title>[arXiv-SE 2026] EmpiRE-Compass: A Neuro-Symbolic Dashboard for Sustainable and Dynamic Knowledge Exploration, Synthesis, and Reuse</title><link>https://arxiv.org/abs/2602.22276</link><description>arXiv:2602.22276v1 Announce Type: new 
Abstract: Software engineering (SE) and requirements engineering (RE) face a significant increase in secondary studies, particularly literature reviews (LRs), due to the ever-growing number of scientific publications. Generative artificial intelligence (GenAI) exacerbates this trend by producing LRs rapidly but often at the expense of quality, rigor, and transparency. At the same time, secondary studies often fail to share underlying data and artifacts, limiting replication and reuse. This paper introduces EmpiRE-Compass, a neuro-symbolic dashboard designed to lower barriers for accessing, replicating, and reusing LR data. Its overarching goal is to demonstrate how LRs can become more sustainable by semantically structuring their underlying data in research knowledge graphs (RKGs) and by leveraging large language models (LLMs) for easy and dynamic access, replication, and reuse. Building on two RE use cases, we developed EmpiRE-Compass with a modular system design and workflows for curated and custom competency questions. The dashboard is freely available online, accompanied by a demonstration video. To manage operational costs, a limit of 25 requests per IP address per day applies to the default LLM (GPT-4o mini). All source code and documentation are released as an open-source project to foster reuse, adoption, and extension. EmpiRE-Compass provides three core capabilities: (1) Exploratory visual analytics for curated competency questions; (2) Neuro-symbolic synthesis for custom competency questions; and (3) Reusable knowledge with all queries, analyses, and results openly available. By unifying RKGs and LLMs in a neuro-symbolic dashboard, EmpiRE-Compass advances sustainable LRs in RE, SE, and beyond. It lowers technical barriers, fosters transparency and reproducibility, and enables collaborative, continuously updated, and reusable LRs</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22276v1</guid></item><item><title>[arXiv-SE 2026] RepoMod-Bench: A Benchmark for Code Repository Modernization via Implementation-Agnostic Testing</title><link>https://arxiv.org/abs/2602.22518</link><description>arXiv:2602.22518v1 Announce Type: new 
Abstract: The evolution of AI coding agents has shifted the frontier from simple snippet completion to autonomous repository-level engineering. However, evaluating these agents remains ill-posed in general code repository generation, where the lack of deterministic ground truth leads to ambiguous metrics. Code modernization via automated translation offers a more rigorous alternative by providing a fixed ground truth -- the source repository; yet existing benchmarks are limited to small-scale repositories and rely on language-specific unit tests visible to the agent, allowing test-driven overfitting.
  We address these limitations by introducing a benchmarking framework for repository-level code modernization built on an implementation-agnostic evaluation paradigm. This framework is instantiated through RepoMod-Bench: a benchmark of 21 real-world repositories with standardized interfaces, spanning 8 programming languages. The benchmark contains 1.6M lines of code (LOC) and 11,616 tests, with repository sizes ranging from 14 to 211K LOC. By targeting repositories with standardized interfaces, we utilize an implementation-agnostic test suite to verify functional equivalence between source and target implementations. This black-box approach ensures verification remains consistent across languages, and our environment hides all test suites from agents to prevent test-driven shortcuts. Evaluating four state-of-the-art agent configurations reveals a sharp scaling collapse: average pass rates drop from 91.3% on projects under 10K LOC to 15.3% on projects exceeding 50K LOC. These results demonstrate that autonomous modernization at scale remains a significant open challenge. Our benchmark and code are available at https://github.com/Modelcode-ai/mcode-benchmark.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22518v1</guid></item><item><title>[arXiv-SE 2026] Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents</title><link>https://arxiv.org/abs/2602.22764</link><description>arXiv:2602.22764v1 Announce Type: new 
Abstract: The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level software engineering tasks from 34 diverse and popular Rust repositories. We then perform a comprehensive study on Rust-SWE-bench with four representative agents and four state-of-the-art LLMs to establish a foundational understanding of their capabilities and limitations in the Rust ecosystem. Our extensive study reveals that while ReAct-style agents are promising, i.e., resolving up to 21.2% of issues, they are limited by two primary challenges: comprehending repository-wide code structure and complying with Rust's strict type and trait semantics. We also find that issue reproduction is rather critical for task resolution. Inspired by these findings, we propose RUSTFORGER, a novel agentic approach that integrates an automated test environment setup with a Rust metaprogramming-driven dynamic tracing strategy to facilitate reliable issue reproduction and dynamic analysis. The evaluation shows that RUSTFORGER using Claude-Sonnet-3.7 significantly outperforms all baselines, resolving 28.6% of tasks on Rust-SWE-bench, i.e., a 34.9% improvement over the strongest baseline, and, in aggregate, uniquely solves 46 tasks that no other agent could solve across all adopted advanced LLMs.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22764v1</guid></item><item><title>[arXiv-SE 2026] Managing Uncertainty in LLM-based Multi-Agent System Operation</title><link>https://arxiv.org/abs/2602.23005</link><description>arXiv:2602.23005v1 Announce Type: new 
Abstract: Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering concern. This paper approaches uncertainty from both system-level and runtime perspectives. We first differentiate epistemological and ontological uncertainties in the context of LLM-based multi-agent software system operation. Building on this foundation, we propose a lifecycle-based uncertainty management framework comprising four mechanisms: representation, identification, evolution, and adaptation. The uncertainty lifecycle governs how uncertainties emerge, transform, and are mitigated across architectural layers and execution phases, enabling structured runtime governance and controlled adaptation. We demonstrate the feasibility of the framework using a real-world LLM-based multi-agent echocardiographic software system developed in clinical collaboration, showing improved reliability and diagnosability in diagnostic reasoning. The proposed approach generalizes to other safety-critical LLM-based multi-agent software systems, supporting principled operational control and runtime assurance beyond model-centric methods.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23005v1</guid></item><item><title>[arXiv-SE 2026] CL4SE: A Context Learning Benchmark For Software Engineering Tasks</title><link>https://arxiv.org/abs/2602.23047</link><description>arXiv:2602.23047v1 Announce Type: new 
Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive &amp; negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23047v1</guid></item><item><title>[arXiv-SE 2026] LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer</title><link>https://arxiv.org/abs/2602.23065</link><description>arXiv:2602.23065v1 Announce Type: new 
Abstract: Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.
  Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23065v1</guid></item><item><title>[arXiv-SE 2026] Utilizing LLMs for Industrial Process Automation</title><link>https://arxiv.org/abs/2602.23331</link><description>arXiv:2602.23331v1 Announce Type: new 
Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23331v1</guid></item><item><title>[arXiv-SE 2026] Array-Carrying Symbolic Execution for Function Contract Generation</title><link>https://arxiv.org/abs/2602.23216</link><description>arXiv:2602.23216v1 Announce Type: cross 
Abstract: Function contract generation is a classical problem in program analysis that targets the automated analysis of functions in a program with multiple procedures. The problem is fundamental in inter-procedural analysis where properties of functions are first obtained via the generation of function contracts and then the generated contracts are used as building blocks to analyze the whole program. Typical objectives in function contract generation include pre-/post-conditions and assigns information (that specifies the modification information over program variables and memory segments during function execution). In programs with array manipulations, a crucial point in function contract generation is the treatment of array segments that imposes challenges in inferring invariants and assigns information over such segments. To address this challenge, we propose a novel symbolic execution framework that carries invariants and assigns information over contiguous segments of arrays. We implement our framework as a prototype within LLVM, and further integrate our prototype with the ACSL assertion format and the Frama-C software verification platform. Experimental evaluation over a variety of benchmarks from the literature and functions from realistic libraries shows that our framework is capable of handling array manipulating functions that indeed involve the carry of array information and are beyond existing approaches.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23216v1</guid></item><item><title>[arXiv-SE 2026] FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven Code Repair Tasks</title><link>https://arxiv.org/abs/2504.06939</link><description>arXiv:2504.06939v2 Announce Type: replace 
Abstract: Code repair is a fundamental task in software development, facilitating efficient bug resolution and software maintenance. Although large language models (LLMs) have demonstrated considerable potential in automated code repair, their ability to comprehend and leverage diverse types of feedback, which is crucial for iterative self-correction in authentic debugging scenarios, remains insufficiently understood.
  To bridge this gap, we introduce FeedbackEval, a systematic benchmark constructed from three heterogeneous sources (HumanEval, CoderEval, and SWE-Bench-verified), to evaluate LLMs' feedback comprehension and code repair performance. We conduct a comprehensive empirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5, Deepseek-R1, GLM-4, and Qwen2.5, to evaluate their behavior under both single-iteration and iterative code repair settings. Our results show that mixed feedback yields the highest repair success (63.6%), with LLM-Expert and test feedback providing strong targeted gains (62.9% and 57.9%, respectively), while minimal (53.1%) and compiler feedback (49.2%) offer moderate benefits and LLM-Skilled proves least effective (48.8%). Iterative feedback further enhances repair performance, though the marginal benefit diminishes after two or three iterations. Moreover, prompt structure is shown to be critical: structured reasoning (RR, CoT) and dynamic example selection deliver notable improvements, whereas removing semantic cues such as docstrings or role-play causes severe degradation. This work introduces a robust benchmark and delivers practical insights to advance the understanding and development of feedback-driven code repair using LLMs.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.06939v2</guid></item><item><title>[arXiv-SE 2026] Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</title><link>https://arxiv.org/abs/2512.14990</link><description>arXiv:2512.14990v3 Announce Type: replace 
Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.14990v3</guid></item><item><title>[arXiv-AI 2026] VeRO: An Evaluation Harness for Agents to Optimize Agents</title><link>https://arxiv.org/abs/2602.22480</link><description>arXiv:2602.22480v1 Announce Type: new 
Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22480v1</guid></item><item><title>[arXiv-AI 2026] ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering</title><link>https://arxiv.org/abs/2602.23193</link><description>arXiv:2602.23193v1 Announce Type: new 
Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.23193v1</guid></item><item><title>[arXiv-AI 2026] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning</title><link>https://arxiv.org/abs/2510.25992</link><description>arXiv:2510.25992v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.25992v2</guid></item><item><title>[arXiv-AI 2026] LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)</title><link>https://arxiv.org/abs/2512.24796</link><description>arXiv:2512.24796v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, current benchmarks fail to adequately measure library-grounded abstraction -- the ability to reason with high-level interfaces and reusable structures central to modern mathematics and software engineering. We introduce LeanCat, a challenging benchmark comprising 100 fully formalized category-theory tasks in Lean. Unlike algebra or arithmetic, category theory serves as a rigorous stress test for structural, interface-level reasoning. Our evaluation reveals a severe abstraction gap: the best state-of-the-art model solves only 12.0% of tasks at pass@4, with performance collapsing from 55.0% on Easy tasks to 0.0% on High-difficulty tasks, highlighting a failure in compositional generalization. To overcome this, we evaluate LeanBridge, a retrieval-augmented agent that employs a retrieve-generate-verify loop. LeanBridge achieves a peak success rate of 24.0% -- doubling the performance of the best static baseline. These results empirically demonstrate that iterative refinement and dynamic library retrieval are not merely optimizations but strict necessities for neuro-symbolic reasoning in abstract domains. LeanCat offers a compact, reusable testbed for tracking progress toward reliable, research-level formalization.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 27 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.24796v2</guid></item><item><title>[arXiv-CR 2026] PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking</title><link>https://arxiv.org/abs/2507.21540</link><description>arXiv:2507.21540v2 Announce Type: replace 
Abstract: The increasing sophistication of large vision-language models (LVLMs) has been accompanied by advances in safety alignment mechanisms designed to prevent harmful content generation. However, these defenses remain vulnerable to sophisticated adversarial attacks. Existing jailbreak methods typically rely on direct and semantically explicit prompts, overlooking subtle vulnerabilities in how LVLMs compose information over multiple reasoning steps. In this paper, we propose a novel and effective jailbreak framework inspired by Return-Oriented Programming (ROP) techniques from software security. Our approach decomposes a harmful instruction into a sequence of individually benign visual gadgets. A carefully engineered textual prompt directs the sequence of inputs, prompting the model to integrate the benign visual gadgets through its reasoning process to produce a coherent and harmful output. This makes the malicious intent emergent and difficult to detect from any single component. We validate our method through extensive experiments on established benchmarks including SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our approach consistently and substantially outperforms existing baselines on state-of-the-art models, achieving near-perfect attack success rates (over 0.90 on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical and underexplored vulnerability that exploits the compositional reasoning abilities of LVLMs, highlighting the urgent need for defenses that secure the entire reasoning process.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.21540v2</guid></item><item><title>[arXiv-CR 2026] CASCADE: LLM-Powered JavaScript Deobfuscator at Google</title><link>https://arxiv.org/abs/2507.17691</link><description>arXiv:2507.17691v2 Announce Type: replace-cross 
Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilities of Gemini with the deterministic transformation capabilities of a compiler Intermediate Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to identify critical prelude functions, the foundational components underlying the most prevalent obfuscation techniques, and leveraging JSIR for subsequent code transformations, CASCADE effectively recovers semantic elements like original strings and API names, and reveals original program behaviors. This method overcomes limitations of existing static and dynamic deobfuscation techniques, eliminating hundreds to thousands of hardcoded rules while achieving reliability and flexibility. CASCADE is already deployed in Google's production environment, demonstrating substantial improvements in JavaScript deobfuscation efficiency and reducing reverse engineering efforts.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.17691v2</guid></item><item><title>[arXiv-SE 2026] Structurally Aligned Subtask-Level Memory for Software Engineering Agents</title><link>https://arxiv.org/abs/2602.21611</link><description>arXiv:2602.21611v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomposition. Extensive experiments on SWE-bench Verified demonstrate that our method consistently outperforms both vanilla agents and strong instance-level memory baselines across diverse backbones, improving mean Pass@1 over the vanilla agent by +4.7 pp on average (e.g., +6.8 pp on Gemini 2.5 Pro). Performance gains grow with more interaction steps, showing that leveraging past experience benefits long-horizon reasoning in complex software engineering tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.21611v1</guid></item><item><title>[arXiv-SE 2026] An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention</title><link>https://arxiv.org/abs/2602.21800</link><description>arXiv:2602.21800v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.21800v1</guid></item><item><title>[arXiv-SE 2026] Enhancing LLM-Based Test Generation by Eliminating Covered Code</title><link>https://arxiv.org/abs/2602.21997</link><description>arXiv:2602.21997v1 Announce Type: new 
Abstract: Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.21997v1</guid></item><item><title>[arXiv-SE 2026] SWE-Prot\'eg\'e: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents</title><link>https://arxiv.org/abs/2602.22124</link><description>arXiv:2602.22124v1 Announce Type: new 
Abstract: Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot\'eg\'e, a post-training framework that reframes software repair as an expert-prot\'eg\'e collaboration problem. In SWE-Prot\'eg\'e, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.22124v1</guid></item><item><title>[arXiv-SE 2026] CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings</title><link>https://arxiv.org/abs/2509.11787</link><description>arXiv:2509.11787v3 Announce Type: replace 
Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and code smells. Traditionally, developers must resolve these warnings manually. Because this process is tedious, developers sometimes ignore warnings, leading to an accumulation of warnings and a degradation of code quality. This paper presents CodeCureAgent, an approach that harnesses LLM-based agents to automatically analyze, classify, and repair static analysis warnings. Unlike previous work, our method does not follow a predetermined algorithm. Instead, we adopt an agentic framework that iteratively invokes tools to gather additional information from the codebase (e.g., via code search) and edit the codebase to resolve the warning. CodeCureAgent detects and suppresses false positives, while fixing true positives when identified. We equip CodeCureAgent with a three-step heuristic to approve patches: (1) build the project, (2) verify that the warning disappears without introducing new warnings, and (3) run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube warnings found in 106 Java projects and covering 291 distinct rules. Our approach produces plausible fixes for 96.8% of the warnings, outperforming state-of-the-art baseline approaches by 29.2%-34.0% in plausible-fix rate. Manual inspection of 291 cases reveals a correct-fix rate of 86.3%, showing that CodeCureAgent can reliably repair static analysis warnings. The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end processing time of about four minutes per warning. We envision CodeCureAgent helping to clean existing codebases and being integrated into CI/CD pipelines to prevent the accumulation of static analysis warnings.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.11787v3</guid></item><item><title>[arXiv-PL 2026] Scylla: Translating an Applicative Subset of C to Safe Rust</title><link>https://arxiv.org/abs/2412.15042</link><description>arXiv:2412.15042v2 Announce Type: replace 
Abstract: The popularity of the Rust language continues to explode; yet, many critical codebases remain authored in C. Automatically translating C to Rust is thus an appealing course of action. Several works have gone down this path, handling an ever-increasing subset of C through a variety of Rust features, such as unsafe. While the prospect of automation is appealing, producing code that relies on unsafe negates the memory safety guarantees offered by Rust, and therefore the main advantages of porting existing codebases to memory-safe languages. We instead advocate for a different approach, where the programmer iterates on the original C, gradually making the code more structured until it becomes eligible for compilation to safe Rust. This means that redesigns and rewrites can be evaluated incrementally for performance and correctness against existing test suites and production environments.
  Compiling structured C to safe Rust relies on the following contributions: a type-directed translation from (a subset of) C to safe Rust; a novel static analysis based on "split trees" which allows expressing C's pointer arithmetic using Rust's slices and splitting operations; an analysis that infers which borrows need to be mutable; and a compilation strategy for C pointer types that is compatible with Rust's distinction between non-owned and owned allocations. We evaluate our approach on real-world cryptographic libraries, binary parsers and serializers, and a file compression library. We show that these can be rewritten to Rust with small refactors of the original C code, and that the resulting Rust code exhibits similar performance characteristics as the original C code. As part of our translation process, we also identify and report undefined behaviors in the bzip2 compression library and in Microsoft's implementation of the FrodoKEM cryptographic primitive.</description><author>cs.PL updates on arXiv.org</author><pubDate>Thu, 26 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2412.15042v2</guid></item><item><title>[arXiv-CR 2026] SoK: Agentic Skills -- Beyond Tool Use in LLM Agents</title><link>https://arxiv.org/abs/2602.20867</link><description>arXiv:2602.20867v1 Announce Type: new 
Abstract: Agentic systems increasingly rely on reusable procedural capabilities, \textit{a.k.a., agentic skills}, to execute long-horizon workflows reliably. These capabilities are callable modules that package procedural knowledge with explicit applicability conditions, execution policies, termination criteria, and reusable interfaces. Unlike one-off plans or atomic tool calls, skills operate (and often do well) across tasks.
  This paper maps the skill layer across the full lifecycle (discovery, practice, distillation, storage, composition, evaluation, and update) and introduces two complementary taxonomies. The first is a system-level set of \textbf{seven design patterns} capturing how skills are packaged and executed in practice, from metadata-driven progressive disclosure and executable code skills to self-evolving libraries and marketplace distribution. The second is an orthogonal \textbf{representation $\times$ scope} taxonomy describing what skills \emph{are} (natural language, code, policy, hybrid) and what environments they operate over (web, OS, software engineering, robotics).
  We analyze the security and governance implications of skill-based agents, covering supply-chain risks, prompt injection via skill payloads, and trust-tiered execution, grounded by a case study of the ClawHavoc campaign in which nearly 1{,}200 malicious skills infiltrated a major agent marketplace, exfiltrating API keys, cryptocurrency wallets, and browser credentials at scale. We further survey deterministic evaluation approaches, anchored by recent benchmark evidence that curated skills can substantially improve agent success rates while self-generated skills may degrade them. We conclude with open challenges toward robust, verifiable, and certifiable skills for real-world autonomous agents.</description><author>cs.CR updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.20867v1</guid></item><item><title>[arXiv-CR 2026] PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring</title><link>https://arxiv.org/abs/2602.20717</link><description>arXiv:2602.20717v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.
  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities.</description><author>cs.CR updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.20717v1</guid></item><item><title>[arXiv-CR 2026] MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models</title><link>https://arxiv.org/abs/2505.11963</link><description>arXiv:2505.11963v3 Announce Type: replace 
Abstract: Hardware security verification is a challenging and time-consuming task. Design engineers may use formal verification, linting, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected. Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools. We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning. MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. It delegates tasks to validate the security policy to individual executor agents. Each executor agent carries out its assigned task using a particular strategy. Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. MARVEL includes executor agents that leverage formal tools, linters, simulation tests, LLM-based detection schemes, and static analysis-based checks. We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition. We find that of the 51 issues reported by MARVEL, 19 are valid security vulnerabilities, 14 are concrete warnings, and 18 are hallucinated reports.</description><author>cs.CR updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.11963v3</guid></item><item><title>[arXiv-SE 2026] Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs</title><link>https://arxiv.org/abs/2602.20799</link><description>arXiv:2602.20799v1 Announce Type: new 
Abstract: In the context of newly release software frameworks, large language models (LLMs) often exhibit poor performance and a high rate of hallucination, as they are not exposed to such environments during training. Although inference-time augmentation techniques such as retrieval-augmented generation (RAG) can partially mitigate hallucinations, knowledge injection through prompting alone is insufficient to enable models to fully understand the intrinsic relationships among different components of a codebase, or to reason about the correct compositions and apply. Although explicit knowledge injection can be achieved through post-training, compared with public code domains, unseen codebases typically provide only source code and lack large volumes of high-quality, usage-oriented code that can be directly leveraged as training data. Consequently, existing data synthesis approaches are insufficient to adequately capture unseen codebases usage scenarios when restricted to source code alone. To address these challenges, we propose UCD-Training, a two-stage training framework for reasoning-aware data synthesis grounded in a code graph constructed from unseen codebases. UCD-Training first parses the source code to build a code graph, then conducts dependency-preserving continued pretraining (CPT) using file-level dependency data, followed by graph-grounded supervised fine-tuning (SFT) on three types of synthesized data augmented with explicit reasoning traces: (1) single-hop relation reasoning data, (2) compositional API reasoning data, and (3) codebase utilization data. We further introduce a new benchmark, UnseenCodeBench, for code generation on unseen codebases and conduct comprehensive experiments across multiple codebases.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.20799v1</guid></item><item><title>[arXiv-SE 2026] QSolver: A Quantum Constraint Solver</title><link>https://arxiv.org/abs/2602.20171</link><description>arXiv:2602.20171v1 Announce Type: cross 
Abstract: With the growing interest in quantum programs, ensuring their correctness is a fundamental challenge. Although constraint-solving techniques can overcome some limitations of traditional testing and verification, they have not yet been sufficiently explored in the context of quantum programs. To address this gap, we present QSolver, the first quantum constraint solver. QSolver provides a structured framework for handling five types of quantum constraints and incorporates an automated assertion generation module to verify quantum states. QSolver transforms quantum programs and multi-moment constraints into symbolic representations, and utilizes an SMT solver to obtain quantum states that satisfy these constraints. To validate the correctness of the generated input states, QSolver automatically generates assertion programs corresponding to each constraint. Experimental results show that QSolver efficiently processes commonly used quantum gates and demonstrates good scalability across quantum programs of different sizes.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.20171v1</guid></item><item><title>[arXiv-SE 2026] MigrateLib: a tool for end-to-end Python library migration</title><link>https://arxiv.org/abs/2510.08810</link><description>arXiv:2510.08810v3 Announce Type: replace 
Abstract: Library migration is the process of replacing a library with a similar one in a software project. Manual library migration is time consuming and error prone, as it requires developers to understand the Application Programming Interfaces (API) of both libraries, map equivalent APIs, and perform the necessary code transformations. Due to the difficulty of the library migration process, most of the existing automated techniques and tooling stop at the API mapping stage or support a limited set of libraries and code transformations. In this paper, we develop an end-to-end solution that can automatically migrate code between any arbitrary pair of Python libraries that provide similar functionality. Due to the promising capabilities of Large Language Models (LLMs) in code generation and transformation, we use LLMs as the primary engine for migration. Before building the tool, we first study the capabilities of LLMs for library migration on a benchmark of 321 real-world library migrations. We find that LLMs can effectively perform library migration, but some post-processing steps can further improve the performance. Based on this, we develop MigrateLib, a command line application that combines the power of LLMs, static analysis, and dynamic analysis to provide accurate library migration. We evaluate MigrateLib on 717 real-world Python applications that are not from our benchmark. We find that MigrateLib can migrate 32% of the migrations with complete correctness. Of the remaining migrations, only 14% of the migration-related changes are left for developers to fix for more than half of the projects.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.08810v3</guid></item><item><title>[arXiv-SE 2026] SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training</title><link>https://arxiv.org/abs/2602.03411</link><description>arXiv:2602.03411v2 Announce Type: replace 
Abstract: In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03411v2</guid></item><item><title>[arXiv-AI 2026] LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification</title><link>https://arxiv.org/abs/2602.21044</link><description>arXiv:2602.21044v1 Announce Type: new 
Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.</description><author>cs.AI updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.21044v1</guid></item><item><title>[arXiv-AI 2026] ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling</title><link>https://arxiv.org/abs/2602.20166</link><description>arXiv:2602.20166v1 Announce Type: cross 
Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.</description><author>cs.AI updates on arXiv.org</author><pubDate>Wed, 25 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.20166v1</guid></item><item><title>[arXiv-CR 2026] ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance</title><link>https://arxiv.org/abs/2602.09548</link><description>arXiv:2602.09548v2 Announce Type: replace 
Abstract: Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches.
  Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture.
  We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09548v2</guid></item><item><title>[arXiv-CR 2026] Watermarking LLM Agent Trajectories</title><link>https://arxiv.org/abs/2602.18700</link><description>arXiv:2602.18700v1 Announce Type: new 
Abstract: LLM agents rely heavily on high-quality trajectory data to guide their problem-solving behaviors, yet producing such data requires substantial task design, high-capacity model generation, and manual filtering. Despite the high cost of creating these datasets, existing literature has overlooked copyright protection for LLM agent trajectories. This gap leaves creators vulnerable to data theft and makes it difficult to trace misuse or enforce ownership rights. This paper introduces ActHook, the first watermarking method tailored for agent trajectory datasets. Inspired by hook mechanisms in software engineering, ActHook embeds hook actions that are activated by a secret input key and do not alter the original task outcome. Like software execution, LLM agents operate sequentially, allowing hook actions to be inserted at decision points without disrupting task flow. When the activation key is present, an LLM agent trained on watermarked trajectories can produce these hook actions at a significantly higher rate, enabling reliable black-box detection. Experiments on mathematical reasoning, web searching, and software engineering agents show that ActHook achieves an average detection AUC of 94.3 on Qwen-2.5-Coder-7B while incurring negligible performance degradation.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18700v1</guid></item><item><title>[arXiv-CR 2026] LLM Scalability Risk for Agentic-AI and Model Supply Chain Security</title><link>https://arxiv.org/abs/2602.19021</link><description>arXiv:2602.19021v1 Announce Type: new 
Abstract: Large Language Models (LLMs) &amp; Generative AI are transforming cybersecurity, enabling both advanced defenses and new attacks. Organizations now use LLMs for threat detection, code review, and DevSecOps automation, while adversaries leverage them to produce malwares and run targeted social-engineering campaigns. This paper presents a unified analysis integrating offensive and defensive perspectives on GenAI-driven cybersecurity. Drawing on 70 academic, industry, and policy sources, it analyzes the rise of AI-facilitated threats and its implications for global security to ground necessity for scalable defensive mechanisms. We introduce two primary contributions: the LLM Scalability Risk Index (LSRI), a parametric framework to stress-test operational risks when deploying LLMs in security-critical environments &amp; a model-supply-chain framework establishing a verifiable root of trust throughout model lifecycle. We also synthesize defense strategies from platforms like Google Play Protect, Microsoft Security Copilot and outline a governance roadmap for secure, large-scale LLM deployment.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.19021v1</guid></item><item><title>[arXiv-CR 2026] Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection</title><link>https://arxiv.org/abs/2602.19025</link><description>arXiv:2602.19025v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.19025v1</guid></item><item><title>[arXiv-CR 2026] Predicting known Vulnerabilities from Attack News: A Transformer-Based Approach</title><link>https://arxiv.org/abs/2602.19606</link><description>arXiv:2602.19606v1 Announce Type: new 
Abstract: Identifying the vulnerabilities exploited during cyberattacks is essential for enabling timely responses and effective mitigation in software security. This paper directly examines the process of predicting software vulnerabilities, specifically Common Vulnerabilities and Exposures (CVEs), from unstructured descriptions of attacks reported in cybersecurity news articles. We propose a semantic similarity-based approach utilizing the multi-qa-mpnet-base-dot-v1 (MPNet) sentence transformer model to generate a ranked list of the most likely CVEs corresponding to each news report. To assess the accuracy of the predicted vulnerabilities, we implement four complementary validation methods: filtering predictions based on similarity thresholds, conducting manual validation, performing semantic comparisons with the first vulnerability explicitly mentioned in each report, and comparing against all CVEs referenced within the report. Experimental results, drawn from a dataset of 100 SecurityWeek news articles, demonstrate that the model attains a precision of 81 percent when employing threshold-based filtering. Manual evaluations report that 70 percent of the predictions are relevant, while comparisons with the initially mentioned CVEs reveal agreement rates of 80 percent with the first listed vulnerability and 78 percent across all referenced CVEs. In 57 percent of the news reports analyzed, at least one predicted vulnerability precisely matched a CVE-ID mentioned in the article. These findings underscore the model's potential to facilitate automated vulnerability identification from real-world cyberattack news reports.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.19606v1</guid></item><item><title>[arXiv-CR 2026] FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing</title><link>https://arxiv.org/abs/2602.19490</link><description>arXiv:2602.19490v1 Announce Type: cross 
Abstract: Traditional database fuzzing techniques primarily focus on syntactic correctness and general SQL structures, leaving critical yet obscure DBMS features, such as system-level modes (e.g., GTID), programmatic constructs (e.g., PROCEDURE), advanced process commands (e.g., KILL), largely underexplored. Although rarely triggered by typical inputs, these features can lead to severe crashes or security issues when executed under edge-case conditions. In this paper, we present FuzzySQL, a novel LLM-powered adaptive fuzzing framework designed to uncover subtle vulnerabilities in DBMS special features. FuzzySQL combines grammar-guided SQL generation with logic-shifting progressive mutation, a novel technique that explores alternative control paths by negating conditions and restructuring execution logic, synthesizing structurally and semantically diverse test cases. To further ensure deeper execution coverage of the back end, FuzzySQL employs a hybrid error repair pipeline that unifies rule-based patching with LLM-driven semantic repair, enabling automatic correction of syntactic and context-sensitive failures. We evaluate FuzzySQL across multiple DBMSs, including MySQL, MariaDB, SQLite, PostgreSQL and Clickhouse, uncovering 37 vulnerabilities, 7 of which are tied to under-tested DBMS special features. As of this writing, 29 cases have been confirmed with 9 assigned CVE identifiers, 14 already fixed by vendors, and additional vulnerabilities scheduled to be patched in upcoming releases. Our results highlight the limitations of conventional fuzzers in semantic feature coverage and demonstrate the potential of LLM-based fuzzing to discover deeply hidden bugs in complex database systems.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.19490v1</guid></item><item><title>[arXiv-SE 2026] Debug2Fix: Supercharging Coding Agents with Interactive Debugging Capabilities</title><link>https://arxiv.org/abs/2602.18571</link><description>arXiv:2602.18571v1 Announce Type: new 
Abstract: While significant progress has been made in automating various aspects of software development through coding agents, there is still significant room for improvement in their bug fixing capabilities. Debugging and investigation of runtime behavior remains largely a manual, developer-driven process. Popular coding agents typically rely on either static analysis of the code or iterative test-fix cycles, which is akin to trial and error debugging. We posit that there is a wealth of rich runtime information that developers routinely access while debugging code, which agents are currently deprived of due to design limitations. Despite how prevalent debuggers are in modern IDEs and command-line tools, they have surprisingly not made their way into coding agents. In this work, we introduce Debug2Fix, a novel framework that incorporates interactive debugging as a core component of a software engineering agent via a subagent architecture. We incorporate debuggers for Java and Python into our agent framework and evaluate against GitBug-Java and SWE-Bench-Live and achieve &gt;20% improvement in performance compared to the baseline for certain models. Furthermore, using our framework, we're able to make weaker models like GPT-5 and Claude Haiku 4.5 match or exceed the performances of stronger models like Claude Sonnet 4.5, showing that better tool design is often just as important as switching to a more expensive model. Finally, we conduct systematic ablations demonstrating the importance of both the subagent architecture and debugger integration.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18571v1</guid></item><item><title>[arXiv-SE 2026] Efficient Dynamic Test Case Generation for Path-Based Coverage Criteria</title><link>https://arxiv.org/abs/2602.18768</link><description>arXiv:2602.18768v1 Announce Type: new 
Abstract: We present a novel approach to test-case generation that satisfies four white-box, path-based coverage criteria: Prime Path, Simple Cycle, Simple Path, and Edge-Acyclic Path. Our method builds on a modified version of Johnson algorithm and enables test cases to be generated incrementally and on demand, rather than requiring the entire test suite to be computed upfront. This streaming capability represents a substantial advancement over existing approaches, as it allows testers to begin executing and refining tests immediately, thereby significantly improving the efficiency of test design. Our solution is inherently memory efficient, as it does not store all discovered coverage items; instead, it retains only the minimal set of paths required to generate subsequent coverage items on the fly. As a result, the approach scales to arbitrarily large graphs. In addition, the algorithm gives testers explicit control over the size of the generated test suite by allowing them to restrict the number of cycles permitted in a test path. The approach is grounded in new theoretical insights, most notably a novel characterization of prime paths in terms of the strongly connected components of control-flow graphs. We complement these theoretical contributions with a practical implementation and a comprehensive empirical evaluation. The results demonstrate that our method not only outperforms existing techniques in terms of execution time and memory consumption, but also provides testers with a more flexible and efficient tool for achieving high coverage while substantially reducing test design overhead.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18768v1</guid></item><item><title>[arXiv-SE 2026] Narrowing the Complexity Gap in the Evaluation of Large Language Models</title><link>https://arxiv.org/abs/2602.18928</link><description>arXiv:2602.18928v1 Announce Type: new 
Abstract: Evaluating Large Language Models (LLMs) with respect to real-world code complexity is essential. Otherwise, there is a risk of overestimating LLMs' programming abilities based on simplistic benchmarks, only to be disappointed when using them in real-world settings. Recently, researchers explored the construction of more realistic benchmarks by mining or augmenting open-source repositories. Such solutions are usually task-specific. Data quality control from real-world projects can also be time-consuming and error-prone. More importantly, evaluating LLMs on fixed benchmark problems is subject to data contamination and overfitting. We propose GeneBench, an automated technique to add real-world complexities to any programming benchmark. GeneBench leverages a multi-objective optimization to increase the complexity of programming problems while maintaining the readability of code similar to real-world programs. Transforming four widely-used programming benchmarks using GeneBench and evaluating 13 LLMs (including two reasoning LLMs) on them shows a notable performance drop across all programming tasks (14.9%-60.5%, avg=35.2%), demonstrating LLMs' struggle under real-world complexities. The struggle persists even when LLMs are few-shot prompted or fine-tuned with examples from different versions of GeneBench, demonstrating the challenging nature of the problems. Finally, we show that the performance of the studied LLMs in bug repair is similar under GeneBench and SWE-Bench. This, along with the consistent reproduction of performance drop of all studied LLMs across four tasks under different versions of GeneBench, makes the technique suitable to evaluate LLMs without costly construction of real-world benchmarks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18928v1</guid></item><item><title>[arXiv-SE 2026] Multi-CoLoR: Context-Aware Localization and Reasoning across Multi-Language Codebases</title><link>https://arxiv.org/abs/2602.19407</link><description>arXiv:2602.19407v1 Announce Type: new 
Abstract: Large language models demonstrate strong capabilities in code generation but struggle to navigate complex, multi-language repositories to locate relevant code. Effective code localization requires understanding both organizational context (e.g., historical issue-fix patterns) and structural relationships within heterogeneous codebases. Existing methods either (i) focus narrowly on single-language benchmarks, (ii) retrieve code across languages via shallow textual similarity, or (iii) assume no prior context. We present Multi-CoLoR, a framework for Context-aware Localization and Reasoning across Multi-Language codebases, which integrates organizational knowledge retrieval with graph-based reasoning to traverse complex software ecosystems. Multi-CoLoR operates in two stages: (i) a similar issue context (SIC) module retrieves semantically and organizationally related historical issues to prune the search space, and (ii) a code graph traversal agent (an extended version of LocAgent, a state-of-the-art localization framework) performs structural reasoning within C++ and QML codebases. Evaluations on a real-world enterprise dataset show that incorporating SIC reduces the search space and improves localization accuracy, and graph-based reasoning generalizes effectively beyond Python-only repositories. Combined, Multi-CoLoR improves Acc@5 over both lexical and graph-based baselines while reducing tool calls on an AMD codebase.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.19407v1</guid></item><item><title>[arXiv-SE 2026] Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering</title><link>https://arxiv.org/abs/2602.19614</link><description>arXiv:2602.19614v1 Announce Type: new 
Abstract: The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic ("big-bang") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.19614v1</guid></item><item><title>[arXiv-SE 2026] Computational Complexity of Edge Coverage Problem for Constrained Control Flow Graphs</title><link>https://arxiv.org/abs/2602.18774</link><description>arXiv:2602.18774v1 Announce Type: cross 
Abstract: The article studies edge coverage for control flow graphs extended with explicit constraints. Achieving a given level of white-box coverage for a given code is a classic problem in software testing. We focus on designing test sets that achieve edge coverage \textit{while respecting additional constraints} between vertices. The paper analyzes how such constraints affect both the feasibility and computational complexity of edge coverage.
  The paper discusses five types of constraints. POSITIVE constraints require at least one test path where a given vertex precedes another. NEGATIVE constraints forbid any such test path. ONCE constraints require exactly one test path with a single occurrence of one vertex before another. MAX ONCE constraints allow such precedence in at most one test path. ALWAYS constraints require every test path containing a given vertex to also contain another vertex later on the same path. Each type models a different test requirement, such as mandatory flows, semantic exclusions, or execution cost limits.
  We investigate the computational complexity of finding a test set that achieves edge coverage and respects a given set of constraints. For POSITIVE constraints, the existence of an edge covering test set is decidable in polynomial time by extending standard edge coverage constructions with additional paths for each constraint. For NEGATIVE, MAX ONCE, ONCE, and ALWAYS constraints, the decision problem is NP-complete. The proofs rely on polynomial reductions from variants of SAT. The NP-completeness results hold even for restricted graph classes, including acyclic graphs, for all these four constraints.
  Finally, we study the fixed-parameter tractability of the NEGATIVE constraint. Although the general problem is NP-complete, the paper presents an FPT algorithm with respect to the number of constraints.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18774v1</guid></item><item><title>[arXiv-SE 2026] LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM</title><link>https://arxiv.org/abs/2512.01356</link><description>arXiv:2512.01356v2 Announce Type: replace 
Abstract: Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.01356v2</guid></item><item><title>[arXiv-SE 2026] TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation</title><link>https://arxiv.org/abs/2602.10471</link><description>arXiv:2602.10471v2 Announce Type: replace 
Abstract: Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.10471v2</guid></item><item><title>[arXiv-AI 2026] Adaptive Collaboration of Arena-Based Argumentative LLMs for Explainable and Contestable Legal Reasoning</title><link>https://arxiv.org/abs/2602.18916</link><description>arXiv:2602.18916v1 Announce Type: cross 
Abstract: Legal reasoning requires not only high accuracy but also the ability to justify decisions through verifiable and contestable arguments. However, existing Large Language Model (LLM) approaches, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), often produce unstructured explanations that lack a formal mechanism for verification or user intervention. To address this limitation, we propose Adaptive Collaboration of Argumentative LLMs (ACAL), a neuro-symbolic framework that integrates adaptive multi-agent collaboration with an Arena-based Quantitative Bipolar Argumentation Framework (A-QBAF). ACAL dynamically deploys expert agent teams to construct arguments, employs a clash resolution mechanism to adjudicate conflicting claims, and utilizes uncertainty-aware escalation for borderline cases. Crucially, our framework supports a Human-in-the-Loop (HITL) contestability workflow, enabling users to directly audit and modify the underlying reasoning graph to influence the final judgment. Empirical evaluations on the LegalBench benchmark demonstrate that ACAL outperforms strong baselines across Gemini-2.5-Flash-Lite and Gemini-2.5-Flash architectures, effectively balancing efficient predictive performance with structured transparency and contestability. Our implementation is available at: https://github.com/loc110504/ACAL.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18916v1</guid></item><item><title>[arXiv-AI 2026] A Secure and Private Distributed Bayesian Federated Learning Design</title><link>https://arxiv.org/abs/2602.20003</link><description>arXiv:2602.20003v1 Announce Type: cross 
Abstract: Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.20003v1</guid></item><item><title>[arXiv-AI 2026] Ambig-SWE: Interactive Agents to Overcome Underspecificity in Software Engineering</title><link>https://arxiv.org/abs/2502.13069</link><description>arXiv:2502.13069v3 Announce Type: replace 
Abstract: AI agents are increasingly being deployed to automate tasks, often based on underspecified user instructions. Making unwarranted assumptions to compensate for the missing information and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle underspecified instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) detecting underspecificity, (b) asking targeted clarification questions, and (c) leveraging the interaction to improve performance in underspecified scenarios. We introduce Ambig-SWE, an underspecified variant of SWE-Bench Verified, specifically designed to evaluate agent behavior under ambiguity and interaction. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user leading to significant improvements in performance, up to 74% over the non-interactive settings, underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle missing information in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2502.13069v3</guid></item><item><title>[arXiv-AI 2026] AgentCgroup: Understanding and Controlling OS Resources of AI Agents</title><link>https://arxiv.org/abs/2602.09345</link><description>arXiv:2602.09345v2 Announce Type: replace-cross 
Abstract: AI agents are increasingly deployed in multi-tenant cloud environments, where they execute diverse tool calls within sandboxed containers, each call with distinct resource demands and rapid fluctuations. We present a systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Our measurements reveal that (1) OS-level execution (tool calls, container and agent initialization) accounts for 56-74% of end-to-end task latency; (2) memory, not CPU, is the concurrency bottleneck; (3) memory spikes are tool-call-driven with a up to 15.4x peak-to-average ratio; and (4) resource demands are highly unpredictable across tasks, runs, and models. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three mismatches in existing resource controls: a granularity mismatch (container-level policies vs. tool-call-level dynamics), a responsiveness mismatch (user-space reaction vs. sub-second unpredictable bursts), and an adaptability mismatch (history-based prediction vs. non-deterministic stateful execution). We propose AgentCgroup, an intent-driven eBPF-based resource controller that exploits agents ability to declare resource needs and reconstruct execution strategies, using hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste. AgentCgroup is open-source at https://github.com/eunomia-bpf/agentcgroup</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 24 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09345v2</guid></item><item><title>[arXiv-CR 2026] AndroWasm: an Empirical Study on Android Malware Obfuscation through WebAssembly</title><link>https://arxiv.org/abs/2602.18082</link><description>arXiv:2602.18082v1 Announce Type: new 
Abstract: In recent years, stealthy Android malware has increasingly adopted sophisticated techniques to bypass automatic detection mechanisms and harden manual analysis. Adversaries typically rely on obfuscation, anti-repacking, steganography, poisoning, and evasion techniques to AI-based tools, and in-memory execution to conceal malicious functionality.
  In this paper, we investigate WebAssembly (Wasm) as a novel technique for hiding malicious payloads and evading traditional static analysis and signature-matching mechanisms. While Wasm is typically employed to render specific gaming activities and interact with the native components in web browsers, we provide an in-depth analysis on the mechanisms Android may employ to include Wasm modules in its execution pipeline. Additionally, we provide Proofs-of-Concept to demonstrate a threat model in which an attacker embeds and executes malicious routines, effectively bypassing IoC detection by industrial state-of-the-art tools, like VirusTotal and MobSF.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 23 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18082v1</guid></item><item><title>[arXiv-SE 2026] Automated LLM-Based Accessibility Remediation: From Conventional Websites to Angular Single-Page Applications</title><link>https://arxiv.org/abs/2602.17887</link><description>arXiv:2602.17887v1 Announce Type: new 
Abstract: Web accessibility remains an unresolved issue for a large part of the web content. There are many tools to detect errors automatically, but fixing those issues is still mostly a manual, slow, and costly process in which it is easy for developers to overlook specific details. The situation becomes even more complex with modern Single-Page Applications (SPAs), whose dynamic nature makes traditional static analysis approaches inadequate. This work proposes a system that aims to address this challenge by using Large Language Models (LLMs) to automate accessibility fixes. The proposal presents a modular workflow applicable to both static websites and complex Angular projects. The framework actively implements corrections within the DOM of static web pages or the source code of SPAs. The system was tested on 12 static websites and 6 open-source Angular projects, fixing 80% of the accessibility issues on public websites and 86% of the issues on Angular applications. Our proposal also generates meaningful visual descriptions for images while preserving the application's design and stability. This work contributes to ensuring that accessibility stops being a technical debt deferred to the future and becomes a natural part of everyday development workflows.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 23 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.17887v1</guid></item><item><title>[arXiv-SE 2026] VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean</title><link>https://arxiv.org/abs/2602.18307</link><description>arXiv:2602.18307v1 Announce Type: new 
Abstract: Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 23 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.18307v1</guid></item><item><title>[arXiv-SE 2026] Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters</title><link>https://arxiv.org/abs/2602.17697</link><description>arXiv:2602.17697v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 23 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.17697v1</guid></item><item><title>[arXiv-SE 2026] HookLens: Visual Analytics for Understanding React Hooks Structures</title><link>https://arxiv.org/abs/2602.17891</link><description>arXiv:2602.17891v1 Announce Type: cross 
Abstract: Maintaining and refactoring React web applications is challenging, as React code often becomes complex due to its core API called Hooks. For example, Hooks often lead developers to create complex dependencies among components, making code behavior unpredictable and reducing maintainability, i.e., anti-patterns. To address this challenge, we present HookLens, an interactive visual analytics system that helps developers understand howHooks define dependencies and data flows between components. Informed by an iterative design process with experienced React developers, HookLens supports users to efficiently understand the structure and dependencies between components and to identify anti-patterns. A quantitative user study with 12 React developers demonstrates that HookLens significantly improves participants' accuracy in detecting anti-patterns compared to conventional code editors. Moreover, a comparative study with state-of-the-art LLM-based coding assistants confirms that these improvements even surpass the capabilities of such coding assistants on the same task.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 23 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.17891v1</guid></item><item><title>[arXiv-AI 2026] MultiVer: Zero-Shot Multi-Agent Vulnerability Detection</title><link>https://arxiv.org/abs/2602.17875</link><description>arXiv:2602.17875v1 Announce Type: cross 
Abstract: We present MultiVer, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. A four-agent ensemble (security, correctness, performance, style) with union voting achieves 82.7% recall on PyVul, exceeding fine-tuned GPT-3.5 (81.3%) by 1.4 percentage points -- the first zeroshot system to surpass fine-tuned performance on this benchmark. On SecurityEval, the same architecture achieves 91.7% detection rate, matching specialized systems. The recall improvement comes at a precision cost: 48.8% precision versus 63.9% for fine-tuned baselines, yielding 61.4% F1. Ablation experiments isolate component contributions: the multi-agent ensemble adds 17 percentage points recall over single-agent security analysis. These results demonstrate that for security applications where false negatives are costlier than false positives, zero-shot multi-agent ensembles can match and exceed fine-tuned models on the metric that matters most.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 23 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.17875v1</guid></item><item><title>[RAID 2026] Malware and Vulnerability Analysis using Graph-synchronized Language Model.</title><link>https://doi.org/10.1109/RAID67961.2025.00046</link><description>Authors: Paventhan Vivekanandan, Alexander Shroyer, Martin Swany
Venue: RAID
Year: 2025</description><author>dblp: new volumes for streams/conf/raid</author><pubDate>Sat, 21 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/raid/VivekanandanSS25</guid></item><item><title>[arXiv-CR 2026] Favia: Forensic Agent for Vulnerability-fix Identification and Analysis</title><link>https://arxiv.org/abs/2602.12500</link><description>arXiv:2602.12500v1 Announce Type: cross 
Abstract: Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 16 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.12500v1</guid></item><item><title>[arXiv-SE 2026] FuncDroid: Towards Inter-Functional Flows for Comprehensive Mobile App GUI Testing</title><link>https://arxiv.org/abs/2602.12834</link><description>arXiv:2602.12834v1 Announce Type: new 
Abstract: As mobile application (app) functionalities grow increasingly complex and their iterations accelerate, ensuring high reliability presents significant challenges. While functionality-oriented GUI testing has attracted growing research attention, existing approaches largely overlook interactions across functionalities, making them ineffective at uncovering deep bugs hidden in inter-functional behaviors. To fill this gap, we first design a Functional Flow Graph (FFG), a behavioral model that explicitly captures an app's functional units and their inter-functional interactions. Based on the FFG, we further introduce an inter-functional-flow-oriented GUI testing approach with the dual goals of precise model construction and deep bug detection. This approach is realized through a long-short-term-view-guided testing process. By combining two complementary test-generation views, it can adaptively refine functional boundaries and systematically explore inter-functional flows under diverse triggering conditions. We implement our approach in a tool called FuncDroid, and evaluate it on two benchmarks: (1) a widely-used open-source benchmark with 50 reproducible crash bugs and (2) a diverse set of 52 popular commercial apps. Experimental results demonstrate that FuncDroid significantly outperforms state-of-the-art baselines in both coverage (+28%) and bug detection number (+107%). Moreover, FuncDroid successfully uncovers 18 previously unknown non-crash functional bugs in commercial apps, confirming its practical effectiveness.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 16 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.12834v1</guid></item><item><title>[arXiv-AI 2026] EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</title><link>https://arxiv.org/abs/2508.11850</link><description>arXiv:2508.11850v2 Announce Type: replace 
Abstract: Integer programming (IP) is central to many combinatorial optimization tasks but remains challenging due to its NP-hard nature. A practical way to improve IP solvers is to manually design acceleration cuts, i.e., inequalities that speed up solving. However, this creative process requires deep expertise and has been difficult to automate. Our proposed framework, EvoCut, automates the generation of acceleration cuts at the symbolic modeling level: it reasons over a symbolic MILP model and a natural language description of the problem to discover a reusable set of acceleration cuts that can be used for each concrete instance of the model. EvoCut (i) initializes a population of candidate cuts via an initializer agent that uses an LLM, (ii) empirically screens candidates on a small verification set by checking that reference solutions remain feasible and that at least one stored LP relaxation solution is cut off, and (iii) iteratively refines the population through evolutionary crossover and mutation agents. Compared to baseline MILP formulations solved with a fixed time budget, EvoCut reduces optimality gaps by up to $76\%$ and reaches target gaps up to $7.2$ times faster (shifted geometric mean speedup). Ablations show its robustness across different LLM backends and across solvers/cut settings. Code: https://github.com/milad1378yz/EvoCut.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 16 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.11850v2</guid></item><item><title>[arXiv-AI 2026] Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges</title><link>https://arxiv.org/abs/2510.23883</link><description>arXiv:2510.23883v2 Announce Type: replace 
Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with planning, tool use, memory, and autonomy, are emerging as powerful, flexible platforms for automation. Their ability to autonomously execute tasks across web, software, and physical environments creates new and amplified security risks, distinct from both traditional AI safety and conventional software security. This survey outlines a taxonomy of threats specific to agentic AI, reviews recent benchmarks and evaluation methodologies, and discusses defense strategies from both technical and governance perspectives. We synthesize current research and highlight open challenges, aiming to support the development of secure-by-design agent systems.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 16 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.23883v2</guid></item><item><title>[arXiv-AI 2026] Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge</title><link>https://arxiv.org/abs/2601.10485</link><description>arXiv:2601.10485v3 Announce Type: replace 
Abstract: Domain-specific knowledge graphs (DKGs) are critical yet often suffer from limited coverage compared to General Knowledge Graphs (GKGs). Existing tasks to enrich DKGs rely primarily on extracting knowledge from external unstructured data or completing KGs through internal reasoning, but the scope and quality of such integration remain limited. This highlights a critical gap: little systematic exploration has been conducted on how comprehensive, high-quality GKGs can be effectively leveraged to supplement DKGs.
  To address this gap, we propose a new and practical task: domain-specific knowledge graph fusion (DKGF), which aims to mine and integrate relevant facts from general knowledge graphs into domain-specific knowledge graphs to enhance their completeness and utility. Unlike previous research, this new task faces two key challenges: (1) high ambiguity of domain relevance, i.e., difficulty in determining whether knowledge from a GKG is truly relevant to the target domain , and (2) cross-domain knowledge granularity misalignment, i.e., GKG facts are typically abstract and coarse-grained, whereas DKGs frequently require more contextualized, fine-grained representations aligned with particular domain scenarios.
  To address these, we present ExeFuse, a neuro-symbolic framework based on a novel Fact-as-Program paradigm. ExeFuse treats fusion as an executable process, utilizing neuro-symbolic execution to infer logical relevance beyond surface similarity and employing target space grounding to calibrate granularity. We construct two new datasets to establish the first standardized evaluation suite for this task. Extensive experiments demonstrate that ExeFuse effectively overcomes domain barriers to achieve superior fusion performance.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 16 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10485v3</guid></item><item><title>[arXiv-CR 2026] TRACE: Timely Retrieval and Alignment for Cybersecurity Knowledge Graph Construction and Expansion</title><link>https://arxiv.org/abs/2602.11211</link><description>arXiv:2602.11211v1 Announce Type: new 
Abstract: The rapid evolution of cyber threats has highlighted significant gaps in security knowledge integration. Cybersecurity Knowledge Graphs (CKGs) relying on structured data inherently exhibit hysteresis, as the timely incorporation of rapidly evolving unstructured data remains limited, potentially leading to the omission of critical insights for risk analysis. To address these limitations, we introduce TRACE, a framework designed to integrate structured and unstructured cybersecurity data sources. TRACE integrates knowledge from 24 structured databases and 3 categories of unstructured data, including APT reports, papers, and repair notices. Leveraging Large Language Models (LLMs), TRACE facilitates efficient entity extraction and alignment, enabling continuous updates to the CKG. Evaluations demonstrate that TRACE achieves a 1.8x increase in node coverage compared to existing CKGs. TRACE attains the precision of 86.08%, the recall of 76.92%, and the F1 score of 81.24% in entity extraction, surpassing the best-known LLM-based baselines by 7.8%. Furthermore, our entity alignment methods effectively harmonize entities with existing knowledge structures, enhancing the integrity and utility of the CKG. With TRACE, threat hunters and attack analysts gain real-time, holistic insights into vulnerabilities, attack methods, and defense technologies.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11211v1</guid></item><item><title>[arXiv-CR 2026] Yaksha-Prashna: Understanding eBPF Bytecode Network Function Behavior</title><link>https://arxiv.org/abs/2602.11232</link><description>arXiv:2602.11232v1 Announce Type: new 
Abstract: Many cloud infrastructure organizations increasingly rely on third-party eBPF-based network functions for use cases like security, observability, and load balancing, so that not everyone requires a team of highly skilled eBPF experts. However, the network functions from third parties (e.g., F5, Palo Alto) are available in bytecode format to cloud operators, giving little or no understanding of their functional correctness and interaction with other network functions in a chain. Also, eBPF developers want to provide proof of functional correctness for their developed network functions without disclosing the source code to the operators. We design Yaksha-Prashna, a system that allows operators/developers to assert and query bytecode's conformance to its specification and dependencies on other bytecodes. Our work builds domain-specific models that enable us to employ scalable program analysis to extract and model eBPF programs. Using Yaksha-Prashna language, we express 24 properties on standard and non-standard eBPF-based network functions with 200-1000x speedup over the state-of-the-art work.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11232v1</guid></item><item><title>[arXiv-CR 2026] SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code</title><link>https://arxiv.org/abs/2602.11209</link><description>arXiv:2602.11209v1 Announce Type: cross 
Abstract: While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11209v1</guid></item><item><title>[arXiv-CR 2026] AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title><link>https://arxiv.org/abs/2508.20866</link><description>arXiv:2508.20866v5 Announce Type: replace 
Abstract: The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the need for reliable automated software vulnerability detection. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurately labeled datasets. Yet existing datasets either suffer from noisy labels, limited vulnerability coverage, or fail to reflect vulnerabilities as they occur in real-world software. This also limits large-scale benchmarking of such solutions. Automated vulnerability injection provides a way to address these limitations, but existing techniques remain limited in coverage, contextual fidelity, or injection success. In this paper, we present AVIATOR, the first AI-agentic vulnerability injection framework. AVIATOR decomposes vulnerability injection into a coordinated workflow of specialized AI agents, tool-based analysis, and iterative self-correction, explicitly mirroring expert reasoning. It integrates RAG and lightweight LoRA-based fine-tuning to produce realistic, category-specific vulnerabilities without relying on handcrafted patterns. Across three benchmarks, AVIATOR achieves high injection fidelity (91-95%) surpassing existing injection techniques in both accuracy and vulnerability coverage. When used for data augmentation to train deep learning-based vulnerability detection (DLVD) models, AVIATOR provides the strongest downstream gains in vulnerability detection. Across models and base datasets, AVIATOR improves average F1 scores by +22% over no augmentation, +25% over VGX, holding the prior best injection success rate, and +3% over VulScribeR, the prior state-of-the-art LLM-based injection model, with +7% higher recall and no precision loss. Its augmented data exhibits the lowest distributional distortion and scales efficiently with &lt;2% syntax rejection at 4.3x lower cost than VulScribeR.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.20866v5</guid></item><item><title>[arXiv-CR 2026] From CVE Entries to Verifiable Exploits: An Automated Multi-Agent Framework for Reproducing CVEs</title><link>https://arxiv.org/abs/2509.01835</link><description>arXiv:2509.01835v2 Announce Type: replace 
Abstract: High-quality datasets of real-world vulnerabilities and their corresponding verifiable exploits are crucial resources in software security research. Yet such resources remain scarce, as their creation demands intensive manual effort and deep security expertise. In this paper, we present CVE-GENIE, an automated, large language model (LLM)-based multi-agent framework designed to reproduce real-world vulnerabilities, provided in Common Vulnerabilities and Exposures (CVE) format, to enable creation of high-quality vulnerability datasets. Given a CVE entry as input, CVE-GENIE gathers the relevant resources of the CVE, automatically reconstructs the vulnerable environment, and (re)produces a verifiable exploit. Our systematic evaluation highlights the efficiency and robustness of CVE-GENIE's design and successfully reproduces approximately 51% (428 of 841) CVEs published in 2024-2025, complete with their verifiable exploits, at an average cost of $2.77 per CVE. Our pipeline offers a robust method to generate reproducible CVE benchmarks, valuable for diverse applications such as fuzzer evaluation, vulnerability patching, and assessing AI's security capabilities.</description><author>cs.CR updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.01835v2</guid></item><item><title>[arXiv-SE 2026] WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements</title><link>https://arxiv.org/abs/2602.11724</link><description>arXiv:2602.11724v1 Announce Type: new 
Abstract: Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.
  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11724v1</guid></item><item><title>[arXiv-SE 2026] Improving Code Generation via Small Language Model-as-a-judge</title><link>https://arxiv.org/abs/2602.11911</link><description>arXiv:2602.11911v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11911v1</guid></item><item><title>[arXiv-SE 2026] Studying Quality Improvements Recommended via Manual and Automated Code Review</title><link>https://arxiv.org/abs/2602.11925</link><description>arXiv:2602.11925v1 Announce Type: new 
Abstract: Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11925v1</guid></item><item><title>[arXiv-SE 2026] An Empirical Study of the Imbalance Issue in Software Vulnerability Detection</title><link>https://arxiv.org/abs/2602.12038</link><description>arXiv:2602.12038v1 Announce Type: new 
Abstract: Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.</description><author>cs.SE updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.12038v1</guid></item><item><title>[arXiv-AI 2026] AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems</title><link>https://arxiv.org/abs/2602.11510</link><description>arXiv:2602.11510v1 Announce Type: new 
Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 &gt; C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11510v1</guid></item><item><title>[arXiv-AI 2026] DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks' Developer Experience Through a Novel Relational Schema Mapping Task</title><link>https://arxiv.org/abs/2602.11198</link><description>arXiv:2602.11198v1 Announce Type: cross 
Abstract: Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomous retrieval of candidate frames and fine-grained linguistic reasoning over table names, columns, and relations. Using the Agent-as-a-Tool pattern, we implement identical agent logic across 10 frameworks and evaluate along two dimensions: (i) code complexity via static analysis, and (ii) AI-assistability -- the extent to which LLMs can autonomously generate correct, framework-specific code. Our results reveal a threefold complexity spectrum, with Pydantic AI and Agno requiring the least implementation overhead. For AI-assistability, structural alignment scores reliably proxy runtime success for frameworks with single canonical patterns, but overestimate correctness for multi-pattern frameworks. Agno emerges as the strongest overall performer, combining lowest complexity with highest structural alignment and 83% pass@1.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11198v1</guid></item><item><title>[arXiv-AI 2026] FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight</title><link>https://arxiv.org/abs/2602.11136</link><description>arXiv:2602.11136v2 Announce Type: replace 
Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.</description><author>cs.AI updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11136v2</guid></item><item><title>[arXiv-LG 2026] Evolutionary Generation of Multi-Agent Systems</title><link>https://arxiv.org/abs/2602.06511</link><description>arXiv:2602.06511v2 Announce Type: replace 
Abstract: Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06511v2</guid></item><item><title>[arXiv-CL 2026] Automatic Simplification of Common Vulnerabilities and Exposures Descriptions</title><link>https://arxiv.org/abs/2602.11982</link><description>arXiv:2602.11982v1 Announce Type: new 
Abstract: Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on investigating how large language models (LLMs) could be utilized in automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions. Automatic text simplification has been studied in several contexts, such as medical, scientific, and news texts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of cyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions, evaluated by two groups of cyber security experts in two survey rounds. We have found that while out-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation. Code and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification\_nmi.</description><author>cs.CL updates on arXiv.org</author><pubDate>Fri, 13 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.11982v1</guid></item><item><title>[TOSEM 2026] Integrating Path Selection for Symbolic Execution and Variable Selection for Constraint Solving</title><link>https://dl.acm.org/doi/abs/10.1145/3735552?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 3, Page 1-28, March 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Fri, 13 Feb 2026 02:34:27 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3735552?af=R</guid></item><item><title>[TOSEM 2026] When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair</title><link>https://dl.acm.org/doi/abs/10.1145/3733599?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 3, Page 1-46, March 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Fri, 13 Feb 2026 02:34:27 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3733599?af=R</guid></item><item><title>[TOSEM 2026] LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models</title><link>https://dl.acm.org/doi/abs/10.1145/3736406?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 3, Page 1-32, March 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Fri, 13 Feb 2026 02:34:27 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3736406?af=R</guid></item><item><title>[TOSEM 2026] CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences</title><link>https://dl.acm.org/doi/abs/10.1145/3736407?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 3, Page 1-36, March 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Fri, 13 Feb 2026 02:34:27 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3736407?af=R</guid></item><item><title>[arXiv-CR 2026] SecCodePRM: A Process Reward Model for Code Security</title><link>https://arxiv.org/abs/2602.10418</link><description>arXiv:2602.10418v1 Announce Type: new 
Abstract: Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.10418v1</guid></item><item><title>[arXiv-CR 2026] Authenticated Workflows: A Systems Approach to Protecting Agentic AI</title><link>https://arxiv.org/abs/2602.10465</link><description>arXiv:2602.10465v1 Announce Type: new 
Abstract: Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.10465v1</guid></item><item><title>[arXiv-CR 2026] Beyond Permissions: An Empirical Static Analysis of Privacy and Security Risks in Children-Oriented and General-Audience Mobile Apps for Gaming</title><link>https://arxiv.org/abs/2602.10877</link><description>arXiv:2602.10877v1 Announce Type: new 
Abstract: Mobile gaming applications (apps) have become increasingly pervasive, including a growing number of games designed for children. Despite their popularity, these apps often integrate complex analytics, advertising, and attribution infrastructures that may introduce privacy and security risks. Existing research has primarily focused on tracking behaviors or monetization models, leaving configuration-level privacy exposure and children-oriented apps underexplored. In this study, we conducted a comparative static analysis of Android mobile games to investigate privacy and security risks beyond permission usage. The analysis follows a three-phase methodology comprising (i) designing study protocol, (ii) Android Package Kit (APK) collection and static inspection, and (iii) data analysis. We examined permissions, manifest-level configuration properties (e.g., backup settings, cleartext network traffic, and exported components), and embedded third-party Software Development Kit (SDK) ecosystems across children-oriented and general-audience mobile games. The extracted indicators are synthesized into qualitative privacy-risk categories to support comparative reporting. The results showed that while children-oriented games often request fewer permissions, they frequently exhibit configuration-level risks and embed third-party tracking SDKs similar to general-audience games. Architectural and configuration decisions play a critical role in shaping privacy risks, particularly for apps targeting children. This study contributes a holistic static assessment of privacy exposure in mobile games and provides actionable insights for developers, platform providers, and researchers seeking to improve privacy-by-design practices in mobile applications.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.10877v1</guid></item><item><title>[arXiv-CR 2026] VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection</title><link>https://arxiv.org/abs/2602.10787</link><description>arXiv:2602.10787v1 Announce Type: cross 
Abstract: Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.10787v1</guid></item><item><title>[arXiv-CR 2026] SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models</title><link>https://arxiv.org/abs/2512.18542</link><description>arXiv:2512.18542v2 Announce Type: replace 
Abstract: AI coding assistants produce vulnerable code in 45\% of security-relevant scenarios~\cite{veracode2025}, yet no public training dataset teaches both traditional web security and AI/ML-specific defenses in a format suitable for instruction tuning.
  We present SecureCode, a production-grade dataset of 2,185 multi-turn security training examples spanning two domains: web application security (1,435 examples covering the OWASP Top 10 2021 across 11 languages and 9 frameworks, 100\% grounded in documented CVEs and security incidents) and AI/ML security (750 examples covering all 10 OWASP LLM Top 10 2025 categories across more than 40 frameworks, including LangChain, OpenAI, and Hugging Face). Every example follows a 4-turn conversational structure -- feature request; vulnerable and secure implementations with attack demonstrations; advanced probing; and defense-in-depth operational guidance -- designed for direct use in instruction tuning pipelines.
  Quality assurance combines automated structural validation with multi-agent review from seven specialist AI perspectives (more than 10{,}500 assessments) and an 8-phase remediation pipeline, producing a rubric-calibrated mean quality score of 93.8/100 ($\sigma = 0.93$) for the AI/ML component. Each example provides SIEM integration strategies, infrastructure hardening recommendations, and testing approaches using production frameworks.
  We release the unified dataset on Hugging Face with domain-specific loading configurations (web, aiml, default), alongside eight fine-tuned open-source models (3B--20B parameters, QLoRA), and an evaluation framework with four security-specific metrics. To our knowledge, SecureCode is the first public dataset that jointly provides OWASP Top 10 2021 web coverage and OWASP LLM Top 10 2025 AI/ML coverage in a unified conversational schema suitable for instruction tuning.</description><author>cs.CR updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.18542v2</guid></item><item><title>[arXiv-SE 2026] PALM: Path-aware LLM-based Test Generation with Comprehension</title><link>https://arxiv.org/abs/2506.19287</link><description>arXiv:2506.19287v2 Announce Type: replace 
Abstract: Symbolic execution is a widely used technique for test generation, offering systematic exploration of program paths through constraint solving. However, it is fundamentally constrained by the capability to model the target code, including library functions, in terms of symbolic constraints and by the capability of underlying constraint solvers. As a result, many paths involving complex features remain unanalyzed or insufficiently modeled. Recent advances in large language models (LLMs) have shown promise in generating diverse and valid test inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths and often fail to cover subtle corner cases. We observe that directly prompting an LLM with the full program leads to missed coverage of interesting paths. In this paper, we present PALM, a test generation system that combines symbolic path enumeration with LLM-assisted test generation. PALM statically enumerates possible paths through AST-level analysis and transforms each into an executable variant with embedded assertions that specify the target path. This avoids the need to translate path constraints into SMT formulas, by instead constructing program variants that the LLM can interpret. Importantly, PALM provides an interactive frontend that visualizes path coverage alongside generated tests, assembling tests based on the specific paths they exercise. A user study with 12 participants demonstrates that PALM's frontend helps users better understand path coverage and identify which paths are actually exercised by PALM-generated tests through verification and visualization of their path profiles.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.19287v2</guid></item><item><title>[arXiv-SE 2026] Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study</title><link>https://arxiv.org/abs/2602.07147</link><description>arXiv:2602.07147v2 Announce Type: replace 
Abstract: Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07147v2</guid></item><item><title>[arXiv-SE 2026] Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications</title><link>https://arxiv.org/abs/2602.08242</link><description>arXiv:2602.08242v2 Announce Type: replace 
Abstract: Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08242v2</guid></item><item><title>[arXiv-SE 2026] SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents</title><link>https://arxiv.org/abs/2602.09447</link><description>arXiv:2602.09447v2 Announce Type: replace 
Abstract: Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.</description><author>cs.SE updates on arXiv.org</author><pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09447v2</guid></item><item><title>[arXiv-CR 2026] One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning</title><link>https://arxiv.org/abs/2602.09182</link><description>arXiv:2602.09182v1 Announce Type: new 
Abstract: Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.</description><author>cs.CR updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09182v1</guid></item><item><title>[arXiv-CR 2026] QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery</title><link>https://arxiv.org/abs/2602.09774</link><description>arXiv:2602.09774v1 Announce Type: new 
Abstract: Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.</description><author>cs.CR updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09774v1</guid></item><item><title>[arXiv-SE 2026] QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs</title><link>https://arxiv.org/abs/2602.09942</link><description>arXiv:2602.09942v1 Announce Type: new 
Abstract: As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09942v1</guid></item><item><title>[arXiv-SE 2026] Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents</title><link>https://arxiv.org/abs/2602.09944</link><description>arXiv:2602.09944v1 Announce Type: new 
Abstract: Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09944v1</guid></item><item><title>[arXiv-SE 2026] Artisan: Agentic Artifact Evaluation</title><link>https://arxiv.org/abs/2602.10046</link><description>arXiv:2602.10046v1 Announce Type: new 
Abstract: Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.10046v1</guid></item><item><title>[arXiv-SE 2026] Can LLMs Find Bugs in Code? An Evaluation from Beginner Errors to Security Vulnerabilities in Python and C++</title><link>https://arxiv.org/abs/2508.16419</link><description>arXiv:2508.16419v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are increasingly embedded in software/application development, supporting tasks from code generation to debugging. Yet, their real-world effectiveness in detecting diverse software bugs, particularly complex, security-relevant vulnerabilities, remains underexplored. This study presents a systematic, empirical evaluation of these three leading LLMs using a benchmark of foundational programming errors, classic security flaws, and advanced, production-grade bugs in C++ and Python. The dataset integrates real code from SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated through local compilation and testing pipelines. A novel multi-stage, context-aware prompting protocol simulates realistic debugging scenarios, while a graded rubric measures detection accuracy, reasoning depth, and remediation quality. Our results show that all models excel at identifying syntactic and semantic issues in well-scoped code, making them promising for educational use and as first-pass reviewers in automated code auditing. Performance diminishes in scenarios involving complex security vulnerabilities and large-scale production code, with ChatGPT-4 and Claude 3 generally providing more nuanced contextual analyses than LLaMA 4. This highlights both the promise and the present constraints of LLMs in serving as reliable code analysis tools.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2508.16419v2</guid></item><item><title>[arXiv-SE 2026] Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation</title><link>https://arxiv.org/abs/2602.08146</link><description>arXiv:2602.08146v2 Announce Type: replace 
Abstract: Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants "hacking" the blind spots of T's current test suite, while T iteratively refines its test cases to "kill" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.</description><author>cs.SE updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08146v2</guid></item><item><title>[arXiv-AI 2026] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</title><link>https://arxiv.org/abs/2602.09443</link><description>arXiv:2602.09443v1 Announce Type: new 
Abstract: The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.</description><author>cs.AI updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.09443v1</guid></item><item><title>[arXiv-LG 2026] Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors</title><link>https://arxiv.org/abs/2512.22699</link><description>arXiv:2512.22699v3 Announce Type: replace 
Abstract: This paper presents a novel learning based framework for predicting power outages caused by extreme events. The proposed approach targets low-probability high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records from 2014 to 2024 with weather, socioeconomic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals patterns of community vulnerability and improves understanding of outage risk during extreme conditions. Four machine learning models are evaluated, including Random Forest (RF), Graph Neural Network (GNN), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM). Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves higher accuracy.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 11 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.22699v3</guid></item><item><title>[arXiv-CR 2026] Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model</title><link>https://arxiv.org/abs/2602.07422</link><description>arXiv:2602.07422v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07422v1</guid></item><item><title>[arXiv-CR 2026] LLMs + Security = Trouble</title><link>https://arxiv.org/abs/2602.08422</link><description>arXiv:2602.08422v1 Announce Type: new 
Abstract: We argue that when it comes to producing secure code with AI, the prevailing "fighting fire with fire" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.
  While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the "vibe coding" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.
  In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08422v1</guid></item><item><title>[arXiv-CR 2026] DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing</title><link>https://arxiv.org/abs/2602.08750</link><description>arXiv:2602.08750v1 Announce Type: new 
Abstract: The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.</description><author>cs.CR updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08750v1</guid></item><item><title>[arXiv-SE 2026] Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability</title><link>https://arxiv.org/abs/2602.07071</link><description>arXiv:2602.07071v1 Announce Type: new 
Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07071v1</guid></item><item><title>[arXiv-SE 2026] A Course on the Introduction to Quantum Software Engineering: Experience Report</title><link>https://arxiv.org/abs/2602.07589</link><description>arXiv:2602.07589v1 Announce Type: new 
Abstract: Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.
  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.
  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07589v1</guid></item><item><title>[arXiv-SE 2026] Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents</title><link>https://arxiv.org/abs/2602.07900</link><description>arXiv:2602.07900v1 Announce Type: new 
Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07900v1</guid></item><item><title>[arXiv-SE 2026] Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects</title><link>https://arxiv.org/abs/2602.08166</link><description>arXiv:2602.08166v1 Announce Type: new 
Abstract: Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \emph{extractors}, and supports \emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08166v1</guid></item><item><title>[arXiv-SE 2026] Specification Vibing for Automated Program Repair</title><link>https://arxiv.org/abs/2602.08263</link><description>arXiv:2602.08263v1 Announce Type: new 
Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.08263v1</guid></item><item><title>[arXiv-SE 2026] Prometheus: Towards Long-Horizon Codebase Navigation for Repository-Level Problem Solving</title><link>https://arxiv.org/abs/2507.19942</link><description>arXiv:2507.19942v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in automating software engineering tasks, spurring the emergence of coding agents that scaffold LLMs with external tools to resolve repository-level problems. However, existing agents still struggle to navigate large-scale codebases, as the Needle-in-a-Haystack problem persists even with million-token context windows, where relevant evidence is often overwhelmed by large volumes of irrelevant code and documentation. Prior codebase navigation approaches, including embedding-based retrieval, file-system exploration, and graph-based retrieval, address parts of this challenge but fail to capture the temporal continuity of agent reasoning, rendering agents stateless and causing repeated repository traversals that hinder scalable planning and reasoning. To address these limitations, we present Prometheus, a memory-centric coding agent framework for long-horizon codebase navigation. Prometheus represents the repository as a unified knowledge graph to encode semantic dependencies and employs a context engine augmented with working memory that retains and reuses previously explored contexts to ensure continuity across reasoning steps. Built upon this engine, Prometheus integrates memory-enhanced navigation into a multi-agent system for automated issue resolution, encompassing issue classification, bug reproduction, patch generation, and verification. Comprehensive experiments are conducted on two widely used issue resolution benchmarks, i.e., SWE-bench Verified and SWE-PolyBench Verified. Powered by GPT-5, Prometheus achieves state-of-the-art performance with 74.4% and 33.8% resolution rates on the two benchmarks, ranking Top-6 and Top-1 among open-source agent systems, respectively. Our data and code are available at https://github.com/EuniAI/Prometheus.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.19942v2</guid></item><item><title>[arXiv-SE 2026] A Dual-Loop Agent Framework for Automated Vulnerability Reproduction</title><link>https://arxiv.org/abs/2602.05721</link><description>arXiv:2602.05721v2 Announce Type: replace 
Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose CVE2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the Tactical Loop for code-level refinement, while the Strategic Loop for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that CVE2PoC achieves 82.9% and 54.3% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3% and 20.4%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05721v2</guid></item><item><title>[arXiv-SE 2026] Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic</title><link>https://arxiv.org/abs/2601.11840</link><description>arXiv:2601.11840v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11840v2</guid></item><item><title>[arXiv-SE 2026] ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</title><link>https://arxiv.org/abs/2602.01655</link><description>arXiv:2602.01655v2 Announce Type: replace-cross 
Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</description><author>cs.SE updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01655v2</guid></item><item><title>[arXiv-PL 2026] Static Analysis Under Non-Deterministic Program Assumptions</title><link>https://arxiv.org/abs/2602.07324</link><description>arXiv:2602.07324v1 Announce Type: new 
Abstract: Static analyses overwhelmingly trade precision for soundness and automation. For this reason, their use-cases are restricted to situations where imprecision isn't prohibitive. In this paper, we propose and specify a static analysis that accepts user-supplied program assumptions that are local to program locations. Such assumptions can be used to counteract imprecision in static analyses, enabling their use in a much wider variety of applications. These assumptions are taken by the analyzer non-deterministically, resulting in a function from sets of accepted assumptions to the resulting analysis under those assumptions. We also demonstrate the utility of such a function in two ways, both of which showcase how it can enable optimization over a search space of assumptions that is otherwise infeasible without the specified analysis.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07324v1</guid></item><item><title>[arXiv-PL 2026] Series-Parallel-Loop Decompositions of Control-flow Graphs</title><link>https://arxiv.org/abs/2602.07627</link><description>arXiv:2602.07627v1 Announce Type: new 
Abstract: Control-flow graphs (CFGs) of structured programs are well known to exhibit strong sparsity properties. Traditionally, this sparsity has been modeled using graph parameters such as treewidth and pathwidth, enabling the development of faster parameterized algorithms for tasks in compiler optimization, model checking, and program analysis. However, these parameters only approximate the structural constraints of CFGs: although every structured CFG has treewidth at most~7, many graphs with treewidth at most~7 cannot arise as CFGs. As a result, existing parameterized techniques are optimized for a substantially broader class of graphs than those encountered in practice.
  In this work, we introduce a new grammar-based decomposition framework that characterizes \emph{exactly} the class of control-flow graphs generated by structured programs. Our decomposition is intuitive, mirrors the syntactic structure of programs, and remains fully compatible with the dynamic-programming paradigm of treewidth-based methods. Using this framework, we design improved algorithms for two classical compiler optimization problems: \emph{Register Allocation} and \emph{Lifetime-Optimal Speculative Partial Redundancy Elimination (LOSPRE)}. Extensive experimental evaluation demonstrates significant performance improvements over previous state-of-the-art approaches, highlighting the benefits of using decompositions tailored specifically to CFGs.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07627v1</guid></item><item><title>[arXiv-PL 2026] Gillian Debugging: Swinging Through the (Compositional Symbolic Execution) Trees, Extended Version</title><link>https://arxiv.org/abs/2602.07742</link><description>arXiv:2602.07742v1 Announce Type: new 
Abstract: In recent years, compositional symbolic execution (CSE) tools have been growing in prominence and are becoming more and more applicable to real-world codebases. Still to this day, however, debugging the output of these tools remains difficult, even for specialist users. To address this, we introduce a debugging interface for symbolic execution tools, integrated with Visual Studio Code and the Gillian multi-language CSE platform, with strong focus on visualisation, interactivity, and intuitive representation of symbolic execution trees. We take care in making this interface tool-agnostic, easing its transfer to other symbolic analysis tools in future. We empirically evaluate our work with a user study, the results of which show the debugger's usefulness in helping early researchers understand the principles of CSE and verify fundamental data structure algorithms in Gillian.</description><author>cs.PL updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07742v1</guid></item><item><title>[arXiv-AI 2026] Report for NSF Workshop on AI for Electronic Design Automation</title><link>https://arxiv.org/abs/2601.14541</link><description>arXiv:2601.14541v3 Announce Type: replace-cross 
Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.</description><author>cs.AI updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14541v3</guid></item><item><title>[arXiv-LG 2026] Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting</title><link>https://arxiv.org/abs/2602.07126</link><description>arXiv:2602.07126v1 Announce Type: new 
Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07126v1</guid></item><item><title>[arXiv-LG 2026] Efficient Planning in Reinforcement Learning via Model Introspection</title><link>https://arxiv.org/abs/2602.07719</link><description>arXiv:2602.07719v1 Announce Type: new 
Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 10 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.07719v1</guid></item><item><title>[arXiv-CR 2026] AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks</title><link>https://arxiv.org/abs/2602.06534</link><description>arXiv:2602.06534v1 Announce Type: new 
Abstract: Automated detection of cyber attacks is a critical capability to counteract the growing volume and sophistication of cyber attacks. However, the high numbers of security alerts issued by intrusion detection systems lead to alert fatigue among analysts working in security operations centres (SOC), which in turn causes slow reaction time and incorrect decision making. Alert grouping, which refers to clustering of security alerts according to their underlying causes, can significantly reduce the number of distinct items analysts have to consider. Unfortunately, conventional time-based alert grouping solutions are unsuitable for large scale computer networks characterised by high levels of false positive alerts and simultaneously occurring attacks. To address these limitations, we propose AlertBERT, a self-supervised framework designed to group alerts from isolated or concurrent attacks in noisy environments. Thereby, our open-source implementation of AlertBERT leverages masked-language-models and density-based clustering to support both real-time or forensic operation. To evaluate our framework, we further introduce a novel data augmentation method that enables flexible control over noise levels and simulates concurrent attack occurrences. Based on the data sets generated through this method, we demonstrate that AlertBERT consistently outperforms conventional time-based grouping techniques, achieving superior accuracy in identifying correct alert groups.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06534v1</guid></item><item><title>[arXiv-CR 2026] Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models</title><link>https://arxiv.org/abs/2602.06687</link><description>arXiv:2602.06687v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06687v1</guid></item><item><title>[arXiv-CR 2026] Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection</title><link>https://arxiv.org/abs/2602.06751</link><description>arXiv:2602.06751v1 Announce Type: new 
Abstract: Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.
  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.
  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06751v1</guid></item><item><title>[arXiv-CR 2026] CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses</title><link>https://arxiv.org/abs/2602.01438</link><description>arXiv:2602.01438v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit secure prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.</description><author>cs.CR updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01438v2</guid></item><item><title>[arXiv-SE 2026] SVRepair: Structured Visual Reasoning for Automated Program Repair</title><link>https://arxiv.org/abs/2602.06090</link><description>arXiv:2602.06090v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06090v1</guid></item><item><title>[arXiv-SE 2026] AgentStepper: Interactive Debugging of Software Development Agents</title><link>https://arxiv.org/abs/2602.06593</link><description>arXiv:2602.06593v1 Announce Type: new 
Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06593v1</guid></item><item><title>[arXiv-SE 2026] Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience</title><link>https://arxiv.org/abs/2602.06831</link><description>arXiv:2602.06831v1 Announce Type: new 
Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06831v1</guid></item><item><title>[arXiv-SE 2026] SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development</title><link>https://arxiv.org/abs/2505.16975</link><description>arXiv:2505.16975v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks. However, feature-driven development, a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world end-to-end feature-driven software development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. We evaluated SWE-Dev across 17 base LLMs, 10 reasoning-focused LLMs, 10 multi-agent systems, and 8 tool-augmented LLM agents. Results show substantial headroom: the best single-turn model reaches only 22.51\% Pass@1 on the hard split, while OpenHands agents improve to 56.44\% but still leave many tasks unsolved. Code is available here https://github.com/DorothyDUUU/SWE-Dev.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.16975v3</guid></item><item><title>[arXiv-SE 2026] LLM-Based Repair of Static Nullability Errors</title><link>https://arxiv.org/abs/2507.20674</link><description>arXiv:2507.20674v2 Announce Type: replace 
Abstract: Modern Java projects increasingly adopt static analysis tools that prevent null-pointer exceptions by treating nullness as a type property. However, integrating such tools into large, existing codebases remains a significant challenge. While annotation inference can eliminate many errors automatically, a subset of residual errors -- typically a mix of real bugs and false positives -- often persist and can only be resolved via code changes. Manually addressing these errors is tedious and error-prone. Large language models (LLMs) offer a promising path toward automating these repairs, but naively-prompted LLMs often generate incorrect, contextually-inappropriate edits. We present NullRepair, a system that integrates LLMs into a structured workflow for resolving the errors from a nullability checker. NullRepair's decision process follows a flowchart derived from manual analysis of 200 real-world errors. It leverages static analysis to identify safe and unsafe usage regions of symbols, using error-free usage examples to contextualize model prompts. Patches are generated through an iterative interaction with the LLM that incorporates project-wide context and decision logic. Our evaluation on 12 real-world Java projects shows that NullRepair resolves 63% of the 1,119 nullability errors that remain after applying a state-of-the-art annotation inference technique. Unlike two baselines (single-shot prompt and mini-SWE-agent), NullRepair also largely preserves program semantics, with all unit tests passing in 10/12 projects after applying every edit proposed by NullRepair, and 98% or more tests passing in the remaining two projects.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.20674v2</guid></item><item><title>[arXiv-SE 2026] KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation</title><link>https://arxiv.org/abs/2511.14224</link><description>arXiv:2511.14224v2 Announce Type: replace 
Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.14224v2</guid></item><item><title>[arXiv-SE 2026] OmniCode: A Benchmark for Evaluating Software Engineering Agents</title><link>https://arxiv.org/abs/2602.02262</link><description>arXiv:2602.02262v2 Announce Type: replace 
Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02262v2</guid></item><item><title>[arXiv-SE 2026] SEAL: Symbolic Execution with Separation Logic (Competition Contribution)</title><link>https://arxiv.org/abs/2602.05703</link><description>arXiv:2602.05703v2 Announce Type: replace 
Abstract: SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.</description><author>cs.SE updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.05703v2</guid></item><item><title>[arXiv-PL 2026] Auditing Rust Crates Effectively</title><link>https://arxiv.org/abs/2602.06466</link><description>arXiv:2602.06466v1 Announce Type: new 
Abstract: We introduce Cargo Scan, the first interactive program analysis tool designed to help developers audit third-party Rust code. Real systems written in Rust rely on thousands of transitive dependencies. These dependencies are as dangerous in Rust as they are in other languages (e.g., C or JavaScript) -- and auditing these dependencies today means manually inspecting every line of code. Unlike for most industrial languages, though, we can take advantage of Rust's type and module system to minimize the amount of code that developers need to inspect to the code that is potentially dangerous. Cargo Scan models such potentially dangerous code as effects and performs a side-effects analysis, tailored to Rust, to identify effects and track them across crate and module boundaries. In most cases (69.2%) developers can inspect flagged effects and decide whether the code is potentially dangerous locally. In some cases, however, the safety of an effect depends on the calling context -- how a function is called, potentially by a crate the developer imports later. Hence, Cargo Scan tracks context-dependent information using a call-graph, and collects audit results into composable and reusable audit files. In this paper, we describe our experience auditing Rust crates with Cargo Scan. In particular, we audit the popular client and server HTTP crate, hyper, and all of its dependencies; our experience shows that Cargo Scan can reduce the auditing burden of potentially dangerous code to a median of 0.2% of lines of code when compared to auditing whole crates. Looking at the Rust ecosystem more broadly, we find that Cargo Scan can automatically classify ~3.5K of the top 10K crates on crates.io as safe; of the crates that do require manual inspection, we find that most of the potentially dangerous side-effects are concentrated in roughly 3% of these crates.</description><author>cs.PL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06466v1</guid></item><item><title>[arXiv-PL 2026] Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)</title><link>https://arxiv.org/abs/2602.06680</link><description>arXiv:2602.06680v1 Announce Type: new 
Abstract: Fixpoint iteration constitutes the algorithmic core of static analyzers. Parallelizing the fixpoint engine can significantly reduce analysis times. Previous approaches typically fix the granularity of tasks upfront, e.g., at the level of program threads or procedures - yielding an engine permanently stuck in one gear. Instead, we propose to parallelize a generic fixpoint engine in a way that is parametric in the task granularity - meaning that our engine can be run in different gears. We build on the top-down solver TD, extended with support for mixed-flow sensitivity, and realize two competing philosophies for parallelization, both building on a task pool that schedules tasks to a fixed number of workers. The nature of tasks differs between the philosophies. In the immediate approach, all tasks access a single thread-safe hash table maintaining solver state, while in the independent approach, each task has its own state and exchanges data with other tasks via a publish/subscribe data structure. We have equipped the fixpoint engine of the static analysis framework Goblint with implementations following both philosophies and report on our results for large real-world programs.</description><author>cs.PL updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06680v1</guid></item><item><title>[arXiv-AI 2026] Agentic Uncertainty Reveals Agentic Overconfidence</title><link>https://arxiv.org/abs/2602.06948</link><description>arXiv:2602.06948v1 Announce Type: new 
Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.</description><author>cs.AI updates on arXiv.org</author><pubDate>Mon, 09 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.06948v1</guid></item><item><title>[ASE 2026] Robust vulnerability detection with limited data via training-efficient adversarial reprogramming.</title><link>https://doi.org/10.1007/s10515-026-00590-4</link><description>Authors: Zhenzhou Tian, Chuang Zhang, Yunpeng Hui, Jiaze Sun, Yanping Chen 0006, Lingwei Chen
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Sun, 08 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/TianZHSCC26</guid></item><item><title>[ASE 2026] Knowledge distillation-driven commit-aware multimodal learning for software vulnerability detection.</title><link>https://doi.org/10.1007/s10515-026-00595-z</link><description>Authors: Rim Mahouachi
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Sun, 08 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/Mahouachi26</guid></item><item><title>[ASE 2026] Vul-R2: A Reasoning LLM for Automated Vulnerability Repair.</title><link>https://doi.org/10.1109/ASE63991.2025.00011</link><description>Authors: Xin-Cheng Wen, Zirui Lin, Yijun Yang, Cuiyun Gao 0001, Deheng Ye
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WenLYGY25</guid></item><item><title>[ASE 2026] Towards More Accurate Static Analysis for Taint-Style Bug Detection in Linux Kernel.</title><link>https://doi.org/10.1109/ASE63991.2025.00039</link><description>Authors: Haonan Li, Hang Zhang 0012, Kexin Pei, Zhiyun Qian
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiZPQ25</guid></item><item><title>[ASE 2026] GlassWing: A Tailored Static Analysis Approach for Flutter Android Apps.</title><link>https://doi.org/10.1109/ASE63991.2025.00050</link><description>Authors: Xiangyu Zhang, Yucheng Su, Lingling Fan 0003, Miaoying Cai, Sen Chen 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangSFCC25</guid></item><item><title>[ASE 2026] BinStruct: Binary Structure Recovery Combining Static Analysis and Semantics.</title><link>https://doi.org/10.1109/ASE63991.2025.00060</link><description>Authors: Yiran Zhang, Zhengzi Xu, Zhe Lang, Chengyue Liu, Yuqiang Sun 0001, Wenbo Guo, Chengwei Liu, Weisong Sun, Yang Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangXLLSGLSL25</guid></item><item><title>[ASE 2026] Hit The Bullseye On The First Shot: Improving LLMs Using Multi-Sample Self-Reward Feedback for Vulnerability Repair.</title><link>https://doi.org/10.1109/ASE63991.2025.00071</link><description>Authors: Rui Jiao, Yue Zhang, Jinku Li, Jianfeng Ma
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiaoZLM25</guid></item><item><title>[ASE 2026] Exploring Static Taint Analysis in LLMs: A Dynamic Benchmarking Framework for Measurement and Enhancement.</title><link>https://doi.org/10.1109/ASE63991.2025.00082</link><description>Authors: Haoran Zhao, Lei Zhang 0006, Keke Lian, Fute Sun, Bofei Chen, Yongheng Liu, Zhiyu Wu, Yuan Zhang 0009, Min Yang 0002
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoZLSCLWZY25</guid></item><item><title>[ASE 2026] LOSVER: Line-Level Modifiability Signal-Guided Vulnerability Detection and Classification.</title><link>https://doi.org/10.1109/ASE63991.2025.00092</link><description>Authors: Doha Nam, Jongmoon Baik
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/NamB25</guid></item><item><title>[ASE 2026] Belief Propagation with Local Structure and Its Applications in Program Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00132</link><description>Authors: Yiqian Wu, Yifan Chen, Yingfei Xiong 0001, Xin Zhang 0035
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuCXZ25</guid></item><item><title>[ASE 2026] Leveraging Mixture-of-Experts Framework for Smart Contract Vulnerability Repair with Large Language Model.</title><link>https://doi.org/10.1109/ASE63991.2025.00140</link><description>Authors: Hang Yuan, Xizhi Hou, Lei Yu, Li Yang, Jiayue Tang, Jiadong Xu, Yifei Liu, Fengjun Zhang, Chun Zuo
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/YuanHYYTXLZZ25</guid></item><item><title>[ASE 2026] Have We Solved Access Control Vulnerability Detection in Smart Contracts? A Benchmark Study.</title><link>https://doi.org/10.1109/ASE63991.2025.00166</link><description>Authors: Han Liu 0012, Daoyuan Wu, Yuqiang Sun 0001, Shuai Wang 0011, Yang Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiuWSWL25</guid></item><item><title>[ASE 2026] Interpretable Vulnerability Detection Reports.</title><link>https://doi.org/10.1109/ASE63991.2025.00168</link><description>Authors: Cludia Mamede, Jos Campos 0001, Claire Le Goues, Rui Abreu 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/MamedeCGA25</guid></item><item><title>[ASE 2026] Incremental Program Analysis in the Wild: An Empirical Study on Real-World Program Changes.</title><link>https://doi.org/10.1109/ASE63991.2025.00194</link><description>Authors: Xizao Wang, Xiangrong Bin, Lanxin Huang, Shangqing Liu, Jianhua Zhao, Lei Bu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangBHLZB25</guid></item><item><title>[ASE 2026] ACTaint: Agent-Based Taint Analysis for Access Control Vulnerabilities in Smart Contracts.</title><link>https://doi.org/10.1109/ASE63991.2025.00210</link><description>Authors: Huarui Lin, Zhipeng Gao 0002, Jiachi Chen, Xiang Chen 0005, Xiaohu Yang 0001, Lingfeng Bao
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LinGCCYB25</guid></item><item><title>[ASE 2026] PALM: Synergizing Program Analysis and LLMs to Enhance Rust Unit Test Coverage.</title><link>https://doi.org/10.1109/ASE63991.2025.00223</link><description>Authors: Bei Chu, Yang Feng 0003, Kui Liu 0001, Hange Shi, Zifan Nan, Zhaoqiang Guo, Baowen Xu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ChuFLSNGX25</guid></item><item><title>[ASE 2026] LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM.</title><link>https://doi.org/10.1109/ASE63991.2025.00245</link><description>Authors: Yuxin Zhang, Yuxia Zhang, Zeyu Sun 0004, Yanjie Jiang, Hui Liu 0003
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhangZSJL25</guid></item><item><title>[ASE 2026] HarmoBridge: Bridging ArkTS and C/C++ for Cross-Language Static Analysis on HarmonyOS.</title><link>https://doi.org/10.1109/ASE63991.2025.00261</link><description>Authors: Jiale Wu, Jiapeng Deng, Yanjie Zhao, Li Li, Haoyu Wang
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuDZLW25</guid></item><item><title>[ASE 2026] SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review.</title><link>https://doi.org/10.1109/ASE63991.2025.00315</link><description>Authors: Kai Wang, Bingcheng Mao, Shuai Jia, Yujie Ding, Dongming Han, Tianyi Ma, Bin Cao
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangMJDHMC25</guid></item><item><title>[ASE 2026] ConfuseTaint: Exploiting Vulnerabilities to Bypass Dynamic Taint Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00340</link><description>Authors: Yufei Wu, Alexandre Bartel
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuB25</guid></item><item><title>[ASE 2026] STaint: Detecting Second-Order Vulnerabilities in PHP Applications with LLM-Assisted Bi-Directional Static Taint Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00347</link><description>Authors: Yuchen Ji, Hongchen Cao, Jingzhu He
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiCH25</guid></item><item><title>[ASE 2026] VUSC: An Extensible Research Platform for Java-Based Static Analysis.</title><link>https://doi.org/10.1109/ASE63991.2025.00354</link><description>Authors: Marc Miltenberger, Steven Arzt
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/MiltenbergerA25</guid></item><item><title>[ASE 2026] Secure Transaction Semantics: Analysis, Vulnerability Detection, and Attack Modeling.</title><link>https://doi.org/10.1109/ASE63991.2025.00398</link><description>Authors: Yixuan Liu
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Liu25</guid></item><item><title>[TDSC 2026] Interaction-Aware Vulnerability Detection in Smart Contract Bytecodes.</title><link>https://doi.org/10.1109/TDSC.2025.3605773</link><description>Authors: Wenkai Li, Xiaoqi Li 0001, Yingjie Mao, Yuqing Zhang 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/LiLMZ26</guid></item><item><title>[TDSC 2026] Pruning Attention Heads Based on Semantic and Code Structure for Smart Contract Vulnerability Detection.</title><link>https://doi.org/10.1109/TDSC.2025.3607471</link><description>Authors: Siyu Jiang, Yuwen Chen, Teng Ouyang, Xue Zhang, Shen Su
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/JiangCOZS26</guid></item><item><title>[TDSC 2026] Alert2Vec: Eliminating Alert Fatigue by Embedding Security Alerts Through Subgraph Learning.</title><link>https://doi.org/10.1109/TDSC.2025.3609834</link><description>Authors: Songyun Wu, Xiaoqing Sun, Enhuan Dong, Zhiliang Wang, Chen Zhao, Jiahai Yang 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/WuSDWZY26</guid></item><item><title>[TDSC 2026] Nonstandard Sinks Matter: A Comprehensive and Efficient Taint Analysis Framework for Vulnerability Detection in Embedded Firmware.</title><link>https://doi.org/10.1109/TDSC.2025.3619200</link><description>Authors: Enzhou Song, Yuhao Zhao, Can Zhang, Jinyuan Zhai, Ruijie Cai, Long Liu, Qichao Yang, Xiaokang Yin 0001, Shengli Liu 0003
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2026</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/SongZZZCLYYL26</guid></item><item><title>[ASE 2026] Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering.</title><link>https://doi.org/10.1109/ASE63991.2025.00276</link><description>Authors: Ziyou Li, Agnia Sergeyuk, Maliheh Izadi
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiSI25</guid></item><item><title>[ASE 2026] RustRepoTrans: Repository-level Context Code Translation Benchmark Targeting Rust.</title><link>https://doi.org/10.1109/ASE63991.2025.00057</link><description>Authors: Guangsheng Ou, Mingwei Liu 0002, Yuxuan Chen, Yanlin Wang 0001, Xin Peng 0001, Zibin Zheng
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/OuLCWPZ25</guid></item><item><title>[ASE 2026] AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion.</title><link>https://doi.org/10.1109/ASE63991.2025.00085</link><description>Authors: Tianyue Jiang, Yanlin Wang 0001, Yanli Wang 0001, Daya Guo, Ensheng Shi, Yuchi Ma, Jiachi Chen, Zibin Zheng
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiangWWGSMCZ25</guid></item><item><title>[ASE 2026] Aligning LLMs to Fully Utilize the Cross-file Context in Repository-level Code Completion.</title><link>https://doi.org/10.1109/ASE63991.2025.00125</link><description>Authors: Jia Li 0012, Hao Zhu, Huanyu Liu 0001, Xianjie Shi, He Zong, Yihong Dong, Kechi Zhang, Siyuan Jiang, Zhi Jin 0001, Ge Li 0001
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiZLSZDZJJL25</guid></item><item><title>[ASE 2026] PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing.</title><link>https://doi.org/10.1109/ASE63991.2025.00153</link><description>Authors: Xiaoxue Ren, Jun Wan, Yun Peng, Zhongxin Liu 0002, Ming Liang, Dajun Chen, Wei Jiang, Yong Li
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/RenWPLLCJL25</guid></item><item><title>[ASE 2026] FastCoder: Accelerating Repository-level Code Generation via Efficient Retrieval and Verification.</title><link>https://doi.org/10.1109/ASE63991.2025.00190</link><description>Authors: Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Jia Li, Lin Shi
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoZLLMJZLS25</guid></item><item><title>[ASE 2026] SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation.</title><link>https://doi.org/10.1109/ASE63991.2025.00192</link><description>Authors: Gustavo Ansaldi Oliva, Gopi Krishnan Rajbahadur, Aaditya Bhatia, Haoxiang Zhang 0001, Yihao Chen, Zhilong Chen, Arthur Leung, Dayi Lin, Boyuan Chen 0002, Ahmed E. Hassan
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/OlivaRBZCCLLCH25</guid></item><item><title>[ASE 2026] Issue Localization via LLM-Driven Iterative Code Graph Searching.</title><link>https://doi.org/10.1109/ASE63991.2025.00249</link><description>Authors: Zhonghao Jiang, Xiaoxue Ren, Meng Yan 0001, Wei Jiang, Yong Li, Zhongxin Liu 0002
Venue: ASE
Year: 2025</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Sat, 07 Feb 2026 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/JiangRYJLL25</guid></item><item><title>[C&amp;S 2026] SemTaint: A scalable taint analysis approach for JavaWeb frameworks and composite containers</title><link>https://www.sciencedirect.com/science/article/pii/S0167404825005103?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers &amp; Security, Volume 163&lt;/p&gt;&lt;p&gt;Author(s): Haotian Huang, Ruibin Yan, Jian Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Computers &amp; Security</author><pubDate>Sat, 07 Feb 2026 16:45:49 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0167404825005103</guid></item><item><title>[IST 2026] EdgeSim: Firmware vulnerability detection with control transfer-enhanced binary code similarity detection</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000091?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Li Liu, Shen Wang, Xunzhi Jiang&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000091</guid></item><item><title>[IST 2026] CSVD-AES: Cross-project software vulnerability detection based on active learning with metric fusion</title><link>https://www.sciencedirect.com/science/article/pii/S0950584926000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Zhidan Yuan, Xiang Chen, Juan Zhang, Weiming Zeng&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584926000042</guid></item><item><title>[IST 2026] VulSEG: Enhanced graph-based vulnerability detection system with advanced text embedding</title><link>https://www.sciencedirect.com/science/article/pii/S0950584925003465?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information and Software Technology, Volume 192&lt;/p&gt;&lt;p&gt;Author(s): Wenjing Cai, Xin Liu, Lipeng Gao&lt;/p&gt;</description><author>ScienceDirect Publication: Information and Software Technology</author><pubDate>Sat, 07 Feb 2026 16:45:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950584925003465</guid></item><item><title>[arXiv-AI 2026] The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution</title><link>https://arxiv.org/abs/2601.15075</link><description>arXiv:2601.15075v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining \textbf{the reason behind agent behaviors}. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems. Codes are available at https://github.com/AI45Lab/AgentDoG.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15075v2</guid></item><item><title>[arXiv-AI 2026] SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</title><link>https://arxiv.org/abs/2601.22129</link><description>arXiv:2601.22129v2 Announce Type: replace-cross 
Abstract: Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</description><author>cs.AI updates on arXiv.org</author><pubDate>Sat, 07 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22129v2</guid></item><item><title>[TOSEM 2026] PonziHunter: Hunting Ethereum Ponzi Contract via Static Analysis and Contrastive Learning on the Bytecode Level</title><link>https://dl.acm.org/doi/abs/10.1145/3735971?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-21, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Wed, 21 Jan 2026 04:18:39 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3735971?af=R</guid></item><item><title>[TOSEM 2026] Abundant Modalities Offer More Nutrients: Multi-Modal-Based Function-Level Vulnerability Detection</title><link>https://dl.acm.org/doi/abs/10.1145/3731557?af=R</link><description>ACM Transactions on Software Engineering and Methodology, Volume 35, Issue 2, Page 1-31, February 2026. &lt;br /&gt;</description><author>Association for Computing Machinery: ACM Transactions on Software Engineering and Methodology: Table of Contents</author><pubDate>Tue, 20 Jan 2026 02:02:29 GMT</pubDate><guid isPermaLink="true">https://dl.acm.org/doi/abs/10.1145/3731557?af=R</guid></item><item><title>[TDSC 2025] HgtJIT: Just-in-Time Vulnerability Detection Based on Heterogeneous Graph Transformer.</title><link>https://doi.org/10.1109/TDSC.2025.3586669</link><description>Authors: Xiaobing Sun 0001, Mingxuan Zhou, Sicong Cao, Xiaoxue Wu 0001, Lili Bo, Di Wu 0050, Bin Li 0006, Yang Xiang 0001
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2025</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/SunZCWBWLX25</guid></item><item><title>[TDSC 2025] Real-World Code Vulnerability Detection Framework: From Data Preprocessing to Multi-Feature Fusion Detection.</title><link>https://doi.org/10.1109/TDSC.2025.3601228</link><description>Authors: Jixian Zhang, Qingfeng Du, Sheng Li, Zhongda Lu, Ting He, Chengwei Liu
Venue: IEEE Trans. Dependable Secur. Comput.
Year: 2025</description><author>dblp: new issues for streams/journals/tdsc</author><pubDate>Mon, 24 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/tdsc/ZhangDLLHL25</guid></item><item><title>[CCS 2025] Autonomous Vulnerability Analysis, Triaging, and Repair: A Historical Perspective.</title><link>https://doi.org/10.1145/3719027.3748270</link><description>Authors: Giovanni Vigna
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Vigna25</guid></item><item><title>[CCS 2025] SyzSpec: Specification Generation for Linux Kernel Fuzzing via Under-Constrained Symbolic Execution.</title><link>https://doi.org/10.1145/3719027.3744811</link><description>Authors: Yu Hao 0006, Juefei Pu, Xingyu Li, Zhiyun Qian, Ardalan Amiri Sani
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/0006PLQS25</guid></item><item><title>[CCS 2025] ZVDetector: State-Guided Vulnerability Detection System for Zigbee Devices.</title><link>https://doi.org/10.1145/3719027.3765035</link><description>Authors: Hai Lin, Chenglong Li 0006, Jiahai Yang 0001, Zhiliang Wang, Jiaqi Bai
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Lin00WB25</guid></item><item><title>[CCS 2025] Poster: LogCraft: Crafting CVE-Aware Synthetic Worlds (Logs).</title><link>https://doi.org/10.1145/3719027.3760736</link><description>Authors: Kai-Xian Wong, Chan-Jien Tan, Yi-Ting Huang, Ying-Ren Guo, Yu-Zih Jheng, Guo-Wei Wong, Meng Chang Chen
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/WongTHGJWC25</guid></item><item><title>[CCS 2025] AI-Augmented Static Analysis: Bridging Heuristics and Completeness for Practical Reverse Engineering.</title><link>https://doi.org/10.1145/3719027.3765565</link><description>Authors: Monika Santra
Venue: CCS
Year: 2025</description><author>dblp: new volumes for streams/conf/ccs</author><pubDate>Sun, 23 Nov 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ccs/Santra25</guid></item><item><title>[USENIXSec 2025] LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lekssays</link><description>Authors: Ahmed Lekssays, Hamza Mouhcine, Khang Tran, Ting Yu 0001, Issa Khalil
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/LekssaysMT0K25</guid></item><item><title>[USENIXSec 2025] Confusing Value with Enumeration: Studying the Use of CVEs in Academia.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/schloegel</link><description>Authors: Moritz Schloegel, Daniel Klischies, Simon Koch 0001, David Klein 0001, Lukas Gerlach 0001, Malte Wessels, Leon Trampert, Martin Johns, Mathy Vanhoef, Michael Schwarz 0001, Thorsten Holz, Jo Van Bulck
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/SchloegelK000WT25</guid></item><item><title>[USENIXSec 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM Analysis.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/kim-youngjoon</link><description>Authors: Youngjoon Kim 0001, Sunguk Shin 0001, Hyoungshick Kim, Jiwon Yoon 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Kim0KY25</guid></item><item><title>[USENIXSec 2025] SCASE: Automated Secret Recovery via Side-Channel-Assisted Symbolic Execution.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/weber</link><description>Authors: Daniel Weber 0007, Lukas Gerlach 0001, Leon Trampert, Youheng L, Jo Van Bulck, Michael Schwarz 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/00070TLB025</guid></item><item><title>[USENIXSec 2025] Hybrid Language Processor Fuzzing via LLM-Based Constraint Solving.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/yang-yupeng</link><description>Authors: Yupeng Yang, Shenglong Yao, Jizhou Chen, Wenke Lee
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/YangYCL25</guid></item><item><title>[USENIXSec 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for Static Analysis Result Verification.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/bao-andrew</link><description>Authors: Andrew Bao, Wenjia Zhao, Yanhao Wang, Yueqiang Cheng, Stephen McCamant, Pen-Chung Yew
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/BaoZWCMY25</guid></item><item><title>[USENIXSec 2025] ZIPPER: Static Taint Analysis for PHP Applications with Precision and Efficiency.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/wang-xinyi</link><description>Authors: Xinyi Wang 0013, Yeting Li, Jie Lu 0009, Shizhe Cui, Chenghang Shi, Qin Mai, Yunpei Zhang, Yang Xiao 0011, Feng Li 0045, Wei Huo
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/WangLLCSMZ00H25</guid></item><item><title>[USENIXSec 2025] Effective Directed Fuzzing with Hierarchical Scheduling for Web Vulnerability Detection.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/lin-zihan</link><description>Authors: Zihan Lin, Yuan Zhang 0009, Jiarun Dai, Xinyou Huang, Bocheng Xiang, Guangliang Yang 0001, Letian Yuan, Lei Zhang 0096, Tian Chen, Min Yang 0002
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Lin0DHX0Y0C025</guid></item><item><title>[USENIXSec 2025] SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/hu-yiwei</link><description>Authors: Yiwei Hu, Zhen Li 0027, Kedie Shu, Shenghua Guan, Deqing Zou, Shouhuai Xu, Bin Yuan, Hai Jin 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/Hu0SGZXY025</guid></item><item><title>[USENIXSec 2025] SoK: Towards Effective Automated Vulnerability Repair.</title><link>https://www.usenix.org/conference/usenixsecurity25/presentation/li-ying</link><description>Authors: Ying Li 0095, Faysal Hossain Shezan, Bomin Wei, Gang Wang 0011, Yuan Tian 0001
Venue: USENIX Security Symposium
Year: 2025</description><author>dblp: new volumes for streams/conf/uss</author><pubDate>Thu, 30 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/uss/0095SW0025</guid></item><item><title>[ASE 2025] GPTVD: vulnerability detection and analysis method based on LLM's chain of thoughts.</title><link>https://doi.org/10.1007/s10515-025-00550-4</link><description>Authors: Yinan Chen, Yuan Huang, Xiangping Chen, Pengfei Shen, Lei Yun
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/ChenHCSY26</guid></item><item><title>[ASE 2025] HMF: Enhancing reentrancy vulnerability detection and repair with a hybrid model framework.</title><link>https://doi.org/10.1007/s10515-025-00546-0</link><description>Authors: Mengliang Li, Qiang Shen, Xiaoxue Ren, Han Fu, Zhuo Li 0014, Jianling Sun
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/LiSRFLS26</guid></item><item><title>[ASE 2025] Graph neural networks for precise bug localization through structural program analysis.</title><link>https://doi.org/10.1007/s10515-025-00556-y</link><description>Authors: Leila Yousofvand, Seyfollah Soleimani, Vahid Rafe, Amin Nikanjam
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YousofvandSRN26</guid></item><item><title>[ASE 2025] ByteEye: A smart contract vulnerability detection framework at bytecode level with graph neural networks.</title><link>https://doi.org/10.1007/s10515-025-00559-9</link><description>Authors: Jinni Yang, Shuang Liu 0007, Surong Dai, Yaozheng Fang, Kunpeng Xie, Ye Lu 0004
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/YangLDFXL26</guid></item><item><title>[ASE 2025] SPVR: syntax-to-prompt vulnerability repair based on large language models.</title><link>https://doi.org/10.1007/s10515-025-00579-5</link><description>Authors: Ruoke Wang, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Yang Xiao, Xuan Wang
Venue: Autom. Softw. Eng.
Year: 2026</description><author>dblp: new issues for streams/journals/ase</author><pubDate>Wed, 08 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:journals/ase/WangLGWXW26</guid></item><item><title>[SOSP 2025] KNighter: Transforming Static Analysis with LLM-Synthesized Checkers.</title><link>https://doi.org/10.1145/3731569.3764827</link><description>Authors: Chenyuan Yang, Zijie Zhao, Zichen Xie, Haoyu Li, Lingming Zhang 0001
Venue: SOSP
Year: 2025</description><author>dblp: new volumes for streams/conf/sosp</author><pubDate>Wed, 01 Oct 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sosp/YangZXLZ25</guid></item><item><title>[IJCAI 2025] SecV: LLM-based Secure Verilog Generation with Clue-Guided Exploration on Hardware-CWE Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/895</link><description>Authors: Fanghao Fan, Yingjie Xia, Li Kuang
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/FanXK25</guid></item><item><title>[IJCAI 2025] POLO: An LLM-Powered Project-Level Code Performance Optimization Framework.</title><link>https://doi.org/10.24963/ijcai.2025/814</link><description>Authors: Jiameng Bai, Ruoyi Xu, Sai Wu, Dingyu Yang, Junbo Zhao 0002, Gang Chen 0001
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/BaiXWY0025</guid></item><item><title>[IJCAI 2025] APIMig: A Project-Level Cross-Multi-Version API Migration Framework Based on Evolution Knowledge Graph.</title><link>https://doi.org/10.24963/ijcai.2025/829</link><description>Authors: Li Kuang, Qi Xie, Haiyang Yang, Yang Yang, Xiang Wei, HaoYue Kang, Yingjie Xia
Venue: IJCAI
Year: 2025</description><author>dblp: new volumes for streams/conf/ijcai</author><pubDate>Sun, 21 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ijcai/KuangXYYWKX25</guid></item><item><title>[EuroS&amp;P 2025] Mitigating Information Leakage in Large Language Models: Evaluating the Impact of Code Obfuscation on Vulnerability Detection.</title><link>https://doi.org/10.1109/EuroSPW67616.2025.00007</link><description>Authors: Beng Glay, Cemal Yilmaz 0001
Venue: EuroS&amp;amp;P (Workshops)
Year: 2025</description><author>dblp: new volumes for streams/conf/eurosp</author><pubDate>Mon, 15 Sep 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/eurosp/GulayY25</guid></item><item><title>[S&amp;P 2025] Code Vulnerability Repair with Large Language Model Using Context-Aware Prompt Tuning.</title><link>https://doi.org/10.1109/SPW67851.2025.00040</link><description>Authors: Arshiya Khan, Guannan Liu, Xing Gao
Venue: SP (Workshops)
Year: 2025</description><author>dblp: new volumes for streams/conf/sp</author><pubDate>Sun, 20 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/sp/KhanLG25</guid></item><item><title>[OSDI 2025] Paralegal: Practical Static Analysis for Privacy Bugs.</title><link>https://www.usenix.org/conference/osdi25/presentation/adam</link><description>Authors: Justus Adam, Carolyn Zech, Livia Zhu, Sreshtaa Rajesh, Nathan Harbison, Mithi Jethwa, Will Crichton, Shriram Krishnamurthi, Malte Schwarzkopf
Venue: OSDI
Year: 2025</description><author>dblp: new volumes for streams/conf/osdi</author><pubDate>Wed, 16 Jul 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/osdi/AdamZZRHJCKS25</guid></item><item><title>[ISSTA 2025] Revisiting the Combination of Static Analysis Error Traces and Dynamic Symbolic Execution: A Potential Approach for True Positive Confirmation (Registered Report).</title><link>https://doi.org/10.1145/3713081.3731720</link><description>Authors: Yihua Xu, Chengyu Zhang 0001, Geguang Pu
Venue: ISSTA Companion
Year: 2025</description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/Xu0P25</guid></item><item><title>[ISSTA 2025] A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection.</title><link>https://doi.org/10.1145/3713081.3731746</link><description>Authors: Junji Yu, Honglin Shu, Michael Fu, Dong Wang 0044, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen 0003
Venue: ISSTA Companion
Year: 2025</description><author>dblp: new volumes for streams/conf/issta</author><pubDate>Mon, 09 Jun 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/issta/YuSF0TK025</guid></item><item><title>[ICLR 2025] IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities.</title><link>https://openreview.net/forum?id=9LdJDU7E91</link><description>Authors: Ziyang Li, Saikat Dutta 0001, Mayur Naik
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Li0N25</guid></item><item><title>[ICLR 2025] SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?</title><link>https://openreview.net/forum?id=riTiq3i21b</link><description>Authors: John Yang 0002, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida Wang 0001, Ofir Press
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/YangJZLYWPMSNY025</guid></item><item><title>[ICLR 2025] RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph.</title><link>https://openreview.net/forum?id=dw9VUsSHGB</link><description>Authors: Siru Ouyang, Wenhao Yu 0002, Kaixin Ma, Zilin Xiao, Zhihan Zhang 0001, Mengzhao Jia, Jiawei Han 0001, Hongming Zhang 0009, Dong Yu 0001
Venue: ICLR
Year: 2025</description><author>dblp: new volumes for streams/conf/iclr</author><pubDate>Sun, 11 May 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/iclr/Ouyang0MX0J00025</guid></item><item><title>[ACSAC 2025] Learning to Unfix: Towards ML Robustness in Vulnerability Detection via Structure-Aware Code Generation.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00014</link><description>Authors: Muhammad Fakhrur Rozi, Takeshi Takahashi 0001
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/Rozi024</guid></item><item><title>[ACSAC 2025] AdVul: Adversarial Attack against ML-based Vulnerability Detection.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00018</link><description>Authors: Marina Katoh, Weiping Pei, Youye Xie
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/KatohPX24</guid></item><item><title>[ACSAC 2025] Software Vulnerability Detection Using LLM: Does Additional Information Help?</title><link>https://doi.org/10.1109/ACSACW65225.2024.00031</link><description>Authors: Samiha Shimmi, Yash Saini, Mark Schaefer, Hamed Okhravi, Mona Rahimi
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/ShimmiSSOR24</guid></item><item><title>[ACSAC 2025] Automated Vulnerability Detection in Smart Contracts using Control Flow Graphs and Machine Learning.</title><link>https://doi.org/10.1109/ACSACW65225.2024.00037</link><description>Authors: Charles Lohest, Samy Bettaieb, Axel Legay
Venue: ACSAC Workshops
Year: 2024</description><author>dblp: new volumes for streams/conf/acsac</author><pubDate>Thu, 24 Apr 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/acsac/LohestBL24</guid></item><item><title>[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.</title><link>https://www.ndss-symposium.org/ndss-paper/from-large-to-mammoth-a-comparative-evaluation-of-large-language-models-in-vulnerability-detection/</link><description>Authors: Jie Lin, David Mohaisen
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/LinM25</guid></item><item><title>[NDSS 2025] Uncovering the iceberg from the tip: Generating API Specifications for Bug Detection via Specification Propagation Analysis.</title><link>https://www.ndss-symposium.org/ndss-paper/uncovering-the-iceberg-from-the-tip-generating-api-specifications-for-bug-detection-via-specification-propagation-analysis/</link><description>Authors: Miaoqian Lin, Kai Chen 0012, Yi Yang 0100, Jinghua Liu
Venue: NDSS
Year: 2025</description><author>dblp: new volumes for streams/conf/ndss</author><pubDate>Tue, 18 Mar 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/ndss/Lin0YL25</guid></item><item><title>[NeurIPS 2025] Suitable is the Best: Task-Oriented Knowledge Fusion in Vulnerability Detection.</title><link>http://papers.nips.cc/paper_files/paper/2024/hash/db7a81128155d6f14970c12d0b5e7a4c-Abstract-Conference.html</link><description>Authors: Jingjing Wang, Minhuan Huang, Yuanping Nie, Xiang Li 0078, Qianjin Du, Wei Kong, Huan Deng, Xiaohui Kuang
Venue: NeurIPS
Year: 2024</description><author>dblp: new volumes for streams/conf/nips</author><pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/nips/WangHN0DKDK24</guid></item><item><title>[ASE 2025] Language-Agnostic Static Analysis of Probabilistic Programs.</title><link>https://doi.org/10.1145/3691620.3695031</link><description>Authors: Markus Bck, Michael Schrder 0005, Jrgen Cito
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Bock0C24</guid></item><item><title>[ASE 2025] Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models.</title><link>https://doi.org/10.1145/3691620.3695013</link><description>Authors: Yulun Wu, Ming Wen 0001, Zeliang Yu, Xiaochen Guo, Hai Jin 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Wu0YG024</guid></item><item><title>[ASE 2025] Snopy: Bridging Sample Denoising with Causal Graph Learning for Effective Vulnerability Detection.</title><link>https://doi.org/10.1145/3691620.3695057</link><description>Authors: Sicong Cao, Xiaobing Sun 0001, Xiaoxue Wu 0001, David Lo 0001, Lili Bo, Bin Li 0006, Xiaolei Liu 0001, Xingwei Lin, Wei Liu 0010
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/Cao000B00L024</guid></item><item><title>[ASE 2025] AdvSCanner: Generating Adversarial Smart Contracts to Exploit Reentrancy Vulnerabilities Using LLM and Static Analysis.</title><link>https://doi.org/10.1145/3691620.3695482</link><description>Authors: Yin Wu, Xiaofei Xie, Chenyang Peng, Dijun Liu, Hao Wu 0100, Ming Fan 0002, Ting Liu 0002, Haijun Wang 0002
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WuXPLW00W24</guid></item><item><title>[ASE 2025] TypeFSL: Type Prediction from Binaries via Inter-procedural Data-flow Analysis and Few-shot Learning.</title><link>https://doi.org/10.1145/3691620.3695502</link><description>Authors: Zirui Song, Yutong Zhou, Shuaike Dong, Ke Zhang 0039, Kehuan Zhang
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/SongZDZZ24</guid></item><item><title>[ASE 2025] Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?</title><link>https://doi.org/10.1145/3691620.3695539</link><description>Authors: Yu Zhao, Lina Gong, Zhiqiu Huang, Yongwei Wang, Mingqiang Wei, Fei Wu 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ZhaoGHWW024</guid></item><item><title>[ASE 2025] STASE: Static Analysis Guided Symbolic Execution for UEFI Vulnerability Signature Generation.</title><link>https://doi.org/10.1145/3691620.3695543</link><description>Authors: Md Shafiuzzaman, Achintya Desai, Laboni Sarker, Tevfik Bultan
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/ShafiuzzamanDSB24</guid></item><item><title>[ASE 2025] DeployFix: Dynamic Repair of Software Deployment Failures via Constraint Solving.</title><link>https://doi.org/10.1145/3691620.3695268</link><description>Authors: Haoyu Liao, Jianmei Guo, Bo Huang 0002, Yujie Han, Dingyu Yang, Kai Shi 0006, Jonathan Ding, Guoyao Xu, Guodong Yang, Liping Zhang 0013
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/LiaoG0HY0DXYZ24</guid></item><item><title>[ASE 2025] Experience Report on Applying Program Analysis Techniques for Mainframe Application Understanding.</title><link>https://doi.org/10.1145/3691620.3695270</link><description>Authors: Shivali Agarwal, Hiroaki Nakamura, Rami Katan
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/AgarwalNK24</guid></item><item><title>[ASE 2025] Enhancing Compositional Static Analysis with Dynamic Analysis.</title><link>https://doi.org/10.1145/3691620.3695599</link><description>Authors: Dino Distefano, Matteo Marescotti, Cons T. hs, Sopot Cela, Gabriela Cunha Sampaio, Radu Grigore, kos Hajdu, Timotej Kapus, Ke Mao, Thibault Suzanne
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/DistefanoMACSGH24</guid></item><item><title>[ASE 2025] Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation.</title><link>https://doi.org/10.1145/3691620.3695291</link><description>Authors: Luqiao Wang, Yangtao Zhou, Huiying Zhuang, Qingshan Li, Di Cui, Yutong Zhao, Lu Wang 0014
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZZLCZ024</guid></item><item><title>[ASE 2025] ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts.</title><link>https://doi.org/10.1145/3691620.3695349</link><description>Authors: Che Wang, Jiashuo Zhang 0001, Jianbo Gao 0003, Libin Xia, Zhi Guan, Zhong Chen 0001
Venue: ASE
Year: 2024</description><author>dblp: new volumes for streams/conf/kbse</author><pubDate>Mon, 06 Jan 2025 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/kbse/WangZGXG024</guid></item><item><title>[FM 2024] Learning Branching-Time Properties in CTL and ATL via Constraint Solving.</title><link>https://doi.org/10.1007/978-3-031-71162-6_16</link><description>Authors: Benjamin Bordais, Daniel Neider, Rajarshi Roy 0002
Venue: FM (1)
Year: 2024</description><author>dblp: new volumes for streams/conf/fm</author><pubDate>Mon, 16 Sep 2024 23:00:00 GMT</pubDate><guid isPermaLink="true">dblp:conf/fm/BordaisNR24</guid></item></channel></rss>